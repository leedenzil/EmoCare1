{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4C: Video Model Training v2 (Improved)\n",
    "\n",
    "This is an **improved version** of the video model training with better handling of class imbalance.\n",
    "\n",
    "## ðŸ†• What's New in v2:\n",
    "\n",
    "### 1. **Class Weights** (CRITICAL - Fixes 0% F1 on Sadness/Surprise!)\n",
    "   - Automatically computed from training data\n",
    "   - Sadness gets ~10x more weight (only 40 training samples!)\n",
    "   - Surprise gets ~4x more weight (only 101 training samples)\n",
    "\n",
    "### 2. **Temporal Data Augmentation**\n",
    "   - Random frame sampling with jitter\n",
    "   - Variable frame intervals (adds diversity)\n",
    "   - Random temporal offset\n",
    "\n",
    "### 3. **More Frames Per Video**\n",
    "   - 8 â†’ 12 frames (better temporal coverage)\n",
    "   - Captures more motion/context\n",
    "\n",
    "### 4. **Early Stopping**\n",
    "   - Patience: 5 epochs (more than image due to smaller dataset)\n",
    "\n",
    "### 5. **Better Learning Rate Scheduling**\n",
    "   - ReduceLROnPlateau with patience=3\n",
    "\n",
    "### 6. **Gradient Clipping**\n",
    "   - Max norm: 1.0\n",
    "\n",
    "### 7. **Label Smoothing**\n",
    "   - Smoothing: 0.1\n",
    "\n",
    "### 8. **More Epochs**\n",
    "   - 15 epochs (up from 5)\n",
    "   - Early stopping prevents overfitting\n",
    "\n",
    "**Expected Improvements:**\n",
    "- **Sadness: 0% F1 â†’ 15-25% F1** (huge improvement!)\n",
    "- **Surprise: 0% F1 â†’ 18-30% F1** (huge improvement!)\n",
    "- Overall: 41.9% acc â†’ 48-55% acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight  # NEW\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"âœ“ Using notebook progress bars\")\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "    print(\"âœ“ Using terminal progress bars\")\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_DATA = \"data/train_set.csv\"\n",
    "VAL_DATA = \"data/validation_set.csv\"\n",
    "MODEL_DIR = \"models\"\n",
    "RESULTS_DIR = \"results/video_model_v2\"  # NEW: Separate results directory\n",
    "\n",
    "# Create directories\n",
    "Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 15  # NEW: Increased epochs\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_FRAMES = 12  # NEW: Increased from 8 to 12 frames\n",
    "\n",
    "# NEW: Training improvements\n",
    "LABEL_SMOOTHING = 0.1\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "LR_SCHEDULER_PATIENCE = 3\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "\n",
    "# Sentiment labels\n",
    "LABELS = ['Anger', 'Joy', 'Neutral/Other', 'Sadness', 'Surprise']\n",
    "LABEL_TO_ID = {label: idx for idx, label in enumerate(LABELS)}\n",
    "ID_TO_LABEL = {idx: label for label, idx in LABEL_TO_ID.items()}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Max Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Frames per video: {NUM_FRAMES}\")\n",
    "print(f\"\\nðŸ†• v2 Improvements:\")\n",
    "print(f\"  - Class weights (Sadness ~10x, Surprise ~4x)\")\n",
    "print(f\"  - Temporal augmentation (random frame sampling)\")\n",
    "print(f\"  - More frames: 8 â†’ {NUM_FRAMES}\")\n",
    "print(f\"  - Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"  - Gradient clipping: {GRADIENT_CLIP_NORM}\")\n",
    "print(f\"  - Early stopping: patience={EARLY_STOP_PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Class Weight Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "val_df = pd.read_csv(VAL_DATA)\n",
    "\n",
    "# Filter for videos only\n",
    "train_df = train_df[train_df['media_type'] == 'video'].reset_index(drop=True)\n",
    "val_df = val_df[val_df['media_type'] == 'video'].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train set (videos only): {len(train_df):,} samples\")\n",
    "print(f\"Validation set (videos only): {len(val_df):,} samples\")\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(\"\\nTrain set sentiment distribution:\")\n",
    "train_counts = train_df['post_sentiment'].value_counts()\n",
    "print(train_counts)\n",
    "print(\"\\nPercentages:\")\n",
    "print((train_counts / len(train_df) * 100).round(2))\n",
    "\n",
    "print(\"\\nValidation set sentiment distribution:\")\n",
    "val_counts = val_df['post_sentiment'].value_counts()\n",
    "print(val_counts)\n",
    "print(\"\\nPercentages:\")\n",
    "print((val_counts / len(val_df) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Compute class weights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPUTING CLASS WEIGHTS (NEW in v2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_labels_numeric = train_df['post_sentiment'].map(LABEL_TO_ID).values\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(LABELS)),\n",
    "    y=train_labels_numeric\n",
    ")\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(\"\\nClass weights:\")\n",
    "for label, weight in zip(LABELS, class_weights):\n",
    "    count = train_counts.get(label, 0)\n",
    "    print(f\"  {label:15s}: {weight:.4f} (n={count:,})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ CRITICAL for Video Model:\")\n",
    "print(\"   Sadness has only 40 training samples (0% F1 in v1)!\")\n",
    "print(\"   Surprise has only 101 training samples (0% F1 in v1)!\")\n",
    "print(\"   With 10x and 4x weights, model will actually learn these classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NEW: Temporal Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=12, augment=False):\n",
    "    \"\"\"\n",
    "    Extract frames from video with optional temporal augmentation.\n",
    "    \n",
    "    Augmentation adds randomness to frame sampling for training diversity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            return None\n",
    "        \n",
    "        if augment and total_frames > num_frames * 2:\n",
    "            # NEW: Random temporal offset for augmentation\n",
    "            max_offset = total_frames - num_frames * 2\n",
    "            offset = random.randint(0, max_offset)\n",
    "            \n",
    "            # Sample with random jitter\n",
    "            base_indices = np.linspace(offset, total_frames - offset - 1, num_frames, dtype=int)\n",
    "            jitter = np.random.randint(-2, 3, size=num_frames)  # Â±2 frames jitter\n",
    "            frame_indices = np.clip(base_indices + jitter, 0, total_frames - 1)\n",
    "        else:\n",
    "            # Normal evenly-spaced sampling\n",
    "            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(frame_rgb)\n",
    "                frames.append(pil_image)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad if needed\n",
    "        if len(frames) < num_frames:\n",
    "            while len(frames) < num_frames:\n",
    "                frames.append(frames[-1] if frames else Image.new('RGB', (224, 224), color='black'))\n",
    "        \n",
    "        return frames[:num_frames]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting frames from {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ“ Frame extraction function with temporal augmentation defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrawlStarsVideoDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, num_frames=12, augment=False):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.num_frames = num_frames\n",
    "        self.augment = augment  # NEW\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        video_path = str(row['local_media_path']).replace('\\\\', '/')\n",
    "        \n",
    "        # Extract frames with optional augmentation\n",
    "        frames = extract_frames(video_path, self.num_frames, augment=self.augment)\n",
    "        \n",
    "        if frames is None or len(frames) == 0:\n",
    "            frames = [Image.new('RGB', (224, 224), color='black') for _ in range(self.num_frames)]\n",
    "        \n",
    "        # Process all frames\n",
    "        pixel_values_list = []\n",
    "        for frame in frames:\n",
    "            inputs = self.processor(images=frame, return_tensors=\"pt\")\n",
    "            pixel_values_list.append(inputs['pixel_values'].squeeze(0))\n",
    "        \n",
    "        pixel_values = torch.stack(pixel_values_list)\n",
    "        label = LABEL_TO_ID[row['post_sentiment']]\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize processor\n",
    "print(\"Loading CLIP processor...\")\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BrawlStarsVideoDataset(train_df, processor, NUM_FRAMES, augment=True)\n",
    "val_dataset = BrawlStarsVideoDataset(val_df, processor, NUM_FRAMES, augment=False)\n",
    "\n",
    "# Create dataloaders (Windows-safe for notebooks)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Windows-safe\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Windows-safe\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created {len(train_loader)} train batches and {len(val_loader)} validation batches\")\n",
    "print(f\"âœ“ Temporal augmentation enabled for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5, num_frames=12):\n",
    "        super(VideoSentimentClassifier, self).__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "        self.num_frames = num_frames\n",
    "        self.vision_embed_dim = self.clip.vision_model.config.hidden_size\n",
    "        \n",
    "        # Temporal aggregation (attention-based)\n",
    "        self.temporal_attention = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vision_embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        batch_size, num_frames, C, H, W = pixel_values.shape\n",
    "        pixel_values = pixel_values.view(batch_size * num_frames, C, H, W)\n",
    "        \n",
    "        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "        frame_embeds = vision_outputs.pooler_output\n",
    "        frame_embeds = frame_embeds.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        attention_scores = self.temporal_attention(frame_embeds)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        video_embed = torch.sum(frame_embeds * attention_weights, dim=1)\n",
    "        \n",
    "        logits = self.classifier(video_embed)\n",
    "        return logits\n",
    "    \n",
    "    def get_embedding(self, pixel_values):\n",
    "        with torch.no_grad():\n",
    "            batch_size, num_frames, C, H, W = pixel_values.shape\n",
    "            pixel_values = pixel_values.view(batch_size * num_frames, C, H, W)\n",
    "            \n",
    "            vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "            frame_embeds = vision_outputs.pooler_output\n",
    "            frame_embeds = frame_embeds.view(batch_size, num_frames, -1)\n",
    "            \n",
    "            attention_scores = self.temporal_attention(frame_embeds)\n",
    "            attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "            video_embed = torch.sum(frame_embeds * attention_weights, dim=1)\n",
    "        \n",
    "        return video_embed\n",
    "\n",
    "# Initialize model\n",
    "model = VideoSentimentClassifier(n_classes=len(LABELS), num_frames=NUM_FRAMES)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ“ Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup with Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Loss function with class weights AND label smoothing\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, weight=None, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_pred = torch.log_softmax(pred, dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_pred)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        if self.weight is not None:\n",
    "            true_dist = true_dist * self.weight.unsqueeze(0)\n",
    "            \n",
    "        return torch.mean(torch.sum(-true_dist * log_pred, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingCrossEntropy(weight=class_weights_tensor, smoothing=LABEL_SMOOTHING)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Optimizer and scheduler configured\")\n",
    "print(f\"ðŸ†• Loss: CrossEntropy + Class Weights + Label Smoothing\")\n",
    "print(f\"   This is CRITICAL for fixing 0% F1 on Sadness/Surprise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(pixel_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # NEW: Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRADIENT_CLIP_NORM)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions/total_samples:.4f}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader), correct_predictions / total_samples\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc='Validation')\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(pixel_values)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "print(\"âœ“ Training functions defined with gradient clipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_f1': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_epoch = 0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING TRAINING (v2 with improvements)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ†• Key improvements:\")\n",
    "print(f\"  - Class weights (Sadness ~10x, Surprise ~4x)\")\n",
    "print(f\"  - Temporal augmentation\")\n",
    "print(f\"  - {NUM_FRAMES} frames per video (was 8)\")\n",
    "print(f\"  - Early stopping (patience={EARLY_STOP_PATIENCE})\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch + 1\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), f\"{MODEL_DIR}/video_specialist_v2_best.pth\")\n",
    "        print(f\"  âœ“ New best model saved! (F1: {val_f1:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "    \n",
    "    if epochs_without_improvement >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\nðŸ›‘ Early stopping! No improvement for {EARLY_STOP_PATIENCE} epochs.\")\n",
    "        print(f\"   Best F1: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best validation F1: {best_val_f1:.4f} (Epoch {best_epoch})\")\n",
    "print(f\"Total epochs run: {epoch + 1}/{EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Models and Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(model.state_dict(), f\"{MODEL_DIR}/video_specialist_v2.pth\")\n",
    "print(f\"âœ“ Saved final model: {MODEL_DIR}/video_specialist_v2.pth\")\n",
    "\n",
    "# Save training history\n",
    "with open(f\"{RESULTS_DIR}/training_history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"âœ“ Saved training history: {RESULTS_DIR}/training_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Evaluation on Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(f\"{MODEL_DIR}/video_specialist_v2_best.pth\"))\n",
    "\n",
    "# Evaluate\n",
    "val_loss, val_acc, val_f1, val_preds, val_labels = eval_model(model, val_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION ON VALIDATION SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Validation Metrics:\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Weighted F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Per-Class Metrics and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-CLASS METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "precision, recall, f1_per_class, support = precision_recall_fscore_support(\n",
    "    val_labels, val_preds, labels=range(len(LABELS)), zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\n{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "print(\"-\" * 65)\n",
    "for i, label in enumerate(LABELS):\n",
    "    print(f\"{label:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1_per_class[i]:<12.4f} {support[i]:<10}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(val_labels, val_preds, target_names=LABELS, digits=4, zero_division=0))\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'val_loss': float(val_loss),\n",
    "    'val_accuracy': float(val_acc),\n",
    "    'val_weighted_f1': float(val_f1),\n",
    "    'per_class_metrics': {\n",
    "        label: {\n",
    "            'precision': float(precision[i]),\n",
    "            'recall': float(recall[i]),\n",
    "            'f1_score': float(f1_per_class[i]),\n",
    "            'support': int(support[i])\n",
    "        }\n",
    "        for i, label in enumerate(LABELS)\n",
    "    },\n",
    "    'best_epoch': best_epoch\n",
    "}\n",
    "\n",
    "with open(f\"{RESULTS_DIR}/final_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Saved metrics: {RESULTS_DIR}/final_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[0, 1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[1, 0].plot(history['val_f1'], label='Val F1', marker='o', color='green')\n",
    "axes[1, 0].axhline(y=best_val_f1, color='r', linestyle='--', label=f'Best F1: {best_val_f1:.4f}')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].set_title('Validation F1 Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "axes[1, 1].plot(history['learning_rates'], marker='o', color='orange')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved: {RESULTS_DIR}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=LABELS, yticklabels=LABELS)\n",
    "plt.title('Confusion Matrix - Video Model v2', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved: {RESULTS_DIR}/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Visualize Per-Class F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(LABELS))\n",
    "plt.bar(x, f1_per_class, alpha=0.8, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Sentiment Class', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('Per-Class F1 Scores - Video Model v2', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, LABELS, rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(f1_per_class):\n",
    "    plt.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/per_class_f1.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Saved: {RESULTS_DIR}/per_class_f1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Comparison with v1 (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_metrics_path = \"results/video_model/final_metrics.json\"\n",
    "if os.path.exists(v1_metrics_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON: v1 vs v2\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    with open(v1_metrics_path, 'r') as f:\n",
    "        v1_metrics = json.load(f)\n",
    "    \n",
    "    # Overall comparison\n",
    "    print(\"\\nðŸ“Š Overall Metrics:\")\n",
    "    print(f\"{'Metric':<20} {'v1':<12} {'v2':<12} {'Change':<12}\")\n",
    "    print(\"-\" * 56)\n",
    "    \n",
    "    v1_acc = v1_metrics['val_accuracy']\n",
    "    v2_acc = val_acc\n",
    "    acc_change = v2_acc - v1_acc\n",
    "    print(f\"{'Accuracy':<20} {v1_acc:<12.4f} {v2_acc:<12.4f} {acc_change:+.4f}\")\n",
    "    \n",
    "    v1_f1 = v1_metrics['val_weighted_f1']\n",
    "    v2_f1 = val_f1\n",
    "    f1_change = v2_f1 - v1_f1\n",
    "    print(f\"{'Weighted F1':<20} {v1_f1:<12.4f} {v2_f1:<12.4f} {f1_change:+.4f}\")\n",
    "    \n",
    "    # Per-class comparison\n",
    "    print(\"\\nðŸ“Š Per-Class F1 Score Improvements:\")\n",
    "    print(f\"{'Class':<15} {'v1 F1':<12} {'v2 F1':<12} {'Change':<12} {'Status':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, label in enumerate(LABELS):\n",
    "        v1_class_f1 = v1_metrics['per_class_metrics'][label]['f1_score']\n",
    "        v2_class_f1 = float(f1_per_class[i])\n",
    "        change = v2_class_f1 - v1_class_f1\n",
    "        \n",
    "        if change > 0.05:\n",
    "            status = \"âœ“ Improved\"\n",
    "        elif change < -0.05:\n",
    "            status = \"âœ— Degraded\"\n",
    "        else:\n",
    "            status = \"â‰ˆ Similar\"\n",
    "        \n",
    "        print(f\"{label:<15} {v1_class_f1:<12.4f} {v2_class_f1:<12.4f} {change:+12.4f} {status:<15}\")\n",
    "    \n",
    "    # Highlight critical improvements\n",
    "    sadness_v1 = v1_metrics['per_class_metrics']['Sadness']['f1_score']\n",
    "    sadness_v2 = float(f1_per_class[LABELS.index('Sadness')])\n",
    "    surprise_v1 = v1_metrics['per_class_metrics']['Surprise']['f1_score']\n",
    "    surprise_v2 = float(f1_per_class[LABELS.index('Surprise')])\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ CRITICAL FIXES (Previously 0% F1):\")\n",
    "    print(f\"  Sadness:  {sadness_v1:.1%} â†’ {sadness_v2:.1%} ({sadness_v2-sadness_v1:+.1%})\")\n",
    "    print(f\"  Surprise: {surprise_v1:.1%} â†’ {surprise_v2:.1%} ({surprise_v2-surprise_v1:+.1%})\")\n",
    "    \n",
    "    # Visualization: v1 vs v2 comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(LABELS))\n",
    "    width = 0.35\n",
    "    \n",
    "    v1_f1_scores = [v1_metrics['per_class_metrics'][label]['f1_score'] for label in LABELS]\n",
    "    v2_f1_scores = [float(f1_per_class[i]) for i in range(len(LABELS))]\n",
    "    \n",
    "    ax.bar(x - width/2, v1_f1_scores, width, label='v1 (Original)', alpha=0.8, color='lightcoral', edgecolor='black')\n",
    "    ax.bar(x + width/2, v2_f1_scores, width, label='v2 (Improved)', alpha=0.8, color='steelblue', edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('Sentiment Class', fontsize=12)\n",
    "    ax.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax.set_title('Video Model: v1 vs v2 Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(LABELS, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/v1_vs_v2_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ Saved comparison chart: {RESULTS_DIR}/v1_vs_v2_comparison.png\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ v1 metrics not found. Skipping comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VIDEO MODEL v2 TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“ Files saved:\")\n",
    "print(f\"  - Best model: {MODEL_DIR}/video_specialist_v2_best.pth\")\n",
    "print(f\"  - Final model: {MODEL_DIR}/video_specialist_v2.pth\")\n",
    "print(f\"  - Metrics: {RESULTS_DIR}/final_metrics.json\")\n",
    "print(f\"  - History: {RESULTS_DIR}/training_history.json\")\n",
    "print(f\"  - Visualizations: {RESULTS_DIR}/*.png\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Best Results:\")\n",
    "print(f\"  - Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  - Validation F1: {val_f1:.4f}\")\n",
    "print(f\"  - Best Epoch: {best_epoch}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ v2 Improvements Applied:\")\n",
    "print(f\"  âœ“ Class weights (Sadness ~10x, Surprise ~4x)\")\n",
    "print(f\"  âœ“ Temporal augmentation (random frame sampling)\")\n",
    "print(f\"  âœ“ More frames: {NUM_FRAMES} (was 8)\")\n",
    "print(f\"  âœ“ Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"  âœ“ Gradient clipping: {GRADIENT_CLIP_NORM}\")\n",
    "print(f\"  âœ“ Early stopping: patience={EARLY_STOP_PATIENCE}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"  1. Review per-class F1 scores (especially Sadness/Surprise)\")\n",
    "print(f\"  2. Compare with v1 results to see improvements\")\n",
    "print(f\"  3. If satisfied, use this model in your pipeline\")\n",
    "print(f\"  4. For even better performance, consider the .py script version (better GPU utilization)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
