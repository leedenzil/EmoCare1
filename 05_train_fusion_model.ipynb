{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Fusion Model Training (Phase 2)\n",
    "\n",
    "This notebook trains the fusion model that combines embeddings from all three specialist models.\n",
    "\n",
    "**Goal**: Train the fusion model to make final sentiment predictions by intelligently combining:\n",
    "- Text embeddings (DistilBERT)\n",
    "- Image embeddings (CLIP)\n",
    "- Video embeddings (CLIP + Temporal Attention)\n",
    "\n",
    "**Approach**:\n",
    "1. Load all three trained specialist models\n",
    "2. Extract embeddings for all posts (use zero vectors for missing modalities)\n",
    "3. Train a fusion network to combine embeddings and predict sentiment\n",
    "\n",
    "**Outputs**:\n",
    "- `models/fusion_model.pth` - Trained fusion model weights\n",
    "- `models/fusion_model_best.pth` - Best fusion model weights\n",
    "- `results/fusion_model/confusion_matrix.png` - Confusion matrix visualization\n",
    "- `results/fusion_model/training_curves.png` - Training/validation loss and accuracy\n",
    "- `results/fusion_model/evaluation_metrics.json` - Detailed metrics\n",
    "- `results/fusion_model/training_history.json` - Epoch-by-epoch history\n",
    "- `results/fusion_model/evaluation_report.txt` - Human-readable report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import tqdm for notebooks, fallback to regular tqdm\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"✓ Using notebook progress bars\")\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "    print(\"✓ Using terminal progress bars (install ipywidgets for notebook progress bars)\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_DATA = \"data/train_set.csv\"\n",
    "VAL_DATA = \"data/validation_set.csv\"\n",
    "MODEL_DIR = \"models\"\n",
    "RESULTS_DIR = \"results/fusion_model\"\n",
    "\n",
    "# Create directories\n",
    "Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Specialist model paths\n",
    "TEXT_MODEL_PATH = f\"{MODEL_DIR}/text_specialist_best.pth\"\n",
    "IMAGE_MODEL_PATH = f\"{MODEL_DIR}/image_specialist_best.pth\"\n",
    "VIDEO_MODEL_PATH = f\"{MODEL_DIR}/video_specialist_best.pth\"\n",
    "\n",
    "# Fusion model configuration\n",
    "BATCH_SIZE = 64  # Larger batch size since we're just using pre-computed embeddings\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model parameters\n",
    "TEXT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "CLIP_MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "MAX_LENGTH = 128\n",
    "NUM_FRAMES = 8\n",
    "\n",
    "# Sentiment labels\n",
    "LABELS = ['Anger', 'Joy', 'Neutral/Other', 'Sadness', 'Surprise']\n",
    "LABEL_TO_ID = {label: idx for idx, label in enumerate(LABELS)}\n",
    "ID_TO_LABEL = {idx: label for label, idx in LABEL_TO_ID.items()}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Labels: {LABELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Specialist Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialist model classes (same as training notebooks)\n",
    "\n",
    "class TextSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5):\n",
    "        super(TextSentimentClassifier, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(TEXT_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    "    def get_embedding(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class ImageSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5):\n",
    "        super(ImageSentimentClassifier, self).__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(CLIP_MODEL_NAME)\n",
    "        self.vision_embed_dim = self.clip.vision_model.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vision_embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def get_embedding(self, pixel_values):\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "            image_embeds = vision_outputs.pooler_output\n",
    "        return image_embeds\n",
    "\n",
    "\n",
    "class VideoSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5, num_frames=8):\n",
    "        super(VideoSentimentClassifier, self).__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(CLIP_MODEL_NAME)\n",
    "        self.num_frames = num_frames\n",
    "        self.vision_embed_dim = self.clip.vision_model.config.hidden_size\n",
    "        \n",
    "        self.temporal_attention = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vision_embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    \n",
    "    def get_embedding(self, pixel_values):\n",
    "        with torch.no_grad():\n",
    "            batch_size, num_frames, C, H, W = pixel_values.shape\n",
    "            pixel_values = pixel_values.view(batch_size * num_frames, C, H, W)\n",
    "            \n",
    "            vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "            frame_embeds = vision_outputs.pooler_output\n",
    "            frame_embeds = frame_embeds.view(batch_size, num_frames, -1)\n",
    "            \n",
    "            attention_scores = self.temporal_attention(frame_embeds)\n",
    "            attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "            video_embed = torch.sum(frame_embeds * attention_weights, dim=1)\n",
    "        \n",
    "        return video_embed\n",
    "\n",
    "print(\"✓ Specialist model classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load specialist models\n",
    "print(\"Loading specialist models...\")\n",
    "\n",
    "# Text model\n",
    "text_model = TextSentimentClassifier(n_classes=len(LABELS))\n",
    "text_model.load_state_dict(torch.load(TEXT_MODEL_PATH, map_location=device))\n",
    "text_model = text_model.to(device)\n",
    "text_model.eval()\n",
    "print(f\"✓ Loaded text model from {TEXT_MODEL_PATH}\")\n",
    "\n",
    "# Image model\n",
    "image_model = ImageSentimentClassifier(n_classes=len(LABELS))\n",
    "image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=device))\n",
    "image_model = image_model.to(device)\n",
    "image_model.eval()\n",
    "print(f\"✓ Loaded image model from {IMAGE_MODEL_PATH}\")\n",
    "\n",
    "# Video model\n",
    "video_model = VideoSentimentClassifier(n_classes=len(LABELS), num_frames=NUM_FRAMES)\n",
    "video_model.load_state_dict(torch.load(VIDEO_MODEL_PATH, map_location=device))\n",
    "video_model = video_model.to(device)\n",
    "video_model.eval()\n",
    "print(f\"✓ Loaded video model from {VIDEO_MODEL_PATH}\")\n",
    "\n",
    "# Embedding dimensions\n",
    "TEXT_EMBED_DIM = 768  # DistilBERT\n",
    "IMAGE_EMBED_DIM = 512  # CLIP vision\n",
    "VIDEO_EMBED_DIM = 512  # CLIP vision\n",
    "TOTAL_EMBED_DIM = TEXT_EMBED_DIM + IMAGE_EMBED_DIM + VIDEO_EMBED_DIM\n",
    "\n",
    "print(f\"\\nEmbedding dimensions:\")\n",
    "print(f\"  Text: {TEXT_EMBED_DIM}\")\n",
    "print(f\"  Image: {IMAGE_EMBED_DIM}\")\n",
    "print(f\"  Video: {VIDEO_EMBED_DIM}\")\n",
    "print(f\"  Total: {TOTAL_EMBED_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and processor\n",
    "print(\"Loading tokenizer and processors...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
    "print(\"✓ Tokenizer and processors loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Embeddings from All Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for frame extraction\n",
    "def extract_frames(video_path, num_frames=8):\n",
    "    \"\"\"Extract evenly spaced frames from a video.\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            return None\n",
    "        \n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(frame_rgb)\n",
    "                frames.append(pil_image)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) < num_frames:\n",
    "            while len(frames) < num_frames:\n",
    "                frames.append(frames[-1] if frames else Image.new('RGB', (224, 224), color='black'))\n",
    "        \n",
    "        return frames[:num_frames]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_dataset(df, text_model, image_model, video_model, tokenizer, clip_processor, device):\n",
    "    \"\"\"\n",
    "    Extract embeddings for all posts in the dataset.\n",
    "    Returns: text_embeds, image_embeds, video_embeds, labels\n",
    "    \"\"\"\n",
    "    all_text_embeds = []\n",
    "    all_image_embeds = []\n",
    "    all_video_embeds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(f\"Extracting embeddings for {len(df)} posts...\")\n",
    "    \n",
    "    for idx in tqdm(range(len(df)), desc=\"Processing posts\"):\n",
    "        row = df.iloc[idx]\n",
    "        \n",
    "        # --- TEXT EMBEDDING ---\n",
    "        title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "        text = str(row['text']) if pd.notna(row['text']) else \"\"\n",
    "        combined_text = f\"{title} {text}\".strip()\n",
    "        \n",
    "        encoding = tokenizer(\n",
    "            combined_text,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        text_embed = text_model.get_embedding(input_ids, attention_mask)\n",
    "        text_embed = text_embed.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Force to exact shape\n",
    "        if text_embed.shape[0] != TEXT_EMBED_DIM:\n",
    "            text_embed = text_embed.flatten()[:TEXT_EMBED_DIM]\n",
    "            if len(text_embed) < TEXT_EMBED_DIM:\n",
    "                text_embed = np.pad(text_embed, (0, TEXT_EMBED_DIM - len(text_embed)))\n",
    "        \n",
    "        all_text_embeds.append(text_embed)\n",
    "        \n",
    "        # --- IMAGE EMBEDDING ---\n",
    "        if row['media_type'] == 'image':\n",
    "            image_path = str(row['local_media_path']).replace('\\\\', '/')\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "                pixel_values = inputs['pixel_values'].to(device)\n",
    "                image_embed = image_model.get_embedding(pixel_values)\n",
    "                image_embed = image_embed.squeeze().cpu().numpy()\n",
    "                \n",
    "                # Force to exact shape\n",
    "                if image_embed.shape[0] != IMAGE_EMBED_DIM:\n",
    "                    image_embed = image_embed.flatten()[:IMAGE_EMBED_DIM]\n",
    "                    if len(image_embed) < IMAGE_EMBED_DIM:\n",
    "                        image_embed = np.pad(image_embed, (0, IMAGE_EMBED_DIM - len(image_embed)))\n",
    "            except Exception as e:\n",
    "                image_embed = np.zeros(IMAGE_EMBED_DIM, dtype=np.float32)\n",
    "        else:\n",
    "            image_embed = np.zeros(IMAGE_EMBED_DIM, dtype=np.float32)\n",
    "        \n",
    "        # Final check for image embed\n",
    "        if image_embed.shape != (IMAGE_EMBED_DIM,):\n",
    "            image_embed = np.zeros(IMAGE_EMBED_DIM, dtype=np.float32)\n",
    "        \n",
    "        all_image_embeds.append(image_embed)\n",
    "        \n",
    "        # --- VIDEO EMBEDDING ---\n",
    "        if row['media_type'] == 'video':\n",
    "            video_path = str(row['local_media_path']).replace('\\\\', '/')\n",
    "            frames = extract_frames(video_path, NUM_FRAMES)\n",
    "            \n",
    "            if frames:\n",
    "                try:\n",
    "                    pixel_values_list = []\n",
    "                    for frame in frames:\n",
    "                        inputs = clip_processor(images=frame, return_tensors=\"pt\")\n",
    "                        pixel_values_list.append(inputs['pixel_values'].squeeze(0))\n",
    "                    pixel_values = torch.stack(pixel_values_list).unsqueeze(0).to(device)\n",
    "                    video_embed = video_model.get_embedding(pixel_values)\n",
    "                    video_embed = video_embed.squeeze().cpu().numpy()\n",
    "                    \n",
    "                    # Force to exact shape\n",
    "                    if video_embed.shape[0] != VIDEO_EMBED_DIM:\n",
    "                        video_embed = video_embed.flatten()[:VIDEO_EMBED_DIM]\n",
    "                        if len(video_embed) < VIDEO_EMBED_DIM:\n",
    "                            video_embed = np.pad(video_embed, (0, VIDEO_EMBED_DIM - len(video_embed)))\n",
    "                except Exception as e:\n",
    "                    video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "            else:\n",
    "                video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "        else:\n",
    "            video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "        \n",
    "        # Final check for video embed\n",
    "        if video_embed.shape != (VIDEO_EMBED_DIM,):\n",
    "            video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "        \n",
    "        all_video_embeds.append(video_embed)\n",
    "        \n",
    "        # --- LABEL ---\n",
    "        label = LABEL_TO_ID[row['post_sentiment']]\n",
    "        all_labels.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    text_embeds = np.array(all_text_embeds, dtype=np.float32)\n",
    "    image_embeds = np.array(all_image_embeds, dtype=np.float32)\n",
    "    video_embeds = np.array(all_video_embeds, dtype=np.float32)\n",
    "    labels = np.array(all_labels, dtype=np.int64)\n",
    "    \n",
    "    # Verify shapes\n",
    "    print(f\"Shape verification:\")\n",
    "    print(f\"  Text embeds: {text_embeds.shape} (expected: {(len(df), TEXT_EMBED_DIM)})\")\n",
    "    print(f\"  Image embeds: {image_embeds.shape} (expected: {(len(df), IMAGE_EMBED_DIM)})\")\n",
    "    print(f\"  Video embeds: {video_embeds.shape} (expected: {(len(df), VIDEO_EMBED_DIM)})\")\n",
    "    \n",
    "    assert text_embeds.shape == (len(df), TEXT_EMBED_DIM), f\"Text embeds shape mismatch!\"\n",
    "    assert image_embeds.shape == (len(df), IMAGE_EMBED_DIM), f\"Image embeds shape mismatch!\"\n",
    "    assert video_embeds.shape == (len(df), VIDEO_EMBED_DIM), f\"Video embeds shape mismatch!\"\n",
    "    \n",
    "    return text_embeds, image_embeds, video_embeds, labels\n",
    "\n",
    "print(\"✓ Embedding extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "val_df = pd.read_csv(VAL_DATA)\n",
    "print(f\"Train set: {len(train_df):,} samples\")\n",
    "print(f\"Validation set: {len(val_df):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CELL - Run this FIRST to catch errors quickly!\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING EMBEDDING EXTRACTION ON 5 SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with just 5 posts\n",
    "test_df = train_df.head(5)\n",
    "\n",
    "try:\n",
    "    test_text, test_image, test_video, test_labels = extract_embeddings_from_dataset(\n",
    "        test_df, text_model, image_model, video_model, tokenizer, clip_processor, device\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ TEST PASSED! Shapes:\")\n",
    "    print(f\"  Text: {test_text.shape}\")\n",
    "    print(f\"  Image: {test_image.shape}\")\n",
    "    print(f\"  Video: {test_video.shape}\")\n",
    "    print(f\"  Labels: {test_labels.shape}\")\n",
    "    print(\"\\n✓ Ready to process full dataset!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ TEST FAILED! Error: {e}\")\n",
    "    print(\"Fix the error above before running on full dataset!\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for training set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING TRAIN SET EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "train_text_embeds, train_image_embeds, train_video_embeds, train_labels = extract_embeddings_from_dataset(\n",
    "    train_df, text_model, image_model, video_model, tokenizer, clip_processor, device\n",
    ")\n",
    "print(f\"\\n✓ Train embeddings extracted:\")\n",
    "print(f\"  Text: {train_text_embeds.shape}\")\n",
    "print(f\"  Image: {train_image_embeds.shape}\")\n",
    "print(f\"  Video: {train_video_embeds.shape}\")\n",
    "print(f\"  Labels: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for validation set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING VALIDATION SET EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "val_text_embeds, val_image_embeds, val_video_embeds, val_labels = extract_embeddings_from_dataset(\n",
    "    val_df, text_model, image_model, video_model, tokenizer, clip_processor, device\n",
    ")\n",
    "print(f\"\\n✓ Validation embeddings extracted:\")\n",
    "print(f\"  Text: {val_text_embeds.shape}\")\n",
    "print(f\"  Image: {val_image_embeds.shape}\")\n",
    "print(f\"  Video: {val_video_embeds.shape}\")\n",
    "print(f\"  Labels: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate embeddings\n",
    "train_combined_embeds = np.concatenate([train_text_embeds, train_image_embeds, train_video_embeds], axis=1)\n",
    "val_combined_embeds = np.concatenate([val_text_embeds, val_image_embeds, val_video_embeds], axis=1)\n",
    "\n",
    "print(f\"\\nCombined embedding dimensions:\")\n",
    "print(f\"  Train: {train_combined_embeds.shape}\")\n",
    "print(f\"  Validation: {val_combined_embeds.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_embeds_tensor = torch.FloatTensor(train_combined_embeds)\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "val_embeds_tensor = torch.FloatTensor(val_combined_embeds)\n",
    "val_labels_tensor = torch.LongTensor(val_labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_embeds_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_embeds_tensor, val_labels_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\n✓ Created {len(train_loader)} train batches and {len(val_loader)} validation batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes=5):\n",
    "        super(FusionModel, self).__init__()\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fusion(x)\n",
    "\n",
    "# Initialize fusion model\n",
    "fusion_model = FusionModel(input_dim=TOTAL_EMBED_DIM, n_classes=len(LABELS))\n",
    "fusion_model = fusion_model.to(device)\n",
    "\n",
    "print(f\"✓ Fusion model initialized with {sum(p.numel() for p in fusion_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimizer and scheduler configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for embeds, labels in progress_bar:\n",
    "        embeds = embeds.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeds)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions/total_samples:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc='Validation')\n",
    "        for embeds, labels in progress_bar:\n",
    "            embeds = embeds.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(embeds)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING FUSION MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(fusion_model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_model(fusion_model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(fusion_model.state_dict(), f\"{MODEL_DIR}/fusion_model_best.pth\")\n",
    "        print(f\"  ✓ New best model saved! (F1: {val_f1:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Best validation F1: {best_val_f1:.4f} (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(fusion_model.state_dict(), f\"{MODEL_DIR}/fusion_model.pth\")\n",
    "print(f\"✓ Final model saved to {MODEL_DIR}/fusion_model.pth\")\n",
    "\n",
    "# Save training history\n",
    "with open(f\"{RESULTS_DIR}/training_history.json\", 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"✓ Training history saved to {RESULTS_DIR}/training_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Best Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "fusion_model.load_state_dict(torch.load(f\"{MODEL_DIR}/fusion_model_best.pth\"))\n",
    "print(f\"✓ Loaded best model (Epoch {best_epoch}, F1: {best_val_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_loss, val_acc, val_f1, val_preds, val_labels_list = eval_model(fusion_model, val_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nFinal Validation Metrics:\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Weighted F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(val_labels_list, val_preds, target_names=LABELS, digits=4)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Save classification report\n",
    "with open(f\"{RESULTS_DIR}/evaluation_report.txt\", 'w') as f:\n",
    "    f.write(\"FUSION MODEL EVALUATION REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Input: Combined embeddings from Text + Image + Video specialists\\n\")\n",
    "    f.write(f\"Embedding dimension: {TOTAL_EMBED_DIM}\\n\")\n",
    "    f.write(f\"Best Epoch: {best_epoch}\\n\")\n",
    "    f.write(f\"Validation Loss: {val_loss:.4f}\\n\")\n",
    "    f.write(f\"Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "    f.write(f\"Validation Weighted F1: {val_f1:.4f}\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\n✓ Evaluation report saved to {RESULTS_DIR}/evaluation_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed metrics JSON\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1_per_class, support = precision_recall_fscore_support(\n",
    "    val_labels_list, val_preds, labels=range(len(LABELS))\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"overall\": {\n",
    "        \"accuracy\": float(val_acc),\n",
    "        \"weighted_f1\": float(val_f1),\n",
    "        \"loss\": float(val_loss)\n",
    "    },\n",
    "    \"per_class\": {}\n",
    "}\n",
    "\n",
    "for idx, label in enumerate(LABELS):\n",
    "    metrics[\"per_class\"][label] = {\n",
    "        \"precision\": float(precision[idx]),\n",
    "        \"recall\": float(recall[idx]),\n",
    "        \"f1_score\": float(f1_per_class[idx]),\n",
    "        \"support\": int(support[idx])\n",
    "    }\n",
    "\n",
    "# Save metrics\n",
    "with open(f\"{RESULTS_DIR}/evaluation_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"✓ Detailed metrics saved to {RESULTS_DIR}/evaluation_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', marker='s')\n",
    "axes[1].plot(history['val_f1'], label='Validation F1', marker='^')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Training and Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training curves saved to {RESULTS_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(val_labels_list, val_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Oranges',\n",
    "    xticklabels=LABELS,\n",
    "    yticklabels=LABELS,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Fusion Model - Validation Set', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Confusion matrix saved to {RESULTS_DIR}/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-class F1 scores\n",
    "f1_scores = [metrics['per_class'][label]['f1_score'] for label in LABELS]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(LABELS, f1_scores, color=['#e74c3c', '#f39c12', '#95a5a6', '#3498db', '#9b59b6'])\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('Per-Class F1 Scores - Fusion Model', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.3f}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/f1_scores_per_class.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Per-class F1 scores plot saved to {RESULTS_DIR}/f1_scores_per_class.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FUSION MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nFiles Generated:\")\n",
    "print(f\"  1. Model weights: {MODEL_DIR}/fusion_model.pth\")\n",
    "print(f\"  2. Best model weights: {MODEL_DIR}/fusion_model_best.pth\")\n",
    "print(f\"  3. Training history: {RESULTS_DIR}/training_history.json\")\n",
    "print(f\"  4. Evaluation metrics: {RESULTS_DIR}/evaluation_metrics.json\")\n",
    "print(f\"  5. Evaluation report: {RESULTS_DIR}/evaluation_report.txt\")\n",
    "print(f\"  6. Training curves: {RESULTS_DIR}/training_curves.png\")\n",
    "print(f\"  7. Confusion matrix: {RESULTS_DIR}/confusion_matrix.png\")\n",
    "print(f\"  8. F1 scores per class: {RESULTS_DIR}/f1_scores_per_class.png\")\n",
    "\n",
    "print(\"\\nFinal Performance:\")\n",
    "print(f\"  Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Weighted F1: {val_f1:.4f}\")\n",
    "print(f\"  Best Epoch: {best_epoch}/{EPOCHS}\")\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores:\")\n",
    "for label in LABELS:\n",
    "    f1 = metrics['per_class'][label]['f1_score']\n",
    "    support = metrics['per_class'][label]['support']\n",
    "    print(f\"  {label:15s}: {f1:.4f} (n={support})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2 COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNext Step: Evaluation on Test Set - Step 6\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emocare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
