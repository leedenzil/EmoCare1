{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Setup & Data Collection\n",
    "\n",
    "This notebook handles:\n",
    "- **Step 0**: Environment setup, API configuration, and folder structure\n",
    "- **Step 1**: Scraping Reddit posts from r/BrawlStars and downloading media locally\n",
    "\n",
    "**Output**: `data/raw_data.csv` with post metadata and local media paths\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Preparation\n",
    "\n",
    "First, we set up our environment. This involves installing all necessary libraries, setting up our API keys, and creating the directories where we'll store our data and media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this in your terminal/virtual environment first)\n",
    "# pip install praw pmaw pandas requests google-generativeai scikit-learn transformers torch torchvision opencv-python-headless tqdm seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7eb6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b96bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import praw                     # For Reddit API access\n",
    "import pandas as pd             # For data manipulation\n",
    "import requests                 # For downloading files\n",
    "import os                       # For file/directory operations\n",
    "from tqdm.auto import tqdm      # For progress bars\n",
    "from datetime import datetime   # For date handling\n",
    "\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6be04",
   "metadata": {},
   "source": [
    "### API Keys & Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Replace the placeholder values with your actual API keys.\n",
    "\n",
    "**Best Practice**: Store these in a `.env` file and use `python-dotenv` to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef92bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Keys & Config ---\n",
    "# !! IMPORTANT: Replace with your actual API keys\n",
    "# Get Reddit API credentials at: https://www.reddit.com/prefs/apps\n",
    "# --- API Keys ---\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = \"BrawlStars Sentiment Scraper v3.0 by /u/YOUR_USERNAME\"\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "print(\"‚úÖ API keys configured\")\n",
    "\n",
    "print(\"‚úÖ API keys configured (make sure you replaced the placeholders!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c604f",
   "metadata": {},
   "source": [
    "### Project Constants & Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b22d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Project Constants ---\n",
    "SUBREDDIT_NAME = \"Brawlstars\"\n",
    "POST_LIMIT = 1200  # Scrape 1200 to aim for ~1000 good posts\n",
    "\n",
    "# --- File & Directory Setup ---\n",
    "MEDIA_DIR = \"media\"\n",
    "IMAGE_DIR = os.path.join(MEDIA_DIR, \"images\")\n",
    "VIDEO_DIR = os.path.join(MEDIA_DIR, \"videos\")\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "RAW_DATA_CSV = os.path.join(DATA_DIR, 'raw_data.csv')\n",
    "\n",
    "print(f\"‚úÖ Directory structure created:\")\n",
    "print(f\"   üìÅ {IMAGE_DIR}\")\n",
    "print(f\"   üìÅ {VIDEO_DIR}\")\n",
    "print(f\"   üìÅ {DATA_DIR}\")\n",
    "print(f\"\\nüìÑ Output will be saved to: {RAW_DATA_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef6618",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884502fe",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection (Scraping and Downloading)\n",
    "\n",
    "Here, we connect to the Reddit API using PRAW, scrape the latest posts from r/BrawlStars, and‚Äîmost importantly‚Äîdownload the associated image or video for each post. We save the *local path* to this media in our DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e70b9",
   "metadata": {},
   "source": [
    "### Initialize Reddit API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f24dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRAW (Reddit API client)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT,\n",
    ")\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    # This should return None for read-only (script) authentication\n",
    "    user = reddit.user.me()\n",
    "    print(f\"‚úÖ Connected to Reddit API\")\n",
    "    print(f\"   User: {user if user else 'Read-only access (script mode)'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Reddit: {e}\")\n",
    "    print(\"   Please check your API credentials!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9f084",
   "metadata": {},
   "source": [
    "### Define Media Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d82ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gallery_post(post):\n",
    "    \"\"\"\n",
    "    Check if a post is a gallery post (multiple images).\n",
    "    \n",
    "    Args:\n",
    "        post: A PRAW submission object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if post is a gallery, False otherwise\n",
    "    \"\"\"\n",
    "    # Gallery posts have gallery_data or media_metadata attributes\n",
    "    return hasattr(post, 'gallery_data') or hasattr(post, 'media_metadata')\n",
    "\n",
    "\n",
    "def download_media(post):\n",
    "    \"\"\"\n",
    "    Downloads the media (image or video) for a PRAW post and returns the local file path.\n",
    "    \n",
    "    Args:\n",
    "        post: A PRAW submission object\n",
    "        \n",
    "    Returns:\n",
    "        str: Local file path if media was downloaded, None otherwise\n",
    "    \"\"\"\n",
    "    # Skip gallery posts\n",
    "    if is_gallery_post(post):\n",
    "        return None\n",
    "    \n",
    "    post_hint = getattr(post, 'post_hint', None)\n",
    "    media_url = None\n",
    "    local_path = None\n",
    "    file_ext = \".unknown\"\n",
    "\n",
    "    try:\n",
    "        if post_hint == 'image':\n",
    "            media_url = post.url\n",
    "            file_ext = os.path.splitext(media_url)[1]\n",
    "            if not file_ext:\n",
    "                file_ext = \".jpg\"  # Default for images without clear extension\n",
    "            local_path = os.path.join(IMAGE_DIR, f\"{post.id}{file_ext}\")\n",
    "\n",
    "        elif post_hint == 'hosted:video':\n",
    "            media_url = post.media['reddit_video']['fallback_url']\n",
    "            file_ext = \".mp4\"\n",
    "            local_path = os.path.join(VIDEO_DIR, f\"{post.id}{file_ext}\")\n",
    "        \n",
    "        elif post_hint == 'rich:video':\n",
    "            # These are often YouTube links, etc. We'll skip downloading them for now.\n",
    "            # You could use youtube-dlp if you want to handle these.\n",
    "            pass\n",
    "\n",
    "        # If we have a URL and a path, download the file\n",
    "        if media_url and local_path:\n",
    "            if os.path.exists(local_path):\n",
    "                return local_path  # Already downloaded\n",
    "\n",
    "            response = requests.get(media_url, stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            return local_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # Silently fail for individual posts\n",
    "        return None\n",
    "    \n",
    "    return None  # No downloadable media\n",
    "\n",
    "print(\"‚úÖ Media download function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e6832",
   "metadata": {},
   "source": [
    "### Scrape Posts and Download Media\n",
    "\n",
    "This cell will:\n",
    "1. Fetch posts from r/BrawlStars\n",
    "2. Download any associated media (images/videos)\n",
    "3. Store all data in a DataFrame\n",
    "4. Save to CSV\n",
    "\n",
    "‚è±Ô∏è **This may take 10-20 minutes depending on your connection speed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING DATA COLLECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- 1. Load Existing Data (if any) ---\n",
    "try:\n",
    "    df_existing = pd.read_csv(RAW_DATA_CSV)\n",
    "    already_scraped_ids = set(df_existing['id'])\n",
    "    print(f\"\\n‚úÖ Loaded {len(df_existing)} previously scraped posts\")\n",
    "except FileNotFoundError:\n",
    "    df_existing = pd.DataFrame()\n",
    "    already_scraped_ids = set()\n",
    "    print(f\"\\nüìù No existing raw data found. Starting from scratch.\")\n",
    "\n",
    "# --- 2. Initialize Scraping ---\n",
    "print(f\"\\nüìä Scraping configuration:\")\n",
    "print(f\"   Target posts to fetch: {POST_LIMIT}\")\n",
    "print(f\"   Already in dataset:    {len(already_scraped_ids)}\")\n",
    "print(f\"   Gallery posts will be skipped\")\n",
    "print(f\"\\n‚èπÔ∏è  Press 'Kernel ‚Üí Interrupt' to stop at any time (progress is saved)\\n\")\n",
    "\n",
    "all_posts_data = []\n",
    "gallery_posts_skipped = 0\n",
    "duplicate_posts_skipped = 0\n",
    "subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
    "\n",
    "# --- 3. Scrape Posts (Interruptible Loop) ---\n",
    "try:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üîç Fetching posts from r/{SUBREDDIT_NAME}...\\n\")\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for post in tqdm(subreddit.hot(limit=POST_LIMIT), total=POST_LIMIT, desc=\"Scraping posts\"):\n",
    "        try:\n",
    "            # Check if we already have this post\n",
    "            if post.id in already_scraped_ids:\n",
    "                duplicate_posts_skipped += 1\n",
    "                continue  # Skip posts we already have\n",
    "            \n",
    "            # Check if it's a gallery post and skip it\n",
    "            if is_gallery_post(post):\n",
    "                gallery_posts_skipped += 1\n",
    "                continue  # Skip gallery posts\n",
    "            \n",
    "            # 1. Download media and get local path\n",
    "            local_media_path = download_media(post)\n",
    "            \n",
    "            # 2. Store all relevant data\n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'url': post.url,\n",
    "                'permalink': post.permalink,\n",
    "                'score': post.score,\n",
    "                'created_utc': post.created_utc,\n",
    "                'post_hint': getattr(post, 'post_hint', 'text_only'),\n",
    "                'local_media_path': local_media_path\n",
    "            }\n",
    "            all_posts_data.append(post_data)\n",
    "            already_scraped_ids.add(post.id)  # Mark as scraped\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Error processing post: {e}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"‚èπÔ∏è  INTERRUPTED BY USER\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Scraping stopped. Will save all posts collected so far...\\n\")\n",
    "\n",
    "finally:\n",
    "    # --- 4. Save Results (even if interrupted) ---\n",
    "    \n",
    "    if len(all_posts_data) == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No new posts were scraped in this session.\")\n",
    "        if len(df_existing) > 0:\n",
    "            print(f\"   Existing dataset: {len(df_existing)} posts\")\n",
    "            df_raw = df_existing  # Use existing data for next cell\n",
    "    else:\n",
    "        print(f\"\\nüìä Processing {len(all_posts_data)} newly scraped posts...\")\n",
    "        \n",
    "        # Convert new posts to DataFrame\n",
    "        df_new = pd.DataFrame(all_posts_data)\n",
    "        \n",
    "        # Combine with existing data\n",
    "        if len(df_existing) > 0:\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        else:\n",
    "            df_combined = df_new\n",
    "        \n",
    "        # Remove any duplicates (just in case)\n",
    "        df_combined = df_combined.drop_duplicates(subset=['id'], keep='first')\n",
    "        \n",
    "        # Save to CSV\n",
    "        df_combined.to_csv(RAW_DATA_CSV, index=False)\n",
    "        \n",
    "        print(f\"\\nüíæ Dataset updated!\")\n",
    "        print(f\"   New posts added:        {len(all_posts_data)}\")\n",
    "        print(f\"   Total posts in dataset: {len(df_combined)}\")\n",
    "        print(f\"   Saved to: {RAW_DATA_CSV}\")\n",
    "        \n",
    "        print(f\"\\nüìã Session statistics:\")\n",
    "        print(f\"   üö´ Gallery posts skipped:    {gallery_posts_skipped}\")\n",
    "        print(f\"   ‚ôªÔ∏è  Duplicate posts skipped:  {duplicate_posts_skipped}\")\n",
    "        print(f\"   ‚úÖ New posts added:          {len(all_posts_data)}\")\n",
    "        \n",
    "        # Store the combined dataframe for the next cell\n",
    "        df_raw = df_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37531b7e",
   "metadata": {},
   "source": [
    "### Data Summary & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure df_raw is loaded\n",
    "if 'df_raw' not in locals() or df_raw is None:\n",
    "    try:\n",
    "        df_raw = pd.read_csv(RAW_DATA_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå No data found. Please run the scraping cell first.\")\n",
    "        df_raw = pd.DataFrame()\n",
    "\n",
    "if len(df_raw) == 0:\n",
    "    print(\"‚ùå Dataset is empty. Please run the scraping cell first.\")\n",
    "else:\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä DATA COLLECTION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüö´ Gallery posts skipped:     {gallery_posts_skipped}\")\n",
    "    print(f\"   (Gallery posts contain multiple images and are excluded from analysis)\")\n",
    "    \n",
    "    print(\"\\n--- Sample of Collected Data ---\")\n",
    "    display(df_raw.head())\n",
    "    \n",
    "    print(\"\\n--- Media Type Breakdown ---\")\n",
    "    media_counts = df_raw['post_hint'].value_counts()\n",
    "    print(media_counts)\n",
    "    \n",
    "    print(\"\\n--- Downloaded Media Statistics ---\")\n",
    "    total_posts = len(df_raw)\n",
    "    posts_with_media = df_raw['local_media_path'].notna().sum()\n",
    "    posts_without_media = total_posts - posts_with_media\n",
    "    \n",
    "    print(f\"Total posts scraped:        {total_posts}\")\n",
    "    print(f\"Posts with local media:     {posts_with_media} ({posts_with_media/total_posts*100:.1f}%)\")\n",
    "    print(f\"Posts without media:        {posts_without_media} ({posts_without_media/total_posts*100:.1f}%)\")\n",
    "    \n",
    "    # Count by media type\n",
    "    images_count = len([p for p in df_raw['local_media_path'] if pd.notna(p) and 'images' in str(p)])\n",
    "    videos_count = len([p for p in df_raw['local_media_path'] if pd.notna(p) and 'videos' in str(p)])\n",
    "    \n",
    "    print(f\"\\n  Images downloaded:        {images_count}\")\n",
    "    print(f\"  Videos downloaded:        {videos_count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Step 1 Complete! You can now proceed to Step 2 (Labeling)\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Resumability & Expandability Features\n",
    "\n",
    "This notebook is designed to be **interruptible, resumable, and expandable**:\n",
    "\n",
    "### ‚úÖ Interruptible\n",
    "- Press `Kernel ‚Üí Interrupt` or the stop button (‚èπÔ∏è) at any time\n",
    "- All progress is saved automatically in the `finally` block\n",
    "- No data loss even if interrupted\n",
    "\n",
    "### ‚úÖ Resumable\n",
    "- Re-run the scraping cell anytime to continue\n",
    "- Automatically loads existing `raw_data.csv`\n",
    "- Skips posts that are already in the dataset (checks by `id`)\n",
    "- Only adds new posts\n",
    "\n",
    "### ‚úÖ Expandable\n",
    "- Run again tomorrow/next week to add more posts\n",
    "- Appends to existing dataset instead of overwriting\n",
    "- Perfect for building datasets over time\n",
    "- Handles duplicates automatically\n",
    "\n",
    "### üìä Example Usage Scenarios:\n",
    "\n",
    "**Scenario 1: Initial Collection**\n",
    "```\n",
    "Day 1: Scrape 1,200 posts ‚Üí 1,000 added (200 were galleries)\n",
    "Result: raw_data.csv has 1,000 posts\n",
    "```\n",
    "\n",
    "**Scenario 2: Interrupted & Resumed**\n",
    "```\n",
    "Day 1: Start scraping 1,200 posts\n",
    "       After 500 posts ‚Üí Keyboard interrupt!\n",
    "Result: raw_data.csv has 500 posts (saved)\n",
    "\n",
    "Day 1: Re-run the cell\n",
    "       Loads 500 existing posts\n",
    "       Skips first 500 duplicates\n",
    "       Adds next 700 posts\n",
    "Result: raw_data.csv has 1,200 posts total\n",
    "```\n",
    "\n",
    "**Scenario 3: Expanding Dataset Over Time**\n",
    "```\n",
    "Day 1: Scrape hot posts ‚Üí 1,000 posts added\n",
    "Day 2: Scrape hot posts again ‚Üí 200 new posts added (800 were duplicates)\n",
    "Day 7: Scrape hot posts again ‚Üí 150 new posts added\n",
    "Result: raw_data.csv grows from 1,000 ‚Üí 1,200 ‚Üí 1,350 posts\n",
    "```\n",
    "\n",
    "### üéØ Pro Tips:\n",
    "\n",
    "1. **For maximum data collection**: Run this weekly to capture different \"hot\" posts\n",
    "2. **For interruption recovery**: Just re-run - it knows where it left off\n",
    "3. **For dataset growth**: Change `subreddit.hot()` to `subreddit.new()` or `subreddit.top('month')` to get different posts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Checkpoint\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Set up project structure and directories\n",
    "- ‚úÖ Connected to Reddit API\n",
    "- ‚úÖ Scraped 1,000+ posts from r/BrawlStars\n",
    "- ‚úÖ Downloaded images and videos locally\n",
    "- ‚úÖ Saved data to `data/raw_data.csv`\n",
    "\n",
    "**Next step:**\n",
    "- üìù **Notebook 02**: AI-Powered Labeling with Gemini API\n",
    "\n",
    "**Files created:**\n",
    "- `data/raw_data.csv` - Contains post metadata and local media paths\n",
    "- `media/images/*` - Downloaded images\n",
    "- `media/videos/*` - Downloaded videos\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emocare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
