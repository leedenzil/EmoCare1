{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Setup & Data Collection\n",
    "\n",
    "This notebook handles:\n",
    "- **Step 0**: Environment setup, API configuration, and folder structure\n",
    "- **Step 1**: Scraping Reddit posts from r/BrawlStars and downloading media locally\n",
    "\n",
    "**Output**: `data/raw_data.csv` with post metadata and local media paths\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Preparation\n",
    "\n",
    "First, we set up our environment. This involves installing all necessary libraries, setting up our API keys, and creating the directories where we'll store our data and media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this in your terminal/virtual environment first)\n",
    "# pip install praw pmaw pandas requests google-generativeai scikit-learn transformers torch torchvision opencv-python-headless tqdm seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import praw                     # For Reddit API access\n",
    "from pmaw import PushshiftAPI   # For historical data scraping (unlimited posts)\n",
    "import pandas as pd             # For data manipulation\n",
    "import requests                 # For downloading files\n",
    "import os                       # For file/directory operations\n",
    "from tqdm.auto import tqdm      # For progress bars\n",
    "from datetime import datetime   # For date handling\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Keys & Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Replace the placeholder values with your actual API keys.\n",
    "\n",
    "**Best Practice**: Store these in a `.env` file and use `python-dotenv` to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef92bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API keys configured\n",
      "‚úÖ API keys configured (make sure you replaced the placeholders!)\n"
     ]
    }
   ],
   "source": [
    "# --- API Keys & Config ---\n",
    "# !! IMPORTANT: Replace with your actual API keys\n",
    "# Get Reddit API credentials at: https://www.reddit.com/prefs/apps\n",
    "# --- API Keys ---\n",
    "REDDIT_CLIENT_ID = \"ML_aby_GTgeEFA2tyA8ryw\"\n",
    "REDDIT_CLIENT_SECRET = \"KcHW88TKXNFgI2FyQYvPUy-ByOB1-g\"\n",
    "REDDIT_USER_AGENT = \"BrawlStars Sentiment Scraper v3.0 by /u/YOUR_USERNAME\"\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c\"\n",
    "\n",
    "print(\"‚úÖ API keys configured\")\n",
    "\n",
    "print(\"‚úÖ API keys configured (make sure you replaced the placeholders!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Constants & Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directory structure created:\n",
      "   üìÅ media\\images\n",
      "   üìÅ media\\videos\n",
      "   üìÅ data\n",
      "\n",
      "üìÑ Output will be saved to: data\\raw_data.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Project Constants ---\n",
    "SUBREDDIT_NAME = \"Brawlstars\"\n",
    "POST_LIMIT = 1200  # Scrape 1200 to aim for ~1000 good posts\n",
    "\n",
    "# --- File & Directory Setup ---\n",
    "MEDIA_DIR = \"media\"\n",
    "IMAGE_DIR = os.path.join(MEDIA_DIR, \"images\")\n",
    "VIDEO_DIR = os.path.join(MEDIA_DIR, \"videos\")\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "RAW_DATA_CSV = os.path.join(DATA_DIR, 'raw_data.csv')\n",
    "\n",
    "print(f\"‚úÖ Directory structure created:\")\n",
    "print(f\"   üìÅ {IMAGE_DIR}\")\n",
    "print(f\"   üìÅ {VIDEO_DIR}\")\n",
    "print(f\"   üìÅ {DATA_DIR}\")\n",
    "print(f\"\\nüìÑ Output will be saved to: {RAW_DATA_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection (Scraping and Downloading)\n",
    "\n",
    "Here, we connect to the Reddit API using PRAW, scrape the latest posts from r/BrawlStars, and‚Äîmost importantly‚Äîdownload the associated image or video for each post. We save the *local path* to this media in our DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Reddit API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Reddit API\n",
      "   User: Read-only access (script mode)\n"
     ]
    }
   ],
   "source": [
    "# Initialize PRAW (Reddit API client)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT,\n",
    ")\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    # This should return None for read-only (script) authentication\n",
    "    user = reddit.user.me()\n",
    "    print(f\"‚úÖ Connected to Reddit API\")\n",
    "    print(f\"   User: {user if user else 'Read-only access (script mode)'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Reddit: {e}\")\n",
    "    print(\"   Please check your API credentials!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Media Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Media download function defined\n"
     ]
    }
   ],
   "source": [
    "def is_gallery_post(post):\n",
    "    \"\"\"\n",
    "    Check if a post is a gallery post (multiple images).\n",
    "    \n",
    "    Args:\n",
    "        post: A PRAW submission object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if post is a gallery, False otherwise\n",
    "    \"\"\"\n",
    "    # Gallery posts have gallery_data or media_metadata attributes\n",
    "    return hasattr(post, 'gallery_data') or hasattr(post, 'media_metadata')\n",
    "\n",
    "\n",
    "def download_media(post):\n",
    "    \"\"\"\n",
    "    Downloads the media (image or video) for a PRAW post and returns the local file path.\n",
    "    \n",
    "    Args:\n",
    "        post: A PRAW submission object\n",
    "        \n",
    "    Returns:\n",
    "        str: Local file path if media was downloaded, None otherwise\n",
    "    \"\"\"\n",
    "    # Skip gallery posts\n",
    "    if is_gallery_post(post):\n",
    "        return None\n",
    "    \n",
    "    post_hint = getattr(post, 'post_hint', None)\n",
    "    media_url = None\n",
    "    local_path = None\n",
    "    file_ext = \".unknown\"\n",
    "\n",
    "    try:\n",
    "        if post_hint == 'image':\n",
    "            media_url = post.url\n",
    "            file_ext = os.path.splitext(media_url)[1]\n",
    "            if not file_ext:\n",
    "                file_ext = \".jpg\"  # Default for images without clear extension\n",
    "            local_path = os.path.join(IMAGE_DIR, f\"{post.id}{file_ext}\")\n",
    "\n",
    "        elif post_hint == 'hosted:video':\n",
    "            media_url = post.media['reddit_video']['fallback_url']\n",
    "            file_ext = \".mp4\"\n",
    "            local_path = os.path.join(VIDEO_DIR, f\"{post.id}{file_ext}\")\n",
    "        \n",
    "        elif post_hint == 'rich:video':\n",
    "            # These are often YouTube links, etc. We'll skip downloading them for now.\n",
    "            # You could use youtube-dlp if you want to handle these.\n",
    "            pass\n",
    "\n",
    "        # If we have a URL and a path, download the file\n",
    "        if media_url and local_path:\n",
    "            if os.path.exists(local_path):\n",
    "                return local_path  # Already downloaded\n",
    "\n",
    "            response = requests.get(media_url, stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            return local_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # Silently fail for individual posts\n",
    "        return None\n",
    "    \n",
    "    return None  # No downloadable media\n",
    "\n",
    "print(\"‚úÖ Media download function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Posts and Download Media\n",
    "\n",
    "This cell will:\n",
    "1. Fetch posts from r/BrawlStars\n",
    "2. Download any associated media (images/videos)\n",
    "3. Store all data in a DataFrame\n",
    "4. Save to CSV\n",
    "\n",
    "‚è±Ô∏è **This may take 10-20 minutes depending on your connection speed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ STARTING DATA COLLECTION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Loaded 2412 previously scraped posts\n",
      "\n",
      "üìä Scraping configuration:\n",
      "   Target posts to fetch: 1200\n",
      "   Already in dataset:    2412\n",
      "   Gallery posts will be skipped\n",
      "\n",
      "‚èπÔ∏è  Press 'Kernel ‚Üí Interrupt' to stop at any time (progress is saved)\n",
      "\n",
      "======================================================================\n",
      "üîç Fetching posts from r/Brawlstars...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e59b45f5bb499e812b26d7b85f854e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping posts:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing 648 newly scraped posts...\n",
      "\n",
      "üíæ Dataset updated!\n",
      "   New posts added:        648\n",
      "   Total posts in dataset: 3060\n",
      "   Saved to: data\\raw_data.csv\n",
      "\n",
      "üìã Session statistics:\n",
      "   üö´ Gallery posts skipped:    166\n",
      "   ‚ôªÔ∏è  Duplicate posts skipped:  184\n",
      "   ‚úÖ New posts added:          648\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING DATA COLLECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- 1. Load Existing Data (if any) ---\n",
    "try:\n",
    "    df_existing = pd.read_csv(RAW_DATA_CSV)\n",
    "    already_scraped_ids = set(df_existing['id'])\n",
    "    print(f\"\\n‚úÖ Loaded {len(df_existing)} previously scraped posts\")\n",
    "except FileNotFoundError:\n",
    "    df_existing = pd.DataFrame()\n",
    "    already_scraped_ids = set()\n",
    "    print(f\"\\nüìù No existing raw data found. Starting from scratch.\")\n",
    "\n",
    "# --- 2. Initialize Scraping ---\n",
    "print(f\"\\nüìä Scraping configuration:\")\n",
    "print(f\"   Target posts to fetch: {POST_LIMIT}\")\n",
    "print(f\"   Already in dataset:    {len(already_scraped_ids)}\")\n",
    "print(f\"   Gallery posts will be skipped\")\n",
    "print(f\"\\n‚èπÔ∏è  Press 'Kernel ‚Üí Interrupt' to stop at any time (progress is saved)\\n\")\n",
    "\n",
    "all_posts_data = []\n",
    "gallery_posts_skipped = 0\n",
    "duplicate_posts_skipped = 0\n",
    "subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
    "\n",
    "# --- 3. Scrape Posts (Interruptible Loop) ---\n",
    "try:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üîç Fetching posts from r/{SUBREDDIT_NAME}...\\n\")\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for post in tqdm(subreddit.hot(limit=POST_LIMIT), total=POST_LIMIT, desc=\"Scraping posts\"):\n",
    "        try:\n",
    "            # Check if we already have this post\n",
    "            if post.id in already_scraped_ids:\n",
    "                duplicate_posts_skipped += 1\n",
    "                continue  # Skip posts we already have\n",
    "            \n",
    "            # Check if it's a gallery post and skip it\n",
    "            if is_gallery_post(post):\n",
    "                gallery_posts_skipped += 1\n",
    "                continue  # Skip gallery posts\n",
    "            \n",
    "            # 1. Download media and get local path\n",
    "            local_media_path = download_media(post)\n",
    "            \n",
    "            # 2. Store all relevant data\n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'url': post.url,\n",
    "                'permalink': post.permalink,\n",
    "                'score': post.score,\n",
    "                'created_utc': post.created_utc,\n",
    "                'post_hint': getattr(post, 'post_hint', 'text_only'),\n",
    "                'local_media_path': local_media_path\n",
    "            }\n",
    "            all_posts_data.append(post_data)\n",
    "            already_scraped_ids.add(post.id)  # Mark as scraped\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Error processing post: {e}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"‚èπÔ∏è  INTERRUPTED BY USER\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Scraping stopped. Will save all posts collected so far...\\n\")\n",
    "\n",
    "finally:\n",
    "    # --- 4. Save Results (even if interrupted) ---\n",
    "    \n",
    "    if len(all_posts_data) == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  No new posts were scraped in this session.\")\n",
    "        if len(df_existing) > 0:\n",
    "            print(f\"   Existing dataset: {len(df_existing)} posts\")\n",
    "            df_raw = df_existing  # Use existing data for next cell\n",
    "    else:\n",
    "        print(f\"\\nüìä Processing {len(all_posts_data)} newly scraped posts...\")\n",
    "        \n",
    "        # Convert new posts to DataFrame\n",
    "        df_new = pd.DataFrame(all_posts_data)\n",
    "        \n",
    "        # Combine with existing data\n",
    "        if len(df_existing) > 0:\n",
    "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        else:\n",
    "            df_combined = df_new\n",
    "        \n",
    "        # Remove any duplicates (just in case)\n",
    "        df_combined = df_combined.drop_duplicates(subset=['id'], keep='first')\n",
    "        \n",
    "        # Save to CSV\n",
    "        df_combined.to_csv(RAW_DATA_CSV, index=False)\n",
    "        \n",
    "        print(f\"\\nüíæ Dataset updated!\")\n",
    "        print(f\"   New posts added:        {len(all_posts_data)}\")\n",
    "        print(f\"   Total posts in dataset: {len(df_combined)}\")\n",
    "        print(f\"   Saved to: {RAW_DATA_CSV}\")\n",
    "        \n",
    "        print(f\"\\nüìã Session statistics:\")\n",
    "        print(f\"   üö´ Gallery posts skipped:    {gallery_posts_skipped}\")\n",
    "        print(f\"   ‚ôªÔ∏è  Duplicate posts skipped:  {duplicate_posts_skipped}\")\n",
    "        print(f\"   ‚úÖ New posts added:          {len(all_posts_data)}\")\n",
    "        \n",
    "        # Store the combined dataframe for the next cell\n",
    "        df_raw = df_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Summary & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä DATA COLLECTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "üö´ Gallery posts skipped:     161\n",
      "   (Gallery posts contain multiple images and are excluded from analysis)\n",
      "\n",
      "--- Sample of Collected Data ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>local_media_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1oeqxv1</td>\n",
       "      <td>Quick Questions, Loot, &amp; General Game Discussi...</td>\n",
       "      <td>Brawl Stars information resources: [Link](http...</td>\n",
       "      <td>https://www.reddit.com/r/Brawlstars/comments/1...</td>\n",
       "      <td>/r/Brawlstars/comments/1oeqxv1/quick_questions...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.761289e+09</td>\n",
       "      <td>text_only</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1nv6ldv</td>\n",
       "      <td>New monthly flairs! Please read before suggest...</td>\n",
       "      <td>Hello again, it‚Äôs been a minute since the last...</td>\n",
       "      <td>https://i.redd.it/9795cfioshsf1.jpeg</td>\n",
       "      <td>/r/Brawlstars/comments/1nv6ldv/new_monthly_fla...</td>\n",
       "      <td>356</td>\n",
       "      <td>1.759321e+09</td>\n",
       "      <td>image</td>\n",
       "      <td>media\\images\\1nv6ldv.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1oi44bf</td>\n",
       "      <td>I now understand why nobody got tick from thei...</td>\n",
       "      <td>They want us to buy it‚Ä¶.</td>\n",
       "      <td>https://i.redd.it/0kdqjns9ftxf1.jpeg</td>\n",
       "      <td>/r/Brawlstars/comments/1oi44bf/i_now_understan...</td>\n",
       "      <td>168</td>\n",
       "      <td>1.761641e+09</td>\n",
       "      <td>image</td>\n",
       "      <td>media\\images\\1oi44bf.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1ohsdgv</td>\n",
       "      <td>Ok now that the event is basically over, did a...</td>\n",
       "      <td>No one that I‚Äôve asked said they have the tick...</td>\n",
       "      <td>https://i.redd.it/yb4pnbcmbqxf1.jpeg</td>\n",
       "      <td>/r/Brawlstars/comments/1ohsdgv/ok_now_that_the...</td>\n",
       "      <td>963</td>\n",
       "      <td>1.761603e+09</td>\n",
       "      <td>image</td>\n",
       "      <td>media\\images\\1ohsdgv.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ohwlh8</td>\n",
       "      <td>Do they know I'm playing a fucking support bra...</td>\n",
       "      <td>Like they even have more healing than me wtf I...</td>\n",
       "      <td>https://i.redd.it/dpxb5mul8rxf1.jpeg</td>\n",
       "      <td>/r/Brawlstars/comments/1ohwlh8/do_they_know_im...</td>\n",
       "      <td>540</td>\n",
       "      <td>1.761615e+09</td>\n",
       "      <td>image</td>\n",
       "      <td>media\\images\\1ohwlh8.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  1oeqxv1  Quick Questions, Loot, & General Game Discussi...   \n",
       "1  1nv6ldv  New monthly flairs! Please read before suggest...   \n",
       "2  1oi44bf  I now understand why nobody got tick from thei...   \n",
       "3  1ohsdgv  Ok now that the event is basically over, did a...   \n",
       "4  1ohwlh8  Do they know I'm playing a fucking support bra...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Brawl Stars information resources: [Link](http...   \n",
       "1  Hello again, it‚Äôs been a minute since the last...   \n",
       "2                          They want us to buy it‚Ä¶.    \n",
       "3  No one that I‚Äôve asked said they have the tick...   \n",
       "4  Like they even have more healing than me wtf I...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/Brawlstars/comments/1...   \n",
       "1               https://i.redd.it/9795cfioshsf1.jpeg   \n",
       "2               https://i.redd.it/0kdqjns9ftxf1.jpeg   \n",
       "3               https://i.redd.it/yb4pnbcmbqxf1.jpeg   \n",
       "4               https://i.redd.it/dpxb5mul8rxf1.jpeg   \n",
       "\n",
       "                                           permalink  score   created_utc  \\\n",
       "0  /r/Brawlstars/comments/1oeqxv1/quick_questions...      3  1.761289e+09   \n",
       "1  /r/Brawlstars/comments/1nv6ldv/new_monthly_fla...    356  1.759321e+09   \n",
       "2  /r/Brawlstars/comments/1oi44bf/i_now_understan...    168  1.761641e+09   \n",
       "3  /r/Brawlstars/comments/1ohsdgv/ok_now_that_the...    963  1.761603e+09   \n",
       "4  /r/Brawlstars/comments/1ohwlh8/do_they_know_im...    540  1.761615e+09   \n",
       "\n",
       "   post_hint           local_media_path  \n",
       "0  text_only                        NaN  \n",
       "1      image  media\\images\\1nv6ldv.jpeg  \n",
       "2      image  media\\images\\1oi44bf.jpeg  \n",
       "3      image  media\\images\\1ohsdgv.jpeg  \n",
       "4      image  media\\images\\1ohwlh8.jpeg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Media Type Breakdown ---\n",
      "post_hint\n",
      "image           900\n",
      "hosted:video    199\n",
      "text_only       111\n",
      "rich:video        6\n",
      "self              3\n",
      "link              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Downloaded Media Statistics ---\n",
      "Total posts scraped:        1220\n",
      "Posts with local media:     1096 (89.8%)\n",
      "Posts without media:        124 (10.2%)\n",
      "\n",
      "  Images downloaded:        897\n",
      "  Videos downloaded:        199\n",
      "\n",
      "============================================================\n",
      "‚úÖ Step 1 Complete! You can now proceed to Step 2 (Labeling)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Make sure df_raw is loaded\n",
    "if 'df_raw' not in locals() or df_raw is None:\n",
    "    try:\n",
    "        df_raw = pd.read_csv(RAW_DATA_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå No data found. Please run the scraping cell first.\")\n",
    "        df_raw = pd.DataFrame()\n",
    "\n",
    "if len(df_raw) == 0:\n",
    "    print(\"‚ùå Dataset is empty. Please run the scraping cell first.\")\n",
    "else:\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä DATA COLLECTION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüö´ Gallery posts skipped:     {gallery_posts_skipped}\")\n",
    "    print(f\"   (Gallery posts contain multiple images and are excluded from analysis)\")\n",
    "    \n",
    "    print(\"\\n--- Sample of Collected Data ---\")\n",
    "    display(df_raw.head())\n",
    "    \n",
    "    print(\"\\n--- Media Type Breakdown ---\")\n",
    "    media_counts = df_raw['post_hint'].value_counts()\n",
    "    print(media_counts)\n",
    "    \n",
    "    print(\"\\n--- Downloaded Media Statistics ---\")\n",
    "    total_posts = len(df_raw)\n",
    "    posts_with_media = df_raw['local_media_path'].notna().sum()\n",
    "    posts_without_media = total_posts - posts_with_media\n",
    "    \n",
    "    print(f\"Total posts scraped:        {total_posts}\")\n",
    "    print(f\"Posts with local media:     {posts_with_media} ({posts_with_media/total_posts*100:.1f}%)\")\n",
    "    print(f\"Posts without media:        {posts_without_media} ({posts_without_media/total_posts*100:.1f}%)\")\n",
    "    \n",
    "    # Count by media type\n",
    "    images_count = len([p for p in df_raw['local_media_path'] if pd.notna(p) and 'images' in str(p)])\n",
    "    videos_count = len([p for p in df_raw['local_media_path'] if pd.notna(p) and 'videos' in str(p)])\n",
    "    \n",
    "    print(f\"\\n  Images downloaded:        {images_count}\")\n",
    "    print(f\"  Videos downloaded:        {videos_count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ Step 1 Complete! You can now proceed to Step 2 (Labeling)\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Resumability & Expandability Features\n",
    "\n",
    "This notebook is designed to be **interruptible, resumable, and expandable**:\n",
    "\n",
    "### ‚úÖ Interruptible\n",
    "- Press `Kernel ‚Üí Interrupt` or the stop button (‚èπÔ∏è) at any time\n",
    "- All progress is saved automatically in the `finally` block\n",
    "- No data loss even if interrupted\n",
    "\n",
    "### ‚úÖ Resumable\n",
    "- Re-run the scraping cell anytime to continue\n",
    "- Automatically loads existing `raw_data.csv`\n",
    "- Skips posts that are already in the dataset (checks by `id`)\n",
    "- Only adds new posts\n",
    "\n",
    "### ‚úÖ Expandable\n",
    "- Run again tomorrow/next week to add more posts\n",
    "- Appends to existing dataset instead of overwriting\n",
    "- Perfect for building datasets over time\n",
    "- Handles duplicates automatically\n",
    "\n",
    "### üìä Example Usage Scenarios:\n",
    "\n",
    "**Scenario 1: Initial Collection**\n",
    "```\n",
    "Day 1: Scrape 1,200 posts ‚Üí 1,000 added (200 were galleries)\n",
    "Result: raw_data.csv has 1,000 posts\n",
    "```\n",
    "\n",
    "**Scenario 2: Interrupted & Resumed**\n",
    "```\n",
    "Day 1: Start scraping 1,200 posts\n",
    "       After 500 posts ‚Üí Keyboard interrupt!\n",
    "Result: raw_data.csv has 500 posts (saved)\n",
    "\n",
    "Day 1: Re-run the cell\n",
    "       Loads 500 existing posts\n",
    "       Skips first 500 duplicates\n",
    "       Adds next 700 posts\n",
    "Result: raw_data.csv has 1,200 posts total\n",
    "```\n",
    "\n",
    "**Scenario 3: Expanding Dataset Over Time**\n",
    "```\n",
    "Day 1: Scrape hot posts ‚Üí 1,000 posts added\n",
    "Day 2: Scrape hot posts again ‚Üí 200 new posts added (800 were duplicates)\n",
    "Day 7: Scrape hot posts again ‚Üí 150 new posts added\n",
    "Result: raw_data.csv grows from 1,000 ‚Üí 1,200 ‚Üí 1,350 posts\n",
    "```\n",
    "\n",
    "### üéØ Pro Tips:\n",
    "\n",
    "1. **For maximum data collection**: Run this weekly to capture different \"hot\" posts\n",
    "2. **For interruption recovery**: Just re-run - it knows where it left off\n",
    "3. **For dataset growth**: Change `subreddit.hot()` to `subreddit.new()` or `subreddit.top('month')` to get different posts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Checkpoint\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Set up project structure and directories\n",
    "- ‚úÖ Connected to Reddit API\n",
    "- ‚úÖ Scraped 1,000+ posts from r/BrawlStars\n",
    "- ‚úÖ Downloaded images and videos locally\n",
    "- ‚úÖ Saved data to `data/raw_data.csv`\n",
    "\n",
    "**Next step:**\n",
    "- üìù **Notebook 02**: AI-Powered Labeling with Gemini API\n",
    "\n",
    "**Files created:**\n",
    "- `data/raw_data.csv` - Contains post metadata and local media paths\n",
    "- `media/images/*` - Downloaded images\n",
    "- `media/videos/*` - Downloaded videos\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
