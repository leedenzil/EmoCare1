{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Final Evaluation on Test Set\n",
    "\n",
    "This notebook evaluates the complete multimodal sentiment analysis system on the held-out test set.\n",
    "\n",
    "**Goal**: Measure the final performance on **completely unseen data** (625 test posts).\n",
    "\n",
    "**Why This Matters**: The test set has NEVER been seen during training or validation. This gives us an unbiased measure of how well the system will perform on real-world data.\n",
    "\n",
    "**Process**:\n",
    "1. Load all four trained models (text, image, video, fusion)\n",
    "2. Load the test set (625 posts)\n",
    "3. Extract embeddings from specialist models\n",
    "4. Run fusion model to get final predictions\n",
    "5. Compare predictions vs. ground truth\n",
    "6. Generate comprehensive evaluation report\n",
    "\n",
    "**Outputs**:\n",
    "- `results/final_evaluation/confusion_matrix.png`\n",
    "- `results/final_evaluation/test_metrics.json`\n",
    "- `results/final_evaluation/test_report.txt`\n",
    "- `results/final_evaluation/f1_scores_per_class.png`\n",
    "- `results/final_evaluation/model_comparison.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import tqdm for notebooks, fallback to regular tqdm\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"✓ Using notebook progress bars\")\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "    print(\"✓ Using terminal progress bars\")\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TEST_DATA = \"data/test_set.csv\"\n",
    "MODEL_DIR = \"models\"\n",
    "RESULTS_DIR = \"results/final_evaluation\"\n",
    "\n",
    "# Create results directory\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model paths\n",
    "TEXT_MODEL_PATH = f\"{MODEL_DIR}/text_specialist_best.pth\"\n",
    "IMAGE_MODEL_PATH = f\"{MODEL_DIR}/image_specialist_best.pth\"\n",
    "VIDEO_MODEL_PATH = f\"{MODEL_DIR}/video_specialist_best.pth\"\n",
    "FUSION_MODEL_PATH = f\"{MODEL_DIR}/fusion_model_best.pth\"\n",
    "\n",
    "# Model parameters\n",
    "TEXT_MODEL_NAME = 'distilbert-base-uncased'\n",
    "CLIP_MODEL_NAME = 'openai/clip-vit-base-patch32'\n",
    "MAX_LENGTH = 128\n",
    "NUM_FRAMES = 8\n",
    "\n",
    "# Embedding dimensions\n",
    "TEXT_EMBED_DIM = 768\n",
    "IMAGE_EMBED_DIM = 512\n",
    "VIDEO_EMBED_DIM = 512\n",
    "TOTAL_EMBED_DIM = TEXT_EMBED_DIM + IMAGE_EMBED_DIM + VIDEO_EMBED_DIM\n",
    "\n",
    "# Sentiment labels\n",
    "LABELS = ['Anger', 'Joy', 'Neutral/Other', 'Sadness', 'Surprise']\n",
    "LABEL_TO_ID = {label: idx for idx, label in enumerate(LABELS)}\n",
    "ID_TO_LABEL = {idx: label for label, idx in LABEL_TO_ID.items()}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Model\n",
    "class TextSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5):\n",
    "        super(TextSentimentClassifier, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(TEXT_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        \n",
    "    def get_embedding(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "# Image Model\n",
    "class ImageSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5):\n",
    "        super(ImageSentimentClassifier, self).__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(CLIP_MODEL_NAME)\n",
    "        self.vision_embed_dim = self.clip.vision_model.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vision_embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def get_embedding(self, pixel_values):\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "            image_embeds = vision_outputs.pooler_output\n",
    "        return image_embeds\n",
    "\n",
    "\n",
    "# Video Model\n",
    "class VideoSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=5, num_frames=8):\n",
    "        super(VideoSentimentClassifier, self).__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(CLIP_MODEL_NAME)\n",
    "        self.num_frames = num_frames\n",
    "        self.vision_embed_dim = self.clip.vision_model.config.hidden_size\n",
    "        \n",
    "        self.temporal_attention = nn.Sequential(\n",
    "            nn.Linear(self.vision_embed_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.vision_embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    \n",
    "    def get_embedding(self, pixel_values):\n",
    "        with torch.no_grad():\n",
    "            batch_size, num_frames, C, H, W = pixel_values.shape\n",
    "            pixel_values = pixel_values.view(batch_size * num_frames, C, H, W)\n",
    "            \n",
    "            vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n",
    "            frame_embeds = vision_outputs.pooler_output\n",
    "            frame_embeds = frame_embeds.view(batch_size, num_frames, -1)\n",
    "            \n",
    "            attention_scores = self.temporal_attention(frame_embeds)\n",
    "            attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "            video_embed = torch.sum(frame_embeds * attention_weights, dim=1)\n",
    "        \n",
    "        return video_embed\n",
    "\n",
    "\n",
    "# Fusion Model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes=5):\n",
    "        super(FusionModel, self).__init__()\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fusion(x)\n",
    "\n",
    "print(\"✓ Model classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading all trained models...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Text model\n",
    "text_model = TextSentimentClassifier(n_classes=len(LABELS))\n",
    "text_model.load_state_dict(torch.load(TEXT_MODEL_PATH, map_location=device))\n",
    "text_model = text_model.to(device)\n",
    "text_model.eval()\n",
    "print(f\"✓ Text model loaded from {TEXT_MODEL_PATH}\")\n",
    "\n",
    "# Image model\n",
    "image_model = ImageSentimentClassifier(n_classes=len(LABELS))\n",
    "image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=device))\n",
    "image_model = image_model.to(device)\n",
    "image_model.eval()\n",
    "print(f\"✓ Image model loaded from {IMAGE_MODEL_PATH}\")\n",
    "\n",
    "# Video model\n",
    "video_model = VideoSentimentClassifier(n_classes=len(LABELS), num_frames=NUM_FRAMES)\n",
    "video_model.load_state_dict(torch.load(VIDEO_MODEL_PATH, map_location=device))\n",
    "video_model = video_model.to(device)\n",
    "video_model.eval()\n",
    "print(f\"✓ Video model loaded from {VIDEO_MODEL_PATH}\")\n",
    "\n",
    "# Fusion model\n",
    "fusion_model = FusionModel(input_dim=TOTAL_EMBED_DIM, n_classes=len(LABELS))\n",
    "fusion_model.load_state_dict(torch.load(FUSION_MODEL_PATH, map_location=device))\n",
    "fusion_model = fusion_model.to(device)\n",
    "fusion_model.eval()\n",
    "print(f\"✓ Fusion model loaded from {FUSION_MODEL_PATH}\")\n",
    "\n",
    "# Load processors\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
    "print(\"✓ Processors loaded\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"All models loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=8):\n",
    "    \"\"\"Extract evenly spaced frames from a video.\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            return None\n",
    "        \n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(frame_rgb)\n",
    "                frames.append(pil_image)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) < num_frames:\n",
    "            while len(frames) < num_frames:\n",
    "                frames.append(frames[-1] if frames else Image.new('RGB', (224, 224), color='black'))\n",
    "        \n",
    "        return frames[:num_frames]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Test Set and Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "print(\"Loading test set...\")\n",
    "test_df = pd.read_csv(TEST_DATA)\n",
    "print(f\"✓ Test set loaded: {len(test_df):,} samples\")\n",
    "\n",
    "print(\"\\nTest set sentiment distribution:\")\n",
    "print(test_df['post_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTING TEST SET EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_text_embeds = []\n",
    "test_image_embeds = []\n",
    "test_video_embeds = []\n",
    "test_labels = []\n",
    "\n",
    "for idx in tqdm(range(len(test_df)), desc=\"Processing test posts\"):\n",
    "    row = test_df.iloc[idx]\n",
    "    \n",
    "    # TEXT EMBEDDING\n",
    "    title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "    text = str(row['text']) if pd.notna(row['text']) else \"\"\n",
    "    combined_text = f\"{title} {text}\".strip()\n",
    "    \n",
    "    encoding = tokenizer(combined_text, max_length=MAX_LENGTH, padding='max_length',\n",
    "                        truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    text_embed = text_model.get_embedding(input_ids, attention_mask).squeeze().cpu().numpy()\n",
    "    \n",
    "    if text_embed.shape[0] != TEXT_EMBED_DIM:\n",
    "        text_embed = text_embed.flatten()[:TEXT_EMBED_DIM]\n",
    "        if len(text_embed) < TEXT_EMBED_DIM:\n",
    "            text_embed = np.pad(text_embed, (0, TEXT_EMBED_DIM - len(text_embed)))\n",
    "    test_text_embeds.append(text_embed)\n",
    "    \n",
    "    # IMAGE EMBEDDING\n",
    "    if row['media_type'] == 'image':\n",
    "        image_path = str(row['local_media_path']).replace('\\\\', '/')\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].to(device)\n",
    "            image_embed = image_model.get_embedding(pixel_values).squeeze().cpu().numpy()\n",
    "            \n",
    "            if image_embed.shape[0] != IMAGE_EMBED_DIM:\n",
    "                image_embed = image_embed.flatten()[:IMAGE_EMBED_DIM]\n",
    "                if len(image_embed) < IMAGE_EMBED_DIM:\n",
    "                    image_embed = np.pad(image_embed, (0, IMAGE_EMBED_DIM - len(image_embed)))\n",
    "        except:\n",
    "            image_embed = np.zeros(IMAGE_EMBED_DIM, dtype=np.float32)\n",
    "    else:\n",
    "        image_embed = np.zeros(IMAGE_EMBED_DIM, dtype=np.float32)\n",
    "    \n",
    "    if image_embed.shape != (IMAGE_EMBED_DIM,):\n",
    "        image_embed = np.zeros(IMAGE_EMBED_DIM, dtype=np.float32)\n",
    "    test_image_embeds.append(image_embed)\n",
    "    \n",
    "    # VIDEO EMBEDDING\n",
    "    if row['media_type'] == 'video':\n",
    "        video_path = str(row['local_media_path']).replace('\\\\', '/')\n",
    "        frames = extract_frames(video_path, NUM_FRAMES)\n",
    "        \n",
    "        if frames:\n",
    "            try:\n",
    "                pixel_values_list = []\n",
    "                for frame in frames:\n",
    "                    inputs = clip_processor(images=frame, return_tensors=\"pt\")\n",
    "                    pixel_values_list.append(inputs['pixel_values'].squeeze(0))\n",
    "                pixel_values = torch.stack(pixel_values_list).unsqueeze(0).to(device)\n",
    "                video_embed = video_model.get_embedding(pixel_values).squeeze().cpu().numpy()\n",
    "                \n",
    "                if video_embed.shape[0] != VIDEO_EMBED_DIM:\n",
    "                    video_embed = video_embed.flatten()[:VIDEO_EMBED_DIM]\n",
    "                    if len(video_embed) < VIDEO_EMBED_DIM:\n",
    "                        video_embed = np.pad(video_embed, (0, VIDEO_EMBED_DIM - len(video_embed)))\n",
    "            except:\n",
    "                video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "        else:\n",
    "            video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "    else:\n",
    "        video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "    \n",
    "    if video_embed.shape != (VIDEO_EMBED_DIM,):\n",
    "        video_embed = np.zeros(VIDEO_EMBED_DIM, dtype=np.float32)\n",
    "    test_video_embeds.append(video_embed)\n",
    "    \n",
    "    # LABEL\n",
    "    label = LABEL_TO_ID[row['post_sentiment']]\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Convert to arrays\n",
    "test_text_embeds = np.array(test_text_embeds, dtype=np.float32)\n",
    "test_image_embeds = np.array(test_image_embeds, dtype=np.float32)\n",
    "test_video_embeds = np.array(test_video_embeds, dtype=np.float32)\n",
    "test_labels = np.array(test_labels, dtype=np.int64)\n",
    "\n",
    "print(f\"\\n✓ Test embeddings extracted:\")\n",
    "print(f\"  Text: {test_text_embeds.shape}\")\n",
    "print(f\"  Image: {test_image_embeds.shape}\")\n",
    "print(f\"  Video: {test_video_embeds.shape}\")\n",
    "print(f\"  Labels: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate embeddings\n",
    "test_combined_embeds = np.concatenate([test_text_embeds, test_image_embeds, test_video_embeds], axis=1)\n",
    "print(f\"Combined test embeddings shape: {test_combined_embeds.shape}\")\n",
    "\n",
    "# Convert to tensors\n",
    "test_embeds_tensor = torch.FloatTensor(test_combined_embeds).to(device)\n",
    "\n",
    "# Run predictions\n",
    "print(\"\\nRunning predictions on test set...\")\n",
    "with torch.no_grad():\n",
    "    outputs = fusion_model(test_embeds_tensor)\n",
    "    _, predictions = torch.max(outputs, dim=1)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "\n",
    "print(f\"✓ Predictions complete: {len(predictions)} predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "test_accuracy = accuracy_score(test_labels, predictions)\n",
    "test_f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL TEST SET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  Weighted F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(test_labels, predictions, target_names=LABELS, digits=4)\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "precision, recall, f1_per_class, support = precision_recall_fscore_support(\n",
    "    test_labels, predictions, labels=range(len(LABELS))\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(f\"{'Sentiment':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "print(\"-\" * 65)\n",
    "for idx, label in enumerate(LABELS):\n",
    "    print(f\"{label:<15} {precision[idx]:<10.4f} {recall[idx]:<10.4f} {f1_per_class[idx]:<10.4f} {support[idx]:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed metrics JSON\n",
    "test_metrics = {\n",
    "    \"overall\": {\n",
    "        \"accuracy\": float(test_accuracy),\n",
    "        \"weighted_f1\": float(test_f1),\n",
    "        \"test_samples\": int(len(test_labels))\n",
    "    },\n",
    "    \"per_class\": {}\n",
    "}\n",
    "\n",
    "for idx, label in enumerate(LABELS):\n",
    "    test_metrics[\"per_class\"][label] = {\n",
    "        \"precision\": float(precision[idx]),\n",
    "        \"recall\": float(recall[idx]),\n",
    "        \"f1_score\": float(f1_per_class[idx]),\n",
    "        \"support\": int(support[idx])\n",
    "    }\n",
    "\n",
    "with open(f\"{RESULTS_DIR}/test_metrics.json\", 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "print(f\"✓ Test metrics saved to {RESULTS_DIR}/test_metrics.json\")\n",
    "\n",
    "# Save text report\n",
    "with open(f\"{RESULTS_DIR}/test_report.txt\", 'w') as f:\n",
    "    f.write(\"FINAL TEST SET EVALUATION REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(\"Multimodal Sentiment Analysis System\\n\")\n",
    "    f.write(\"Brawl Stars Reddit Posts\\n\\n\")\n",
    "    f.write(f\"Test Set Size: {len(test_labels)} samples\\n\")\n",
    "    f.write(f\"Overall Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\\n\")\n",
    "    f.write(f\"Weighted F1 Score: {test_f1:.4f}\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(report)\n",
    "print(f\"✓ Test report saved to {RESULTS_DIR}/test_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='RdYlGn',\n",
    "    xticklabels=LABELS,\n",
    "    yticklabels=LABELS,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Final Test Set Evaluation', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Confusion matrix saved to {RESULTS_DIR}/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class F1 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(LABELS, f1_per_class, color=['#e74c3c', '#f39c12', '#95a5a6', '#3498db', '#9b59b6'])\n",
    "plt.xlabel('Sentiment', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('Per-Class F1 Scores - Final Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, f1_per_class):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{score:.3f}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_DIR}/f1_scores_per_class.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ F1 scores plot saved to {RESULTS_DIR}/f1_scores_per_class.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFiles Generated:\")\n",
    "print(f\"  1. Test metrics: {RESULTS_DIR}/test_metrics.json\")\n",
    "print(f\"  2. Test report: {RESULTS_DIR}/test_report.txt\")\n",
    "print(f\"  3. Confusion matrix: {RESULTS_DIR}/confusion_matrix.png\")\n",
    "print(f\"  4. F1 scores: {RESULTS_DIR}/f1_scores_per_class.png\")\n",
    "\n",
    "print(\"\\nFinal Performance Summary:\")\n",
    "print(f\"  Test Set Size: {len(test_labels)} posts\")\n",
    "print(f\"  Overall Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  Weighted F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores:\")\n",
    "for label, f1 in zip(LABELS, f1_per_class):\n",
    "    print(f\"  {label:15s}: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIMODAL SENTIMENT ANALYSIS SYSTEM - EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext Step: Use the system for predictions on new data (Step 7)\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
