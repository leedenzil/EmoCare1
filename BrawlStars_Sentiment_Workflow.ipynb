{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Sentiment Analysis for r/BrawlStars\n",
    "\n",
    "This notebook implements the complete, end-to-end workflow for building a multimodal (text, image, video) sentiment prediction model. We will follow the 7-step process outlined, from raw data collection to a final, usable prediction pipeline.\n",
    "\n",
    "**Workflow Overview:**\n",
    "\n",
    "1.  **Step 0: Preparation:** Install libraries, set API keys, create folders.\n",
    "2.  **Step 1: Data Collection:** Scrape Reddit data and download all media locally.\n",
    "3.  **Step 2: AI-Powered Labeling:** Use the Gemini API to create the \"golden dataset.\"\n",
    "4.  **Step 3: Data Splitting:** Create `train`, `validation`, and `test` sets.\n",
    "5.  **Step 4: Phase 1 Training:** Fine-tune specialist models (Text, Image, Video).\n",
    "6.  **Step 5: Phase 2 Training:** Train the fusion model on embeddings.\n",
    "7.  **Step 6: Evaluation:** Test the final system on unseen data.\n",
    "8.  **Step 7: Prediction:** Build the final inference function for new, raw posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Preparation\n",
    "\n",
    "First, we set up our environment. This involves installing all necessary libraries, setting up our API keys, and creating the directories where we'll store our data and media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell in your terminal within your virtual environment:\n",
    "\n",
    "# !pip install praw pandas requests google-generativeai scikit-learn transformers torch torchvision opencv-python-headless tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import praw                          # For Reddit scraping\n",
    "import pandas as pd                  # For data manipulation\n",
    "import requests                      # For downloading files\n",
    "import os                            # For file/directory operations\n",
    "import json                          # For handling API responses\n",
    "from tqdm.auto import tqdm           # For progress bars\n",
    "import google.generativeai as genai  # For Gemini API\n",
    "from sklearn.model_selection import train_test_split # For splitting data\n",
    "import cv2                           # For video processing (if needed)\n",
    "import time\n",
    "\n",
    "# --- API Keys & Config ---\n",
    "# !! IMPORTANT: Replace with your actual API keys\n",
    "# !! Best practice: Store these in a .env file and use python-dotenv to load them\n",
    "REDDIT_CLIENT_ID = \"YOUR_CLIENT_ID_HERE\"\n",
    "REDDIT_CLIENT_SECRET = \"YOUR_CLIENT_SECRET_HERE\"\n",
    "REDDIT_USER_AGENT = \"BrawlStars Sentiment Scraper v1.0 by /u/YOUR_USERNAME\"\n",
    "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"\n",
    "\n",
    "# --- Project Constants ---\n",
    "SUBREDDIT_NAME = \"Brawlstars\"\n",
    "POST_LIMIT = 1200  # Scrape 1200 to aim for ~1000 good posts\n",
    "LABEL_TARGET = 1000\n",
    "\n",
    "# --- File & Directory Setup ---\n",
    "MEDIA_DIR = \"media\"\n",
    "IMAGE_DIR = os.path.join(MEDIA_DIR, \"images\")\n",
    "VIDEO_DIR = os.path.join(MEDIA_DIR, \"videos\")\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "RAW_DATA_CSV = os.path.join(DATA_DIR, 'raw_data.csv')\n",
    "LABELED_DATA_CSV = os.path.join(DATA_DIR, 'labeled_data_1k.csv')\n",
    "TRAIN_SET_CSV = os.path.join(DATA_DIR, 'train_set.csv')\n",
    "VALIDATION_SET_CSV = os.path.join(DATA_DIR, 'validation_set.csv')\n",
    "TEST_SET_CSV = os.path.join(DATA_DIR, 'test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection (Scraping and Downloading)\n",
    "\n",
    "Here, we connect to the Reddit API using PRAW, scrape the latest posts from r/BrawlStars, and—most importantly—download the associated image or video for each post. We save the *local path* to this media in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize PRAW (Reddit API client)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"ML_aby_GTgeEFA2tyA8ryw\",\n",
    "    client_secret=\"KcHW88TKXNFgI2FyQYvPUy-ByOB1-g\",\n",
    "    user_agent=REDDIT_USER_AGENT,\n",
    ")\n",
    "\n",
    "print(reddit.user.me()) # Should show 'None' if using read-only (script) auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_media(post):\n",
    "    \"\"\"\n",
    "    Downloads the media (image or video) for a PRAW post and returns the local file path.\n",
    "    \"\"\"\n",
    "    post_hint = getattr(post, 'post_hint', None)\n",
    "    media_url = None\n",
    "    local_path = None\n",
    "    file_ext = \".unknown\"\n",
    "\n",
    "    try:\n",
    "        if post_hint == 'image':\n",
    "            media_url = post.url\n",
    "            file_ext = os.path.splitext(media_url)[1]\n",
    "            if not file_ext:\n",
    "                file_ext = \".jpg\" # Default for images without clear extension\n",
    "            local_path = os.path.join(IMAGE_DIR, f\"{post.id}{file_ext}\")\n",
    "\n",
    "        elif post_hint == 'hosted:video':\n",
    "            media_url = post.media['reddit_video']['fallback_url']\n",
    "            file_ext = \".mp4\"\n",
    "            local_path = os.path.join(VIDEO_DIR, f\"{post.id}{file_ext}\")\n",
    "        \n",
    "        elif post_hint == 'rich:video':\n",
    "            # These are often YouTube links, etc. We'll skip downloading them for now.\n",
    "            # You could use youtube-dlp if you want to handle these.\n",
    "            pass\n",
    "\n",
    "        # If we have a URL and a path, download the file\n",
    "        if media_url and local_path:\n",
    "            if os.path.exists(local_path):\n",
    "                return local_path # Already downloaded\n",
    "\n",
    "            response = requests.get(media_url, stream=True)\n",
    "            response.raise_for_status() # Raise an exception for bad status codes\n",
    "            \n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            return local_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error downloading media for post {post.id}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None # No downloadable media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape 1200 posts from r/Brawlstars...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d548740b01a44a619c65bc1788185f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m subreddit \u001b[38;5;241m=\u001b[39m reddit\u001b[38;5;241m.\u001b[39msubreddit(SUBREDDIT_NAME)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Use tqdm for a progress bar\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m tqdm(subreddit\u001b[38;5;241m.\u001b[39mhot(limit\u001b[38;5;241m=\u001b[39mPOST_LIMIT), total\u001b[38;5;241m=\u001b[39mPOST_LIMIT):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# 1. Download media and get local path\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         local_media_path \u001b[38;5;241m=\u001b[39m download_media(post)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\praw\\models\\listing\\generator.py:66\u001b[0m, in \u001b[0;36mListingGenerator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\praw\\models\\listing\\generator.py:90\u001b[0m, in \u001b[0;36mListingGenerator._next_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exhausted:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_sublist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listing)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\praw\\util\\deprecate_args.py:46\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     40\u001b[0m     warn(\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\praw\\reddit.py:731\u001b[0m, in \u001b[0;36mReddit.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;129m@_deprecate_args\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     params: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    :param path: The path to fetch.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;124;03m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \n\u001b[0;32m    730\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objectify_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\praw\\reddit.py:514\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_objectify_request\u001b[39m(\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    497\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    498\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m \n\u001b[0;32m    512\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objector\u001b[38;5;241m.\u001b[39mobjectify(\n\u001b[1;32m--> 514\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\praw\\util\\deprecate_args.py:46\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[0;32m     40\u001b[0m     warn(\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_old_args, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\praw\\reddit.py:963\u001b[0m, in \u001b[0;36mReddit.request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(msg)\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\prawcore\\sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\prawcore\\sessions.py:234\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    232\u001b[0m retry_strategy_state\u001b[38;5;241m.\u001b[39msleep()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(data, method, params, url)\n\u001b[1;32m--> 234\u001b[0m response, saved_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_strategy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m do_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munauthorized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\prawcore\\sessions.py:186\u001b[0m, in \u001b[0;36mSession._make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    176\u001b[0m     data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Response, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rate_limiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_header_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m bytes) (rst-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:rem-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:used-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m ratelimit) at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m             response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m             time\u001b[38;5;241m.\u001b[39mtime(),\n\u001b[0;32m    206\u001b[0m         )\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\prawcore\\rate_limit.py:47\u001b[0m, in \u001b[0;36mRateLimiter.call\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelay()\n\u001b[0;32m     46\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m set_header_callback()\n\u001b[1;32m---> 47\u001b[0m response \u001b[38;5;241m=\u001b[39m request_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(response\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\prawcore\\requestor.py:68\u001b[0m, in \u001b[0;36mRequestor.request\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;241m*\u001b[39margs, timeout\u001b[38;5;241m=\u001b[39mtimeout \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:  \u001b[38;5;66;03m# noqa: BLE001\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestException(exc, args, kwargs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\urllib3\\connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Starting to scrape {POST_LIMIT} posts from r/{SUBREDDIT_NAME}...\")\n",
    "\n",
    "all_posts_data = []\n",
    "subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
    "\n",
    "# Use tqdm for a progress bar\n",
    "for post in tqdm(subreddit.hot(limit=POST_LIMIT), total=POST_LIMIT):\n",
    "    try:\n",
    "        # 1. Download media and get local path\n",
    "        local_media_path = download_media(post)\n",
    "        \n",
    "        # 2. Store all relevant data\n",
    "        post_data = {\n",
    "            'id': post.id,\n",
    "            'title': post.title,\n",
    "            'text': post.selftext,\n",
    "            'url': post.url, # The original URL (to image, or to post)\n",
    "            'permalink': post.permalink,\n",
    "            'score': post.score,\n",
    "            'created_utc': post.created_utc,\n",
    "            'post_hint': getattr(post, 'post_hint', 'text_only'),\n",
    "            'local_media_path': local_media_path # Our new, crucial column\n",
    "        }\n",
    "        all_posts_data.append(post_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing post {post.id}: {e}\")\n",
    "\n",
    "# 3. Convert to DataFrame and save\n",
    "df_raw = pd.DataFrame(all_posts_data)\n",
    "df_raw.to_csv(RAW_DATA_CSV, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully scraped and processed {len(df_raw)} posts.\")\n",
    "print(f\"Raw data saved to: {RAW_DATA_CSV}\")\n",
    "\n",
    "# Show a summary of what we collected\n",
    "print(\"\\n--- Data Summary ---\")\n",
    "print(df_raw.head())\n",
    "\n",
    "print(\"\\n--- Media Type Breakdown ---\")\n",
    "print(df_raw['post_hint'].value_counts())\n",
    "\n",
    "print(\"\\n--- Downloaded Media Check ---\")\n",
    "print(f\"{df_raw['local_media_path'].notna().sum()} posts have associated local media.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: AI-Powered Labeling (Creating the \"Golden Dataset\")\n",
    "\n",
    "Now we take our raw data and use the Gemini API to generate sentiment labels. We will define a function that takes a post's text and its *local media file*, sends them to the API, and parses the JSON response. We'll apply this to our first 1,000 posts.\n",
    "\n",
    "**Note:** This step will make many API calls and may take a long time and cost money. We'll start by processing just a few posts as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# Configure the Gemini API client\n",
    "try:\n",
    "    genai.configure(api_key=\"AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c\")\n",
    "    # Use the multimodal-capable model\n",
    "    gemini_model = genai.GenerativeModel('gemini-2.5-flash') # Or 'gemini-1.5-pro'\n",
    "    print(\"Gemini model configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Gemini: {e}. Please check your API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the detailed few-shot prompt for the API\n",
    "# NOTE: The JSON examples use {{ and }} to escape the braces.\n",
    "\n",
    "LABELING_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert sentiment analyst for the game Brawl Stars. Your task is to analyze a Reddit post (which may include text, an image, and/or a video) and provide a structured JSON output.\n",
    "\n",
    "Analyze the user's sentiment and categorize the post. The user's post content is provided first, followed by the media.\n",
    "\n",
    "The 5 possible 'post_sentiment' values are:\n",
    "1.  **Joy**: Happiness, excitement, pride (e.g., getting a new Brawler, winning a hard match, liking a new skin).\n",
    "2.  **Anger**: Frustration, rage, annoyance (e.g., losing to a specific Brawler, bad teammates, game bugs, matchmaking issues).\n",
    "3.  **Sadness**: Disappointment, grief (e.g., missing a shot, losing a high-stakes game, a favorite Brawler getting nerfed).\n",
    "4.  **Surprise**: Shock, disbelief (e.g., a sudden clutch play, an unexpected new feature, a rare bug).\n",
    "5.  **Neutral/Other**: Objective discussion, questions, news, or art that doesn't convey a strong emotion.\n",
    "\n",
    "The 6 possible 'post_classification' values are:\n",
    "1.  **Gameplay Clip**: A video or image showing a match, a specific play, or a replay.\n",
    "2.  **Meme/Humor**: A meme, joke, or funny edit.\n",
    "3.  **Discussion**: A text-based post asking a question or starting a conversation.\n",
    "4.  **Feedback/Rant**: A post providing feedback, suggestions, or complaining about the game.\n",
    "5.  **Art/Concept**: Fan art, skin concepts, or creative edits.\n",
    "6.  **Achievement/Loot**: A screenshot of a new Brawler unlock, a high rank, or a Starr Drop reward.\n",
    "\n",
    "--- EXAMPLES ---\n",
    "\n",
    "[EXAMPLE 1]\n",
    "Post Text: \"This is the 5th time I've lost to an Edgar in a row. FIX YOUR GAME SUPERCELL!!\"\n",
    "Post Media: \n",
    "Output:\n",
    "{{\n",
    "  \"post_classification\": \"Feedback/Rant\",\n",
    "  \"post_sentiment\": \"Anger\",\n",
    "  \"sentiment_analysis\": \"The user is clearly angry, using all-caps ('FIX YOUR GAME') and expressing frustration at repeatedly losing to a specific Brawler (Edgar). The defeat screen image confirms the loss.\"\n",
    "}}\n",
    "\n",
    "[EXAMPLE 2]\n",
    "Post Text: \"I CAN'T BELIEVE I FINALLY GOT HIM!!\"\n",
    "Post Media: \n",
    "Output:\n",
    "{{\n",
    "  \"post_classification\": \"Achievement/Loot\",\n",
    "  \"post_sentiment\": \"Joy\",\n",
    "  \"sentiment_analysis\": \"The user is excited and happy, indicated by the all-caps text and the celebratory nature of unlocking a new legendary Brawler, which is a rare event.\"\n",
    "}}\n",
    "\n",
    "[EXAMPLE 3]\n",
    "Post Text: \"Check out this insane 1v3 I pulled off with Mortis\"\n",
    "Post Media: [Video showing a fast-paced gameplay clip where the player (Mortis) defeats three opponents]\n",
    "Output:\n",
    "{{\n",
    "  \"post_classification\": \"Gameplay Clip\",\n",
    "  \"post_sentiment\": \"Joy\",\n",
    "  \"sentiment_analysis\": \"The user is proud and excited about their 'insane 1v3' play. This is a clear expression of joy and pride in their own skill. The video clip demonstrates the achievement.\"\n",
    "}}\n",
    "\n",
    "--- TASK ---\n",
    "\n",
    "Analyze the following post and provide ONLY the JSON output. Do not include '```json' or any other text outside the JSON block.\n",
    "\n",
    "[POST CONTENT]\n",
    "Title: {post_title}\n",
    "Text: {post_text}\n",
    "\n",
    "[POST MEDIA]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_label(post_row):\n",
    "    \"\"\"\n",
    "    Takes a row from the DataFrame, sends its text and local media to Gemini,\n",
    "    and returns the raw JSON string response.\n",
    "    \"\"\"\n",
    "    post_title = post_row['title']\n",
    "    post_text = post_row['text']\n",
    "    media_path = post_row['local_media_path']\n",
    "    post_hint = post_row['post_hint']\n",
    "\n",
    "    # 1. Format the text part of the prompt\n",
    "    prompt = LABELING_PROMPT_TEMPLATE.format(post_title=post_title, post_text=post_text)\n",
    "    \n",
    "    # 2. Prepare the media part\n",
    "    media_payload = []\n",
    "    if pd.notna(media_path) and os.path.exists(media_path):\n",
    "        try:\n",
    "            # Use genai.upload_file for persistent storage and retrieval\n",
    "            # This is more robust for video.\n",
    "            print(f\"Uploading {media_path}...\")\n",
    "            uploaded_file = genai.upload_file(path=media_path)\n",
    "            media_payload.append(uploaded_file)\n",
    "            \n",
    "            # Wait for the file to be processed, especially important for videos\n",
    "            while uploaded_file.state.name == \"PROCESSING\":\n",
    "                time.sleep(2)\n",
    "                uploaded_file = genai.get_file(uploaded_file.name)\n",
    "            \n",
    "            if uploaded_file.state.name == \"FAILED\":\n",
    "                print(f\"File upload failed: {media_path}\")\n",
    "                return {\"error\": \"File upload failed\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading file {media_path}: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "    else:\n",
    "        media_payload.append(\"No media provided.\")\n",
    "\n",
    "    # 3. Combine prompt and media and make the API call\n",
    "    try:\n",
    "        full_prompt = [prompt] + media_payload\n",
    "        response = gemini_model.generate_content(full_prompt)\n",
    "        \n",
    "        # Clean up the uploaded file(s) after getting the response\n",
    "        for file in media_payload:\n",
    "            if hasattr(file, 'name'):\n",
    "                genai.delete_file(file.name)\n",
    "        \n",
    "        # Return the clean text response, ready for JSON parsing\n",
    "        return response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call for post {post_row['id']}: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1014 previously labeled posts from data\\labeled_data_1k.csv.\n",
      "Found 500 new posts. Starting labeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64af62e522104fb48633a8a024646753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating AI Labels:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading media\\images\\1oi2nhp.gif...\n",
      "Error uploading file media\\images\\1oi2nhp.gif: <HttpError 400 when requesting https://generativelanguage.googleapis.com/upload/v1beta/files?key=AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c&alt=json&uploadType=resumable returned \"Unsupported MIME type: image/gif\". Details: \"Unsupported MIME type: image/gif\">\n",
      "Uploading media\\images\\1og62qx.gif...\n",
      "Error uploading file media\\images\\1og62qx.gif: <HttpError 400 when requesting https://generativelanguage.googleapis.com/upload/v1beta/files?key=AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c&alt=json&uploadType=resumable returned \"Unsupported MIME type: image/gif\". Details: \"Unsupported MIME type: image/gif\">\n",
      "Uploading media\\images\\1oh8d2k.gif...\n",
      "Error uploading file media\\images\\1oh8d2k.gif: <HttpError 400 when requesting https://generativelanguage.googleapis.com/upload/v1beta/files?key=AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c&alt=json&uploadType=resumable returned \"Unsupported MIME type: image/gif\". Details: \"Unsupported MIME type: image/gif\">\n",
      "Uploading media\\images\\1ofucfh.gif...\n",
      "Error uploading file media\\images\\1ofucfh.gif: <HttpError 400 when requesting https://generativelanguage.googleapis.com/upload/v1beta/files?key=AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c&alt=json&uploadType=resumable returned \"Unsupported MIME type: image/gif\". Details: \"Unsupported MIME type: image/gif\">\n",
      "Uploading media\\images\\1og120e.gif...\n",
      "Error uploading file media\\images\\1og120e.gif: <HttpError 400 when requesting https://generativelanguage.googleapis.com/upload/v1beta/files?key=AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c&alt=json&uploadType=resumable returned \"Unsupported MIME type: image/gif\". Details: \"Unsupported MIME type: image/gif\">\n",
      "Uploading media\\images\\1oe7lzp.gif...\n",
      "Error uploading file media\\images\\1oe7lzp.gif: <HttpError 400 when requesting https://generativelanguage.googleapis.com/upload/v1beta/files?key=AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c&alt=json&uploadType=resumable returned \"Unsupported MIME type: image/gif\". Details: \"Unsupported MIME type: image/gif\">\n",
      "Uploading media\\images\\1ofpdla.gif...\n",
      "Error uploading file media\\images\\1ofpdla.gif: <HttpError 400 when requesting https://generativelanguage.googleapis.com/upload/v1beta/files?key=AIzaSyDwY_B-gmATDWP80lryU3okDSVARnNRh0c&alt=json&uploadType=resumable returned \"Unsupported MIME type: image/gif\". Details: \"Unsupported MIME type: image/gif\">\n",
      "Uploading media\\images\\gk71kh.jpg...\n",
      "Error during Gemini API call for post gk71kh: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 10.658482427s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "Uploading media\\images\\l3f8p8.jpg...\n",
      "Error during Gemini API call for post l3f8p8: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
      "Please retry in 7.09228937s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "]\n",
      "Uploading media\\videos\\iutcyd.mp4...\n",
      "\n",
      "\n",
      "--- INTERRUPTED BY USER ---\n",
      "Labeling process stopped. Will parse and save completed posts...\n",
      "\n",
      "Parsing 9 newly labeled posts...\n",
      "Successfully parsed 0 new labels.\n",
      "9 new posts failed labeling and will be skipped.\n",
      "No new valid labels were parsed. File remains unchanged.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2 (Revised): AI-Powered Labeling (Interruptible & Resumable) ---\n",
    "\n",
    "# --- 1. Define How Many New Posts to Label in This Batch ---\n",
    "# This is NOT the total; it's the max you want to add *right now*.\n",
    "# You can stop early (⏹️) at any time.\n",
    "NEW_LABEL_TARGET = 500 \n",
    "\n",
    "# --- 2. Find Out What's Already Labeled ---\n",
    "try:\n",
    "    df_old_labeled = pd.read_csv(LABELED_DATA_CSV)\n",
    "    already_labeled_ids = set(df_old_labeled['id'])\n",
    "    print(f\"Loaded {len(df_old_labeled)} previously labeled posts from {LABELED_DATA_CSV}.\")\n",
    "except FileNotFoundError:\n",
    "    df_old_labeled = pd.DataFrame() # Create an empty one if it's the first run\n",
    "    already_labeled_ids = set()\n",
    "    print(f\"No existing file found at {LABELED_DATA_CSV}. Starting from scratch.\")\n",
    "\n",
    "# --- 3. Find Out What's Left to Label ---\n",
    "df_all_raw = pd.read_csv(RAW_DATA_CSV)\n",
    "\n",
    "# Filter out posts we've already labeled\n",
    "df_to_label = df_all_raw[~df_all_raw['id'].isin(already_labeled_ids)].copy()\n",
    "\n",
    "# Limit this run to the NEW_LABEL_TARGET\n",
    "df_to_label = df_to_label.head(NEW_LABEL_TARGET)\n",
    "\n",
    "if len(df_to_label) == 0:\n",
    "    print(\"No new posts to label. Your golden dataset is up to date with the raw data.\")\n",
    "else:\n",
    "    print(f\"Found {len(df_to_label)} new posts. Starting labeling...\")\n",
    "    \n",
    "    # --- 4. Label New Posts (Interruptible Loop) ---\n",
    "    df_to_label['gemini_raw_json'] = None # Create the column to fill\n",
    "    \n",
    "    try:\n",
    "        # Use iterrows() for a row-by-row, interruptible loop\n",
    "        for index, row in tqdm(df_to_label.iterrows(), total=len(df_to_label), desc=\"Generating AI Labels\"):\n",
    "            json_response = get_gemini_label(row)\n",
    "            \n",
    "            # Save result immediately to the DataFrame\n",
    "            df_to_label.at[index, 'gemini_raw_json'] = json_response\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n--- INTERRUPTED BY USER ---\")\n",
    "        print(\"Labeling process stopped. Will parse and save completed posts...\")\n",
    "\n",
    "    finally:\n",
    "        # --- 5. Process and Parse Whatever Was Completed ---\n",
    "        \n",
    "        # Filter to only the rows that were actually processed in this run\n",
    "        df_newly_processed = df_to_label.dropna(subset=['gemini_raw_json']).copy()\n",
    "        \n",
    "        if len(df_newly_processed) == 0:\n",
    "            print(\"No new posts were labeled in this session.\")\n",
    "        else:\n",
    "            print(f\"\\nParsing {len(df_newly_processed)} newly labeled posts...\")\n",
    "            \n",
    "            # --- JSON Parsing (from your original cell) ---\n",
    "            parsed_labels = []\n",
    "            for index, row in df_newly_processed.iterrows():\n",
    "                raw_json = row['gemini_raw_json']\n",
    "                try:\n",
    "                    # Clean up potential markdown blocks\n",
    "                    if isinstance(raw_json, str):\n",
    "                        clean_json_str = raw_json.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "                        data = json.loads(clean_json_str)\n",
    "                        parsed_labels.append({\n",
    "                            'id': row['id'],\n",
    "                            'post_classification': data.get('post_classification'),\n",
    "                            'post_sentiment': data.get('post_sentiment'),\n",
    "                            'sentiment_analysis': data.get('sentiment_analysis'),\n",
    "                            'labeling_error': None\n",
    "                        })\n",
    "                    else:\n",
    "                        # Handle error dictionaries returned from the function\n",
    "                        parsed_labels.append({\n",
    "                            'id': row['id'],\n",
    "                            'labeling_error': str(raw_json.get('error', 'Unknown error'))\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    parsed_labels.append({\n",
    "                        'id': row['id'],\n",
    "                        'labeling_error': str(e)\n",
    "                    })\n",
    "\n",
    "            # Convert parsed data to a DataFrame and merge with the newly processed info\n",
    "            df_labels = pd.DataFrame(parsed_labels)\n",
    "            df_new_labeled_final = pd.merge(df_newly_processed, df_labels, on='id', how='left')\n",
    "\n",
    "            # Drop the raw JSON column\n",
    "            df_new_labeled_final = df_new_labeled_final.drop(columns=['gemini_raw_json'])\n",
    "\n",
    "            # Filter out posts that had errors\n",
    "            df_new_golden = df_new_labeled_final[df_new_labeled_final['labeling_error'].isna()].copy()\n",
    "            df_errors = df_new_labeled_final[df_new_labeled_final['labeling_error'].notna()]\n",
    "\n",
    "            print(f\"Successfully parsed {len(df_new_golden)} new labels.\")\n",
    "            if len(df_errors) > 0:\n",
    "                print(f\"{len(df_errors)} new posts failed labeling and will be skipped.\")\n",
    "\n",
    "            # --- 6. Combine Old and New Datasets and Save ---\n",
    "            if len(df_new_golden) > 0:\n",
    "                # Concatenate the old labeled data with the new golden data\n",
    "                df_combined = pd.concat([df_old_labeled, df_new_golden], ignore_index=True)\n",
    "                \n",
    "                # Ensure column consistency (in case raw data had extra columns)\n",
    "                if not df_old_labeled.empty:\n",
    "                     df_combined = df_combined.reindex(columns=df_old_labeled.columns)\n",
    "                \n",
    "                # Save the final \"golden dataset\"\n",
    "                df_combined.to_csv(LABELED_DATA_CSV, index=False)\n",
    "                print(f\"\\nGolden dataset updated. Total labeled posts: {len(df_combined)}\")\n",
    "                print(f\"Saved to: {LABELED_DATA_CSV}\")\n",
    "\n",
    "                # Display results\n",
    "                print(\"\\n--- Updated Labeled Data Head ---\")\n",
    "                print(df_combined[['id', 'title', 'post_sentiment', 'post_classification']].tail())\n",
    "\n",
    "                print(\"\\n--- Updated Sentiment Distribution ---\")\n",
    "                print(df_combined['post_sentiment'].value_counts())\n",
    "            else:\n",
    "                print(\"No new valid labels were parsed. File remains unchanged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Splitting\n",
    "\n",
    "We split our `labeled_data_1k.csv` into three distinct sets: `train_set` (for training), `validation_set` (for tuning), and `test_set` (for final evaluation). We use **stratification** on the `post_sentiment` column to ensure all three sets have a similar distribution of emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1781 labeled posts from data\\labeled_data_1k.csv\n",
      "\n",
      "Data splitting complete:\n",
      "  Training set:   1424 rows -> data\\train_set.csv\n",
      "  Validation set: 178 rows -> data\\validation_set.csv\n",
      "  Test set:       179 rows -> data\\test_set.csv\n",
      "\n",
      "Training Set Sentiment Distribution:\n",
      "post_sentiment\n",
      "Neutral/Other    0.333567\n",
      "Joy              0.301264\n",
      "Anger            0.239466\n",
      "Surprise         0.068820\n",
      "Sadness          0.056882\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation Set Sentiment Distribution:\n",
      "post_sentiment\n",
      "Neutral/Other    0.331461\n",
      "Joy              0.303371\n",
      "Anger            0.241573\n",
      "Surprise         0.067416\n",
      "Sadness          0.056180\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Set Sentiment Distribution:\n",
      "post_sentiment\n",
      "Neutral/Other    0.335196\n",
      "Joy              0.301676\n",
      "Anger            0.240223\n",
      "Surprise         0.067039\n",
      "Sadness          0.055866\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load the golden dataset\n",
    "try:\n",
    "    df_labeled = pd.read_csv(LABELED_DATA_CSV)\n",
    "    print(f\"Loaded {len(df_labeled)} labeled posts from {LABELED_DATA_CSV}\")\n",
    "\n",
    "    # Define split sizes\n",
    "    TEST_SIZE = 0.10  # 10% for the final test set\n",
    "    VALIDATION_SIZE = 0.10 # 10% for the validation set (10% of original, which is ~11% of the remaining 90%)\n",
    "\n",
    "    # 1. Split off the test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df_labeled,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=42,\n",
    "        stratify=df_labeled['post_sentiment']\n",
    "    )\n",
    "\n",
    "    # 2. Split the remaining into train and validation\n",
    "    # We adjust the test_size to be relative to the *new* dataframe's size\n",
    "    relative_val_size = VALIDATION_SIZE / (1.0 - TEST_SIZE)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=relative_val_size,\n",
    "        random_state=42,\n",
    "        stratify=train_val_df['post_sentiment']\n",
    "    )\n",
    "\n",
    "    # 3. Save the splits\n",
    "    train_df.to_csv(TRAIN_SET_CSV, index=False)\n",
    "    val_df.to_csv(VALIDATION_SET_CSV, index=False)\n",
    "    test_df.to_csv(TEST_SET_CSV, index=False)\n",
    "\n",
    "    # 4. Report the results\n",
    "    print(\"\\nData splitting complete:\")\n",
    "    print(f\"  Training set:   {len(train_df)} rows -> {TRAIN_SET_CSV}\")\n",
    "    print(f\"  Validation set: {len(val_df)} rows -> {VALIDATION_SET_CSV}\")\n",
    "    print(f\"  Test set:       {len(test_df)} rows -> {TEST_SET_CSV}\")\n",
    "\n",
    "    print(\"\\nTraining Set Sentiment Distribution:\")\n",
    "    print(train_df['post_sentiment'].value_counts(normalize=True))\n",
    "\n",
    "    print(\"\\nValidation Set Sentiment Distribution:\")\n",
    "    print(val_df['post_sentiment'].value_counts(normalize=True))\n",
    "\n",
    "    print(\"\\nTest Set Sentiment Distribution:\")\n",
    "    print(test_df['post_sentiment'].value_counts(normalize=True))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Labeled data file not found at {LABELED_DATA_CSV}\")\n",
    "    print(\"Please run Step 2 successfully before running Step 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Phase 1 Training (Fine-Tuning the Specialists)\n",
    "\n",
    "This is where we build our three specialist models. Each model (Text, Image, Video) is fine-tuned *independently* to predict the **overall post sentiment**. This teaches them what features in their modality (e.g., words, pixels, motion) correspond to emotions like \"Joy\" or \"Anger\" in the context of Brawl Stars.\n",
    "\n",
    "**Note:** The code below provides the *structure* and *placeholders*. You will need to implement the detailed PyTorch/TensorFlow logic (Datasets, DataLoaders, model definitions, training loops, and evaluation loops) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 labels: ['Anger', 'Joy', 'Neutral/Other', 'Sadness', 'Surprise']\n",
      "Label map: {'Anger': 0, 'Joy': 1, 'Neutral/Other': 2, 'Sadness': 3, 'Surprise': 4}\n"
     ]
    }
   ],
   "source": [
    "# --- Common Setup for Phase 1 ---\n",
    "\n",
    "# TODO: Define your sentiment labels and map them to integers\n",
    "SENTIMENT_LABELS = sorted(train_df['post_sentiment'].unique())\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Found {NUM_LABELS} labels: {SENTIMENT_LABELS}\")\n",
    "print(f\"Label map: {label_to_id}\")\n",
    "\n",
    "# TODO: Set training parameters\n",
    "DEVICE = \"cuda\" # or \"cpu\"\n",
    "TEXT_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "IMAGE_MODEL_NAME = \"resnet-50\"\n",
    "VIDEO_MODEL_NAME = \"X\" # e.g., 'MCG-NJU/videomae-base'\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A: Text Model (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 1: Text Model Training ---\n",
      "Using device: cuda\n",
      "Found 5 labels: {'Anger': 0, 'Joy': 1, 'Neutral/Other': 2, 'Sadness': 3, 'Surprise': 4}\n",
      "Loading tokenizer and building datasets...\n",
      "Loading pre-trained model: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n",
      "\n",
      "--- Epoch 1 / 5 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c51a857460944e594365363faf3e3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.4010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfe59a9c5b2427ebc188ee5abe0b51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2306\n",
      "Validation Accuracy: 0.5449\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 2 / 5 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80378ba47797409686991f981914bbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.1471\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed56053bc84d4436b68d280a73aa9c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0568\n",
      "Validation Accuracy: 0.5899\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 3 / 5 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de272e0c38b04178b17223880f9efe84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.9199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129bd15a0991428cad564b5d0bf0b03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0269\n",
      "Validation Accuracy: 0.6124\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 4 / 5 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356af9f6a5374b3e82cdc64c3ee89670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.7645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd6b3db9a2d452ba54d1560dab0a11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0122\n",
      "Validation Accuracy: 0.6180\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 5 / 5 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318a6f789a8c414c9b8e4fe51fa3f3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cafaa754404c00bc3f2338e1f36969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0083\n",
      "Validation Accuracy: 0.6124\n",
      "\n",
      "--- Training Complete ---\n",
      "Best Validation Accuracy: 0.6180\n",
      "Model and tokenizer saved to: ./models/text_specialist\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4.A: Text Model (DistilBERT) ---\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"--- Starting Phase 1: Text Model Training ---\")\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MODEL_SAVE_PATH = \"./models/text_specialist\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5  # Start with 3-5, you can increase if needed\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 128 # Max token length for a post\n",
    "\n",
    "# Ensure model save directory exists\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Create Label Maps ---\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(f\"Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    print(\"Please run Step 3: Data Splitting before this cell.\")\n",
    "    # This will stop the cell if files aren't found\n",
    "    raise\n",
    "\n",
    "# Create sentiment label maps from the training data\n",
    "SENTIMENT_LABELS = sorted(train_df['post_sentiment'].unique())\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Found {NUM_LABELS} labels: {label_to_id}\")\n",
    "\n",
    "# --- 3. Define Custom PyTorch Dataset ---\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, label_map, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        # Combine title and text for a richer input\n",
    "        self.texts = (dataframe['title'].fillna('') + \" [SEP] \" + dataframe['text'].fillna('')).tolist()\n",
    "        self.labels = dataframe['post_sentiment'].map(label_map).tolist()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 4. Initialize Tokenizer, Datasets, and DataLoaders ---\n",
    "print(\"Loading tokenizer and building datasets...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = TextSentimentDataset(train_df, tokenizer, label_to_id, MAX_LEN)\n",
    "val_dataset = TextSentimentDataset(val_df, tokenizer, label_to_id, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# --- 5. Load Model, Optimizer, and Scheduler ---\n",
    "print(f\"Loading pre-trained model: {MODEL_NAME}\")\n",
    "text_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=NUM_LABELS\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(text_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0, # Default, no warmup\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 6. The Training & Validation Loop ---\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1} / {EPOCHS} ---\")\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    text_model.train()\n",
    "    total_train_loss = 0\n",
    "    train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in train_progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Clear old gradients\n",
    "        text_model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # Get loss and logits\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(text_model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    text_model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad(): # No need to calculate gradients\n",
    "        for batch in val_progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = text_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # --- Save the Best Model ---\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        print(\"New best model! Saving...\")\n",
    "        text_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Model and tokenizer saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B: Image Model (ResNet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 1: Image Model Training (CLIP) ---\n",
      "Using device: cuda\n",
      "Found 5 labels: {'Anger': 0, 'Joy': 1, 'Neutral/Other': 2, 'Sadness': 3, 'Surprise': 4}\n",
      "Loading processor and building datasets...\n",
      "\n",
      "--- DATASET SIZE WARNING ---\n",
      "Total training images: 1050\n",
      "Total validation images: 124\n",
      "This is very small and will overfit. This is expected.\n",
      "----------------------------\n",
      "\n",
      "Loading pre-trained model: openai/clip-vit-base-patch32\n",
      "--- Starting Training ---\n",
      "\n",
      "--- Epoch 1 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c22038e5a941d6a0e64d19f46cb009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.4993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd260f744e240d58f1e9c7565386a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.4263\n",
      "Validation Accuracy: 0.3468\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 2 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a0714315c745a39febc6fa550698e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.1480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5f747b3a854349840d4042a00e7d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.4017\n",
      "Validation Accuracy: 0.3871\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 3 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcebe6e798034ce3b97d1aea0bc36202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.5773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec88682c5adc41e19057bed803be91c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.5161\n",
      "Validation Accuracy: 0.4597\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 4 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34806af4890b4611a3376ceda96167b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52faa819469748d688d1c1217320b4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8580\n",
      "Validation Accuracy: 0.4113\n",
      "\n",
      "--- Epoch 5 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7db06ef3014f4a90ef5d8674b7af8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.1082\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26ac532e127456dabc94aa4d9495518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2240\n",
      "Validation Accuracy: 0.4032\n",
      "\n",
      "--- Epoch 6 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3d2628aad84b4daf6ce1e3e1789733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a778d99246734fb19856a16b704de16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.5157\n",
      "Validation Accuracy: 0.4274\n",
      "\n",
      "--- Epoch 7 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad923592910047e0a5a8020725cb5bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4ca8da940248a29f5e6c2bb9699b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6061\n",
      "Validation Accuracy: 0.4194\n",
      "\n",
      "--- Epoch 8 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374964a70010457da6e8168833c307ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c6ede9f7f9478c95de551ebb2b5b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6944\n",
      "Validation Accuracy: 0.4355\n",
      "\n",
      "--- Epoch 9 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a484e596a8174b7a949d33f37d5e0a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad0fd9156cd47a78331882567154eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6385\n",
      "Validation Accuracy: 0.4274\n",
      "\n",
      "--- Epoch 10 / 10 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256b0972afd5430885aff77e0453f3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d8cb3353d54863ba3cea69e0f71065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6321\n",
      "Validation Accuracy: 0.4355\n",
      "\n",
      "--- Training Complete ---\n",
      "Best Validation Accuracy: 0.4597\n",
      "Model weights (state_dict) saved to: ./models/image_specialist.pth\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4.B: Image Model (CLIP-Vision) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    CLIPImageProcessor, \n",
    "    CLIPVisionModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "print(\"--- Starting Phase 1: Image Model Training (CLIP) ---\")\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "# We use the 'vision' part of the CLIP model\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\" \n",
    "MODEL_SAVE_PATH = \"./models/image_specialist.pth\" # Save state dict\n",
    "BATCH_SIZE = 8 # Use a SMALLER batch size for images\n",
    "EPOCHS = 10 # Train for a few more epochs since the dataset is tiny\n",
    "LEARNING_RATE = 1e-5 # Use a smaller LR for fine-tuning vision models\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Re-create Label Maps ---\n",
    "# We MUST use the exact same label mapping as the text model\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(f\"Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    print(\"Please run Step 3: Data Splitting before this cell.\")\n",
    "    raise\n",
    "\n",
    "# Create sentiment label maps from the training data\n",
    "SENTIMENT_LABELS = sorted(train_df['post_sentiment'].unique())\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Found {NUM_LABELS} labels: {label_to_id}\")\n",
    "\n",
    "\n",
    "# --- 3. Define Custom Image Dataset ---\n",
    "class ImageSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, label_map):\n",
    "        self.processor = processor\n",
    "        self.label_map = label_map\n",
    "        \n",
    "        # --- IMPORTANT: Filter for posts that are images ---\n",
    "        self.data = dataframe[\n",
    "            (dataframe['post_hint'] == 'image') & \n",
    "            (dataframe['local_media_path'].notna())\n",
    "        ].copy()\n",
    "        \n",
    "        self.data['labels'] = self.data['post_sentiment'].map(self.label_map)\n",
    "        \n",
    "        self.paths = self.data['local_media_path'].tolist()\n",
    "        self.labels = self.data['labels'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        try:\n",
    "            # Open the image file\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load image {img_path}. Using a blank image. Error: {e}\")\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0)) # Return blank image\n",
    "            \n",
    "        # Process the image (resize, normalize, etc.)\n",
    "        processed_image = self.processor(\n",
    "            images=image, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # .pixel_values is a dictionary, we just want the tensor\n",
    "        # .squeeze() removes the batch dimension\n",
    "        pixel_values = processed_image['pixel_values'].squeeze() \n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 4. Define Custom Model with Classification Head ---\n",
    "# We must add our own classifier on top of the CLIP vision model\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        # Load the pre-trained vision model (the \"body\")\n",
    "        self.clip_vision_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Get the dimension of the embeddings (e.g., 768)\n",
    "        embedding_dim = self.clip_vision_model.config.hidden_size\n",
    "        \n",
    "        # Create a new classification head\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # Pass images through the CLIP model\n",
    "        outputs = self.clip_vision_model(\n",
    "            pixel_values=pixel_values\n",
    "        )\n",
    "        \n",
    "        # Get the pooled embedding (the [CLS] token equivalent)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Pass the embedding through our classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n",
    "            \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "        \n",
    "\n",
    "# --- 5. Initialize Processor, Datasets, and DataLoaders ---\n",
    "print(\"Loading processor and building datasets...\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = ImageSentimentDataset(train_df, image_processor, label_to_id)\n",
    "val_dataset = ImageSentimentDataset(val_df, image_processor, label_to_id)\n",
    "\n",
    "print(f\"\\n--- DATASET SIZE WARNING ---\")\n",
    "print(f\"Total training images: {len(train_dataset)}\")\n",
    "print(f\"Total validation images: {len(val_dataset)}\")\n",
    "print(\"This is very small and will overfit. This is expected.\")\n",
    "print(\"----------------------------\\n\")\n",
    "\n",
    "# If your dataset is tiny (e.g., < 10 images), this will fail.\n",
    "# You need more labeled data.\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"ERROR: No images found in training set. Cannot train image model.\")\n",
    "    print(\"Please label more data (especially image posts) and re-run Step 3.\")\n",
    "    raise ValueError(\"No training data for image model.\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# --- 6. Load Model, Optimizer, and Scheduler ---\n",
    "print(f\"Loading pre-trained model: {MODEL_NAME}\")\n",
    "image_model = CustomCLIPModel(MODEL_NAME, NUM_LABELS).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(image_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 7. The Training & Validation Loop ---\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1} / {EPOCHS} ---\")\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    image_model.train()\n",
    "    total_train_loss = 0\n",
    "    train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in train_progress_bar:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        image_model.zero_grad()\n",
    "\n",
    "        outputs = image_model(\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs['loss']\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(image_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    image_model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = image_model(\n",
    "                pixel_values=pixel_values,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "            \n",
    "    # Handle case where validation set might be empty\n",
    "    if len(val_loader) > 0:\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Save the Best Model ---\n",
    "        # We save the model's 'state_dict' (just the weights)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(\"New best model! Saving...\")\n",
    "            torch.save(image_model.state_dict(), MODEL_SAVE_PATH)\n",
    "    else:\n",
    "        print(\"Validation set is empty. Skipping validation.\")\n",
    "        # Save the model from the last epoch if no validation\n",
    "        torch.save(image_model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "if len(val_loader) > 0:\n",
    "    print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Model weights (state_dict) saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.C: Video Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f64872",
   "metadata": {},
   "source": [
    "YOUR_HUGGINGFACE_TOKEN_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42e92b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b8e9d8235f4b0faecaaafaa29e6ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 1: Video Model Training (VideoMAE) ---\n",
      "Using device: cuda\n",
      "Found 5 labels: {'Anger': 0, 'Joy': 1, 'Neutral/Other': 2, 'Sadness': 3, 'Surprise': 4}\n",
      "Loading processor from: MCG-NJU/videomae-base\n",
      "\n",
      "--- DATASET SIZE WARNING ---\n",
      "Total training videos: 212\n",
      "Total validation videos: 35\n",
      "This is very small. Overfitting is expected.\n",
      "----------------------------\n",
      "\n",
      "Loading model from: MCG-NJU/videomae-base-finetuned-kinetics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n",
      "\n",
      "--- Epoch 1 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61d08ffa93a4209831c91631bc72e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\transformers\\feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:256.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.4221\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c455416caf624b6783bd3cc8d8316ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6729\n",
      "Validation Accuracy: 0.3143\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 2 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a3ced5cca741c997f767a48d332daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.9355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f1cebc989d406cae230e47303469c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6697\n",
      "Validation Accuracy: 0.4000\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 3 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b6610b186d4816861cf6bf15889bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.4934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6a83f681ea4fcd89a98e8c7aa7e90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7797\n",
      "Validation Accuracy: 0.4000\n",
      "\n",
      "--- Epoch 4 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef6ef85b5204c968bbf750bca682ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.1935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b9570e650e4547bed8daff500b1efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.9104\n",
      "Validation Accuracy: 0.4286\n",
      "New best model! Saving...\n",
      "\n",
      "--- Epoch 5 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c192cdbf1ada45c6bb202ae3ea2cd359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cec892a3c34ad0999a13da30403046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2172\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 6 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f48ee80254240aaa6743a6b5b6ecc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5c5cb59aa040f9bc7408b15791c61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.4391\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 7 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0c8bd86d4f466d852279bceea82cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c90904cecec4457ba3add66d1f1f585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.3644\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 8 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eee9a79b3d4f8d980437d6392c7cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e713f6b86f47f88ba0ea84f0464a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.4161\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 9 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5dad22d92746d69ccc39e39ecfc43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f9b1725f1245969c5cace388948bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.4680\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 10 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3570e6778b94cebbc4116f164ebab54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91feeec3a3704720aa5bfa356c9ce551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6306\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 11 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b352bef15ab459fb9dc2f3f6200f491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52ceaf4f1fd4e8eab6e03f621ab208c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6588\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 12 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133d08c2fed24a5290aa35331607f54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935a9632ab0d40a0962b8e8e96aca304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6777\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 13 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d58d475b6cf47c5a73d846961f19180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c9b1c8b5a34a2daf5123b1a17dc0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7365\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 14 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67daadd62604d0c87ea0930941b825f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86f401b2d66473ea3a7499b5f8672be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7537\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 15 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cae177c59114115873169d04d0dcbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f4fcb517de4f7a9c708d17fe662de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7539\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 16 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f893ba1965466aaefc0e5641d23124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde655683ac94a3dae042cbf2f2ba94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7741\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 17 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2434f32039384914b6250c1c78dc11d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad7bc14817c4cc9a7439f1c4831d16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7875\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 18 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37adf574abcf4f119d7feaa52950a28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e76dbf67114d1785589805d34f39c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.7976\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 19 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c6cdb11d284d5d92b4f68b0194ddc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4183470cd4a40bb9c30a10dda900e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.8061\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Epoch 20 / 20 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a285176719e24147855182d2ed85373e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.0008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402c0f55d1c7477db268e8dbe13f6089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.8089\n",
      "Validation Accuracy: 0.3714\n",
      "\n",
      "--- Training Complete ---\n",
      "Best Validation Accuracy: 0.4286\n",
      "Model and processor saved to: ./models/video_specialist\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4.C: Video Model (VideoMAE) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEForVideoClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import cv2  # OpenCV for video processing\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting Phase 1: Video Model Training (VideoMAE) ---\")\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# We load the PROCESSOR from the BASE model\n",
    "PROCESSOR_NAME = \"MCG-NJU/videomae-base\" \n",
    "# We load the MODEL from the FINE-TUNED model (with the correct name)\n",
    "MODEL_NAME = \"MCG-NJU/videomae-base-finetuned-kinetics\"  # <-- FINAL FIX\n",
    "# ---------------------\n",
    "\n",
    "MODEL_SAVE_PATH = \"./models/video_specialist\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_FRAMES = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Re-create Label Maps ---\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    raise\n",
    "\n",
    "SENTIMENT_LABELS = sorted(train_df['post_sentiment'].unique())\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Found {NUM_LABELS} labels: {label_to_id}\")\n",
    "\n",
    "\n",
    "# --- 3. Define Custom Video Dataset ---\n",
    "class VideoSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, label_map, num_frames):\n",
    "        self.processor = processor\n",
    "        self.label_map = label_map\n",
    "        self.num_frames = num_frames\n",
    "        \n",
    "        self.data = dataframe[\n",
    "            (dataframe['post_hint'] == 'hosted:video') & \n",
    "            (dataframe['local_media_path'].notna())\n",
    "        ].copy()\n",
    "        \n",
    "        self.data['labels'] = self.data['post_sentiment'].map(self.label_map)\n",
    "        \n",
    "        self.paths = self.data['local_media_path'].tolist()\n",
    "        self.labels = self.data['labels'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _sample_frames(self, video_path):\n",
    "        frames = []\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            if total_frames <= 0:\n",
    "                raise IOError(f\"Video file seems empty: {video_path}\")\n",
    "            \n",
    "            indices = np.linspace(0, total_frames - 1, num=self.num_frames, dtype=int)\n",
    "            \n",
    "            for idx in indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    # if frame read fails, try to grab the next one\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        continue \n",
    "                \n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(Image.fromarray(frame_rgb))\n",
    "                \n",
    "            cap.release()\n",
    "            \n",
    "            if not frames:\n",
    "                 raise IOError(f\"Could not read any frames from: {video_path}\")\n",
    "\n",
    "            return frames\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {e}. Using blank frames.\")\n",
    "            return [Image.new(\"RGB\", (224, 224), (0, 0, 0))] * self.num_frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        frames = self._sample_frames(video_path)\n",
    "        \n",
    "        processed_video = self.processor(\n",
    "            images=frames, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        pixel_values = processed_video['pixel_values'].squeeze() \n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "\n",
    "# --- 4. Initialize Processor, Datasets, and DataLoaders ---\n",
    "print(f\"Loading processor from: {PROCESSOR_NAME}\")\n",
    "# Load processor from the BASE model\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(PROCESSOR_NAME) \n",
    "\n",
    "train_dataset = VideoSentimentDataset(train_df, image_processor, label_to_id, NUM_FRAMES)\n",
    "val_dataset = VideoSentimentDataset(val_df, image_processor, label_to_id, NUM_FRAMES)\n",
    "\n",
    "print(f\"\\n--- DATASET SIZE WARNING ---\")\n",
    "print(f\"Total training videos: {len(train_dataset)}\")\n",
    "print(f\"Total validation videos: {len(val_dataset)}\")\n",
    "print(\"This is very small. Overfitting is expected.\")\n",
    "print(\"----------------------------\\n\")\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"WARNING: No videos found in training set. Skipping video model training.\")\n",
    "    video_model_trained = False\n",
    "else:\n",
    "    video_model_trained = True\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# --- 5. Load Model, Optimizer, and Scheduler ---\n",
    "if video_model_trained:\n",
    "    print(f\"Loading model from: {MODEL_NAME}\")\n",
    "    # Load model from the FINE-TUNED model\n",
    "    video_model = VideoMAEForVideoClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True # Drops the old head, adds our new one\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(video_model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping video model training as no video data was found.\")\n",
    "\n",
    "# --- 6. The Training & Validation Loop ---\n",
    "if video_model_trained:\n",
    "    print(\"--- Starting Training ---\")\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1} / {EPOCHS} ---\")\n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        video_model.train() \n",
    "        total_train_loss = 0\n",
    "        train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch in train_progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            video_model.zero_grad()\n",
    "\n",
    "            outputs = video_model(\n",
    "                pixel_values=pixel_values,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(video_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        if len(train_loader) > 0:\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        else:\n",
    "            print(\"No training batches.\")\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        video_model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress_bar:\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = video_model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Check if loss is valid\n",
    "                if loss is not None:\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "                \n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "                \n",
    "        if len(val_loader) > 0:\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            # --- Save the Best Model ---\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                print(\"New best model! Saving...\")\n",
    "                video_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "                image_processor.save_pretrained(MODEL_SAVE_PATH)\n",
    "        else:\n",
    "            print(\"Validation set is empty. Saving model from this epoch.\")\n",
    "            video_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "            image_processor.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    if len(val_loader) > 0:\n",
    "        print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "    print(f\"Model and processor saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Phase 2 Training (Training the Fusion Model)\n",
    "\n",
    "Now that we have our specialists, we *discard* their temporary classification heads. We use the *output embeddings* (the feature vectors) from these models as input for our new, simple **Fusion Model**. This model's job is to learn how to combine the signals from text, image, and video to make the best final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.A: Create Embedding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 2: Fusion Model Training ---\n",
      "Using device: cuda\n",
      "Combined embedding dimension will be: 2304\n",
      "Loaded 1424 train posts and 178 validation posts.\n",
      "Using 5 labels: {'Anger': 0, 'Joy': 1, 'Neutral/Other': 2, 'Sadness': 3, 'Surprise': 4}\n",
      "Loading specialist models...\n",
      "Text specialist loaded.\n",
      "Image specialist loaded.\n",
      "Video specialist loaded.\n",
      "\n",
      "--- Starting Step 5.A: Creating Embedding Datasets ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186433f801974426b3299cbe436c7f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train_embeddings.pt:   0%|          | 0/1424 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to data\\train_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072abbb2e6614fd7a404a9c23ef9e856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val_embeddings.pt:   0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to data\\val_embeddings.pt\n",
      "Embedding datasets created successfully.\n",
      "\n",
      "--- Starting Step 5.B: Training Fusion Model ---\n",
      "Training fusion model...\n",
      "Epoch 1/50 | Train Loss: 0.6339 | Val Loss: 1.1168 | Val Acc: 0.5843\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 2/50 | Train Loss: 0.2136 | Val Loss: 1.2123 | Val Acc: 0.5730\n",
      "Epoch 3/50 | Train Loss: 0.1316 | Val Loss: 1.2782 | Val Acc: 0.5843\n",
      "Epoch 4/50 | Train Loss: 0.0972 | Val Loss: 1.3043 | Val Acc: 0.6011\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 5/50 | Train Loss: 0.0771 | Val Loss: 1.3714 | Val Acc: 0.5955\n",
      "Epoch 6/50 | Train Loss: 0.0624 | Val Loss: 1.3839 | Val Acc: 0.6124\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 7/50 | Train Loss: 0.0556 | Val Loss: 1.4252 | Val Acc: 0.6124\n",
      "Epoch 8/50 | Train Loss: 0.0470 | Val Loss: 1.4530 | Val Acc: 0.6067\n",
      "Epoch 9/50 | Train Loss: 0.0433 | Val Loss: 1.4805 | Val Acc: 0.6124\n",
      "Epoch 10/50 | Train Loss: 0.0384 | Val Loss: 1.5240 | Val Acc: 0.6011\n",
      "Epoch 11/50 | Train Loss: 0.0358 | Val Loss: 1.5378 | Val Acc: 0.5955\n",
      "Epoch 12/50 | Train Loss: 0.0336 | Val Loss: 1.5569 | Val Acc: 0.6067\n",
      "Epoch 13/50 | Train Loss: 0.0315 | Val Loss: 1.5881 | Val Acc: 0.5955\n",
      "Epoch 14/50 | Train Loss: 0.0273 | Val Loss: 1.6050 | Val Acc: 0.6011\n",
      "Epoch 15/50 | Train Loss: 0.0265 | Val Loss: 1.6488 | Val Acc: 0.6067\n",
      "Epoch 16/50 | Train Loss: 0.0242 | Val Loss: 1.6635 | Val Acc: 0.6124\n",
      "Epoch 17/50 | Train Loss: 0.0251 | Val Loss: 1.6877 | Val Acc: 0.6011\n",
      "Epoch 18/50 | Train Loss: 0.0209 | Val Loss: 1.7124 | Val Acc: 0.6124\n",
      "Epoch 19/50 | Train Loss: 0.0202 | Val Loss: 1.7229 | Val Acc: 0.6180\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 20/50 | Train Loss: 0.0171 | Val Loss: 1.7452 | Val Acc: 0.6124\n",
      "Epoch 21/50 | Train Loss: 0.0155 | Val Loss: 1.7699 | Val Acc: 0.6124\n",
      "Epoch 22/50 | Train Loss: 0.0170 | Val Loss: 1.7920 | Val Acc: 0.6180\n",
      "Epoch 23/50 | Train Loss: 0.0156 | Val Loss: 1.8237 | Val Acc: 0.6180\n",
      "Epoch 24/50 | Train Loss: 0.0154 | Val Loss: 1.8252 | Val Acc: 0.6011\n",
      "Epoch 25/50 | Train Loss: 0.0136 | Val Loss: 1.8452 | Val Acc: 0.6124\n",
      "Epoch 26/50 | Train Loss: 0.0125 | Val Loss: 1.8755 | Val Acc: 0.6180\n",
      "Epoch 27/50 | Train Loss: 0.0129 | Val Loss: 1.8928 | Val Acc: 0.6180\n",
      "Epoch 28/50 | Train Loss: 0.0137 | Val Loss: 1.9256 | Val Acc: 0.6124\n",
      "Epoch 29/50 | Train Loss: 0.0111 | Val Loss: 1.9296 | Val Acc: 0.6124\n",
      "Epoch 30/50 | Train Loss: 0.0101 | Val Loss: 1.9454 | Val Acc: 0.6180\n",
      "Epoch 31/50 | Train Loss: 0.0108 | Val Loss: 1.9785 | Val Acc: 0.5955\n",
      "Epoch 32/50 | Train Loss: 0.0106 | Val Loss: 1.9857 | Val Acc: 0.6124\n",
      "Epoch 33/50 | Train Loss: 0.0091 | Val Loss: 2.0478 | Val Acc: 0.6236\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 34/50 | Train Loss: 0.0081 | Val Loss: 2.0232 | Val Acc: 0.6067\n",
      "Epoch 35/50 | Train Loss: 0.0097 | Val Loss: 2.0468 | Val Acc: 0.6236\n",
      "Epoch 36/50 | Train Loss: 0.0078 | Val Loss: 2.0770 | Val Acc: 0.5899\n",
      "Epoch 37/50 | Train Loss: 0.0074 | Val Loss: 2.0823 | Val Acc: 0.6180\n",
      "Epoch 38/50 | Train Loss: 0.0057 | Val Loss: 2.0975 | Val Acc: 0.6236\n",
      "Epoch 39/50 | Train Loss: 0.0073 | Val Loss: 2.1127 | Val Acc: 0.6180\n",
      "Epoch 40/50 | Train Loss: 0.0054 | Val Loss: 2.1269 | Val Acc: 0.6180\n",
      "Epoch 41/50 | Train Loss: 0.0064 | Val Loss: 2.1398 | Val Acc: 0.5955\n",
      "Epoch 42/50 | Train Loss: 0.0064 | Val Loss: 2.1723 | Val Acc: 0.6236\n",
      "Epoch 43/50 | Train Loss: 0.0055 | Val Loss: 2.2072 | Val Acc: 0.6292\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 44/50 | Train Loss: 0.0074 | Val Loss: 2.1766 | Val Acc: 0.6180\n",
      "Epoch 45/50 | Train Loss: 0.0054 | Val Loss: 2.2016 | Val Acc: 0.5955\n",
      "Epoch 46/50 | Train Loss: 0.0042 | Val Loss: 2.2043 | Val Acc: 0.6180\n",
      "Epoch 47/50 | Train Loss: 0.0037 | Val Loss: 2.2268 | Val Acc: 0.6180\n",
      "Epoch 48/50 | Train Loss: 0.0071 | Val Loss: 2.2492 | Val Acc: 0.5955\n",
      "Epoch 49/50 | Train Loss: 0.0063 | Val Loss: 2.2472 | Val Acc: 0.6180\n",
      "Epoch 50/50 | Train Loss: 0.0043 | Val Loss: 2.2548 | Val Acc: 0.6292\n",
      "\n",
      "--- Fusion Training Complete ---\n",
      "Best Validation Accuracy: 0.6292\n",
      "Final fusion model saved to: ./models/fusion_model.pth\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Phase 2 Training (The Fusion Model) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertModel,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModel,\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEModel\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting Phase 2: Fusion Model Training ---\")\n",
    "\n",
    "# --- 0. Configuration & Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model paths\n",
    "TEXT_MODEL_PATH = \"./models/text_specialist\"\n",
    "IMAGE_MODEL_PATH = \"./models/image_specialist.pth\"\n",
    "VIDEO_MODEL_PATH = \"./models/video_specialist\"\n",
    "FUSION_MODEL_SAVE_PATH = \"./models/fusion_model.pth\"\n",
    "\n",
    "# Define embedding dimensions (based on the 'base' models we used)\n",
    "TEXT_EMBED_DIM = 768  # (from DistilBERT-base)\n",
    "IMAGE_EMBED_DIM = 768  # (from CLIP-ViT-base)\n",
    "VIDEO_EMBED_DIM = 768  # (from VideoMAE-base)\n",
    "COMBINED_EMBED_DIM = TEXT_EMBED_DIM + IMAGE_EMBED_DIM + VIDEO_EMBED_DIM\n",
    "\n",
    "print(f\"Combined embedding dimension will be: {COMBINED_EMBED_DIM}\")\n",
    "\n",
    "# Fusion model training config\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50 # We can train for more epochs, it's a very small model\n",
    "LEARNING_RATE = 1e-4 # A slightly higher LR is fine for this MLP\n",
    "\n",
    "# --- 1. Re-define CustomCLIPModel (to load weights) ---\n",
    "# This MUST be the same class definition as in Step 4.B\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        self.clip_vision_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        embedding_dim = self.clip_vision_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.clip_vision_model(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n",
    "            \n",
    "        return {\"loss\": loss, \"logits\": logits, \"embedding\": pooled_output}\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Label Maps ---\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    raise\n",
    "\n",
    "SENTIMENT_LABELS = sorted(train_df['post_sentiment'].unique())\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Loaded {len(train_df)} train posts and {len(val_df)} validation posts.\")\n",
    "print(f\"Using {NUM_LABELS} labels: {label_to_id}\")\n",
    "\n",
    "\n",
    "# --- 3. Load All Specialist Models and Processors ---\n",
    "\n",
    "print(\"Loading specialist models...\")\n",
    "\n",
    "# --- Text Specialist ---\n",
    "try:\n",
    "    text_tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "    # Load the base model, not the classification head\n",
    "    text_model = DistilBertModel.from_pretrained(TEXT_MODEL_PATH).to(device)\n",
    "    text_model.eval()\n",
    "    print(\"Text specialist loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load text model: {e}\")\n",
    "    text_model = None\n",
    "\n",
    "# --- Image Specialist ---\n",
    "try:\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    # Instantiate the class, then load the saved weights\n",
    "    temp_image_model = CustomCLIPModel(\"openai/clip-vit-base-patch32\", NUM_LABELS)\n",
    "    temp_image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=device))\n",
    "    \n",
    "    # --- IMPORTANT: We only keep the core vision model ---\n",
    "    image_model = temp_image_model.clip_vision_model.to(device)\n",
    "    image_model.eval()\n",
    "    print(\"Image specialist loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load image model: {e}\")\n",
    "    image_model = None\n",
    "\n",
    "# --- Video Specialist ---\n",
    "try:\n",
    "    video_processor = VideoMAEImageProcessor.from_pretrained(VIDEO_MODEL_PATH)\n",
    "    # Load the base model, not the classification head\n",
    "    video_model = VideoMAEModel.from_pretrained(VIDEO_MODEL_PATH).to(device)\n",
    "    video_model.eval()\n",
    "    print(\"Video specialist loaded.\")\n",
    "    video_model_trained = True\n",
    "except Exception as e:\n",
    "    print(f\"Could not load video model from {VIDEO_MODEL_PATH}: {e}\")\n",
    "    print(\"Will use zeros for video embeddings.\")\n",
    "    video_model = None\n",
    "    video_model_trained = False\n",
    "\n",
    "\n",
    "# --- 4. Helper Functions for Embedding Extraction ---\n",
    "\n",
    "# (We re-use the frame sampler from the video training)\n",
    "def _sample_frames(video_path, num_frames=16):\n",
    "    frames = []\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0: raise IOError(\"Video file empty\")\n",
    "        \n",
    "        indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame_rgb))\n",
    "        cap.release()\n",
    "        \n",
    "        if not frames: raise IOError(\"Could not read frames\")\n",
    "        return frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error sampling {video_path}: {e}. Using blank frames.\")\n",
    "        return [Image.new(\"RGB\", (224, 224), (0, 0, 0))] * num_frames\n",
    "\n",
    "# This function processes ONE row of the dataframe\n",
    "def get_embeddings(post_row):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # --- 1. Text Embedding ---\n",
    "        if text_model is not None and pd.notna(post_row['title']):\n",
    "            text_str = post_row.get('text', '')\n",
    "            text = post_row['title'] + \" [SEP] \" + (text_str if pd.notna(text_str) else '')\n",
    "            encoding = text_tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding=True).to(device)\n",
    "            outputs = text_model(**encoding)\n",
    "            # Get the embedding of the [CLS] token\n",
    "            text_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu()\n",
    "        else:\n",
    "            text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "\n",
    "        # --- 2. Image Embedding ---\n",
    "        if (image_model is not None and \n",
    "            post_row['post_hint'] == 'image' and \n",
    "            pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(post_row['local_media_path']).convert(\"RGB\")\n",
    "                processed_image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "                outputs = image_model(**processed_image)\n",
    "                image_emb = outputs.pooler_output.squeeze().cpu()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {post_row['local_media_path']}: {e}\")\n",
    "                image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "        else:\n",
    "            image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "\n",
    "        # --- 3. Video Embedding ---\n",
    "        if (video_model is not None and \n",
    "            post_row['post_hint'] == 'hosted:video' and \n",
    "            pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            \n",
    "            frames = _sample_frames(post_row['local_media_path'])\n",
    "            processed_video = video_processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "            outputs = video_model(**processed_video)\n",
    "            video_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu() # Avg pool frames\n",
    "        else:\n",
    "            video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "            \n",
    "        # --- 4. Concat and Return ---\n",
    "        combined_emb = torch.cat((text_emb, image_emb, video_emb))\n",
    "        label_id = label_to_id[post_row['post_sentiment']]\n",
    "        \n",
    "        return combined_emb, label_id\n",
    "\n",
    "\n",
    "# --- 5.A: Create Embedding Dataset ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5.A: Creating Embedding Datasets ---\")\n",
    "\n",
    "def create_dataset(df, filename):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {filename}\"):\n",
    "        emb, label = get_embeddings(row)\n",
    "        embeddings.append(emb)\n",
    "        labels.append(label)\n",
    "        \n",
    "    # Stack all tensors into a single tensor\n",
    "    embeddings = torch.stack(embeddings)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # Save to disk\n",
    "    save_path = os.path.join(DATA_DIR, filename)\n",
    "    torch.save((embeddings, labels), save_path)\n",
    "    print(f\"Saved dataset to {save_path}\")\n",
    "    return embeddings, labels\n",
    "\n",
    "train_embeddings, train_labels = create_dataset(train_df, \"train_embeddings.pt\")\n",
    "val_embeddings, val_labels = create_dataset(val_df, \"val_embeddings.pt\")\n",
    "\n",
    "print(\"Embedding datasets created successfully.\")\n",
    "\n",
    "\n",
    "# --- 5.B: Train Fusion Model ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5.B: Training Fusion Model ---\")\n",
    "\n",
    "# 1. Define simple Dataset for embeddings\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "train_emb_dataset = EmbeddingDataset(train_embeddings, train_labels)\n",
    "val_emb_dataset = EmbeddingDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_emb_loader = DataLoader(train_emb_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_emb_loader = DataLoader(val_emb_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 2. Define the Fusion Model architecture\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "fusion_model = FusionModel(\n",
    "    input_dim=COMBINED_EMBED_DIM, \n",
    "    hidden_dim=512,  # A reasonable hidden layer size\n",
    "    output_dim=NUM_LABELS\n",
    ").to(device)\n",
    "\n",
    "# 3. Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 4. Training Loop\n",
    "print(\"Training fusion model...\")\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # --- Training ---\n",
    "    fusion_model.train()\n",
    "    total_train_loss = 0\n",
    "    for embs, labels in train_emb_loader:\n",
    "        embs = embs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = fusion_model(embs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_emb_loader)\n",
    "\n",
    "    # --- Validation ---\n",
    "    fusion_model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embs, labels in val_emb_loader:\n",
    "            embs = embs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = fusion_model(embs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_emb_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        print(f\"New best model! Saving to {FUSION_MODEL_SAVE_PATH}\")\n",
    "        torch.save(fusion_model.state_dict(), FUSION_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- Fusion Training Complete ---\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Final fusion model saved to: {FUSION_MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.B: Train Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 2: Fusion Model Training ---\n",
      "Using device: cuda\n",
      "Combined embedding dimension will be: 2304\n",
      "Loaded 386 train posts and 49 validation posts.\n",
      "Using 5 labels: {'Anger': 0, 'Joy': 1, 'Neutral/Other': 2, 'Sadness': 3, 'Surprise': 4}\n",
      "Loading specialist models...\n",
      "Text specialist loaded.\n",
      "Image specialist loaded.\n",
      "Video specialist loaded.\n",
      "\n",
      "--- Starting Step 5.A: Creating Embedding Datasets ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b9eeb07ab34cc6a57cade48b3daf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train_embeddings.pt:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to data\\train_embeddings.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6152380052e5456b8d89de9b250d1e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val_embeddings.pt:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denzil\\miniconda3\\envs\\emotion_training\\lib\\site-packages\\PIL\\Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to data\\val_embeddings.pt\n",
      "Embedding datasets created successfully.\n",
      "\n",
      "--- Starting Step 5.B: Training Fusion Model ---\n",
      "Training fusion model...\n",
      "Epoch 1/50 | Train Loss: 1.2361 | Val Loss: 1.3022 | Val Acc: 0.4694\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 2/50 | Train Loss: 0.7389 | Val Loss: 1.2643 | Val Acc: 0.4490\n",
      "Epoch 3/50 | Train Loss: 0.5155 | Val Loss: 1.2668 | Val Acc: 0.4490\n",
      "Epoch 4/50 | Train Loss: 0.3762 | Val Loss: 1.2937 | Val Acc: 0.4490\n",
      "Epoch 5/50 | Train Loss: 0.3587 | Val Loss: 1.3386 | Val Acc: 0.4694\n",
      "Epoch 6/50 | Train Loss: 0.2259 | Val Loss: 1.3753 | Val Acc: 0.4694\n",
      "Epoch 7/50 | Train Loss: 0.1712 | Val Loss: 1.3936 | Val Acc: 0.4490\n",
      "Epoch 8/50 | Train Loss: 0.2188 | Val Loss: 1.4224 | Val Acc: 0.4898\n",
      "New best model! Saving to ./models/fusion_model.pth\n",
      "Epoch 9/50 | Train Loss: 0.1353 | Val Loss: 1.4824 | Val Acc: 0.4694\n",
      "Epoch 10/50 | Train Loss: 0.1215 | Val Loss: 1.4509 | Val Acc: 0.4898\n",
      "Epoch 11/50 | Train Loss: 0.1127 | Val Loss: 1.4784 | Val Acc: 0.4490\n",
      "Epoch 12/50 | Train Loss: 0.1440 | Val Loss: 1.5256 | Val Acc: 0.4694\n",
      "Epoch 13/50 | Train Loss: 0.0940 | Val Loss: 1.6452 | Val Acc: 0.4694\n",
      "Epoch 14/50 | Train Loss: 0.0871 | Val Loss: 1.5833 | Val Acc: 0.4694\n",
      "Epoch 15/50 | Train Loss: 0.1959 | Val Loss: 1.5754 | Val Acc: 0.4490\n",
      "Epoch 16/50 | Train Loss: 0.0799 | Val Loss: 1.5588 | Val Acc: 0.4286\n",
      "Epoch 17/50 | Train Loss: 0.0743 | Val Loss: 1.5646 | Val Acc: 0.4694\n",
      "Epoch 18/50 | Train Loss: 0.0720 | Val Loss: 1.5919 | Val Acc: 0.4490\n",
      "Epoch 19/50 | Train Loss: 0.0627 | Val Loss: 1.6239 | Val Acc: 0.4694\n",
      "Epoch 20/50 | Train Loss: 0.0618 | Val Loss: 1.6210 | Val Acc: 0.4694\n",
      "Epoch 21/50 | Train Loss: 0.0613 | Val Loss: 1.6322 | Val Acc: 0.4490\n",
      "Epoch 22/50 | Train Loss: 0.0562 | Val Loss: 1.6432 | Val Acc: 0.4694\n",
      "Epoch 23/50 | Train Loss: 0.0509 | Val Loss: 1.6457 | Val Acc: 0.4490\n",
      "Epoch 24/50 | Train Loss: 0.0460 | Val Loss: 1.6535 | Val Acc: 0.4490\n",
      "Epoch 25/50 | Train Loss: 0.0451 | Val Loss: 1.6526 | Val Acc: 0.4490\n",
      "Epoch 26/50 | Train Loss: 0.0477 | Val Loss: 1.6558 | Val Acc: 0.4490\n",
      "Epoch 27/50 | Train Loss: 0.0420 | Val Loss: 1.6822 | Val Acc: 0.4490\n",
      "Epoch 28/50 | Train Loss: 0.0411 | Val Loss: 1.6796 | Val Acc: 0.4286\n",
      "Epoch 29/50 | Train Loss: 0.0418 | Val Loss: 1.7319 | Val Acc: 0.4898\n",
      "Epoch 30/50 | Train Loss: 0.0360 | Val Loss: 1.7283 | Val Acc: 0.4898\n",
      "Epoch 31/50 | Train Loss: 0.0373 | Val Loss: 1.7138 | Val Acc: 0.4694\n",
      "Epoch 32/50 | Train Loss: 0.0663 | Val Loss: 1.7168 | Val Acc: 0.4694\n",
      "Epoch 33/50 | Train Loss: 0.0569 | Val Loss: 1.7784 | Val Acc: 0.4694\n",
      "Epoch 34/50 | Train Loss: 0.0437 | Val Loss: 1.7331 | Val Acc: 0.4898\n",
      "Epoch 35/50 | Train Loss: 0.0343 | Val Loss: 1.7464 | Val Acc: 0.4694\n",
      "Epoch 36/50 | Train Loss: 0.0302 | Val Loss: 1.7383 | Val Acc: 0.4694\n",
      "Epoch 37/50 | Train Loss: 0.0334 | Val Loss: 1.7344 | Val Acc: 0.4490\n",
      "Epoch 38/50 | Train Loss: 0.0304 | Val Loss: 1.7458 | Val Acc: 0.4490\n",
      "Epoch 39/50 | Train Loss: 0.0285 | Val Loss: 1.7765 | Val Acc: 0.4694\n",
      "Epoch 40/50 | Train Loss: 0.0263 | Val Loss: 1.7899 | Val Acc: 0.4490\n",
      "Epoch 41/50 | Train Loss: 0.0383 | Val Loss: 1.7865 | Val Acc: 0.4694\n",
      "Epoch 42/50 | Train Loss: 0.0244 | Val Loss: 1.7939 | Val Acc: 0.4898\n",
      "Epoch 43/50 | Train Loss: 0.0334 | Val Loss: 1.7872 | Val Acc: 0.4490\n",
      "Epoch 44/50 | Train Loss: 0.0249 | Val Loss: 1.7713 | Val Acc: 0.4898\n",
      "Epoch 45/50 | Train Loss: 0.0242 | Val Loss: 1.7749 | Val Acc: 0.4490\n",
      "Epoch 46/50 | Train Loss: 0.0228 | Val Loss: 1.7886 | Val Acc: 0.4082\n",
      "Epoch 47/50 | Train Loss: 0.0214 | Val Loss: 1.8147 | Val Acc: 0.4694\n",
      "Epoch 48/50 | Train Loss: 0.0197 | Val Loss: 1.8200 | Val Acc: 0.4898\n",
      "Epoch 49/50 | Train Loss: 0.0178 | Val Loss: 1.8313 | Val Acc: 0.4898\n",
      "Epoch 50/50 | Train Loss: 0.0188 | Val Loss: 1.8344 | Val Acc: 0.4694\n",
      "\n",
      "--- Fusion Training Complete ---\n",
      "Best Validation Accuracy: 0.4898\n",
      "Final fusion model saved to: ./models/fusion_model.pth\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Phase 2 Training (The Fusion Model) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertModel,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModel,\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEModel\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting Phase 2: Fusion Model Training ---\")\n",
    "\n",
    "# --- 0. Configuration & Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model paths\n",
    "TEXT_MODEL_PATH = \"./models/text_specialist\"\n",
    "IMAGE_MODEL_PATH = \"./models/image_specialist.pth\"\n",
    "VIDEO_MODEL_PATH = \"./models/video_specialist\"\n",
    "FUSION_MODEL_SAVE_PATH = \"./models/fusion_model.pth\"\n",
    "\n",
    "# Define embedding dimensions (based on the 'base' models we used)\n",
    "TEXT_EMBED_DIM = 768  # (from DistilBERT-base)\n",
    "IMAGE_EMBED_DIM = 768  # (from CLIP-ViT-base)\n",
    "VIDEO_EMBED_DIM = 768  # (from VideoMAE-base)\n",
    "COMBINED_EMBED_DIM = TEXT_EMBED_DIM + IMAGE_EMBED_DIM + VIDEO_EMBED_DIM\n",
    "\n",
    "print(f\"Combined embedding dimension will be: {COMBINED_EMBED_DIM}\")\n",
    "\n",
    "# Fusion model training config\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50 # We can train for more epochs, it's a very small model\n",
    "LEARNING_RATE = 1e-4 # A slightly higher LR is fine for this MLP\n",
    "\n",
    "# --- 1. Re-define CustomCLIPModel (to load weights) ---\n",
    "# This MUST be the same class definition as in Step 4.B\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        self.clip_vision_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        embedding_dim = self.clip_vision_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.clip_vision_model(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n",
    "            \n",
    "        return {\"loss\": loss, \"logits\": logits, \"embedding\": pooled_output}\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Label Maps ---\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    raise\n",
    "\n",
    "SENTIMENT_LABELS = sorted(train_df['post_sentiment'].unique())\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Loaded {len(train_df)} train posts and {len(val_df)} validation posts.\")\n",
    "print(f\"Using {NUM_LABELS} labels: {label_to_id}\")\n",
    "\n",
    "\n",
    "# --- 3. Load All Specialist Models and Processors ---\n",
    "\n",
    "print(\"Loading specialist models...\")\n",
    "\n",
    "# --- Text Specialist ---\n",
    "try:\n",
    "    text_tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "    # Load the base model, not the classification head\n",
    "    text_model = DistilBertModel.from_pretrained(TEXT_MODEL_PATH).to(device)\n",
    "    text_model.eval()\n",
    "    print(\"Text specialist loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load text model: {e}\")\n",
    "    text_model = None\n",
    "\n",
    "# --- Image Specialist ---\n",
    "try:\n",
    "    image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    # Instantiate the class, then load the saved weights\n",
    "    temp_image_model = CustomCLIPModel(\"openai/clip-vit-base-patch32\", NUM_LABELS)\n",
    "    temp_image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=device))\n",
    "    \n",
    "    # --- IMPORTANT: We only keep the core vision model ---\n",
    "    image_model = temp_image_model.clip_vision_model.to(device)\n",
    "    image_model.eval()\n",
    "    print(\"Image specialist loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load image model: {e}\")\n",
    "    image_model = None\n",
    "\n",
    "# --- Video Specialist ---\n",
    "try:\n",
    "    video_processor = VideoMAEImageProcessor.from_pretrained(VIDEO_MODEL_PATH)\n",
    "    # Load the base model, not the classification head\n",
    "    video_model = VideoMAEModel.from_pretrained(VIDEO_MODEL_PATH).to(device)\n",
    "    video_model.eval()\n",
    "    print(\"Video specialist loaded.\")\n",
    "    video_model_trained = True\n",
    "except Exception as e:\n",
    "    print(f\"Could not load video model from {VIDEO_MODEL_PATH}: {e}\")\n",
    "    print(\"Will use zeros for video embeddings.\")\n",
    "    video_model = None\n",
    "    video_model_trained = False\n",
    "\n",
    "\n",
    "# --- 4. Helper Functions for Embedding Extraction ---\n",
    "\n",
    "# (We re-use the frame sampler from the video training)\n",
    "def _sample_frames(video_path, num_frames=16):\n",
    "    frames = []\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0: raise IOError(\"Video file empty\")\n",
    "        \n",
    "        indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame_rgb))\n",
    "        cap.release()\n",
    "        \n",
    "        if not frames: raise IOError(\"Could not read frames\")\n",
    "        return frames\n",
    "    except Exception as e:\n",
    "        print(f\"Error sampling {video_path}: {e}. Using blank frames.\")\n",
    "        return [Image.new(\"RGB\", (224, 224), (0, 0, 0))] * num_frames\n",
    "\n",
    "# This function processes ONE row of the dataframe\n",
    "def get_embeddings(post_row):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # --- 1. Text Embedding ---\n",
    "        if text_model is not None and pd.notna(post_row['title']):\n",
    "            text_str = post_row.get('text', '')\n",
    "            text = post_row['title'] + \" [SEP] \" + (text_str if pd.notna(text_str) else '')\n",
    "            encoding = text_tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding=True).to(device)\n",
    "            outputs = text_model(**encoding)\n",
    "            # Get the embedding of the [CLS] token\n",
    "            text_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu()\n",
    "        else:\n",
    "            text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "\n",
    "        # --- 2. Image Embedding ---\n",
    "        if (image_model is not None and \n",
    "            post_row['post_hint'] == 'image' and \n",
    "            pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(post_row['local_media_path']).convert(\"RGB\")\n",
    "                processed_image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "                outputs = image_model(**processed_image)\n",
    "                image_emb = outputs.pooler_output.squeeze().cpu()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {post_row['local_media_path']}: {e}\")\n",
    "                image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "        else:\n",
    "            image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "\n",
    "        # --- 3. Video Embedding ---\n",
    "        if (video_model is not None and \n",
    "            post_row['post_hint'] == 'hosted:video' and \n",
    "            pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            \n",
    "            frames = _sample_frames(post_row['local_media_path'])\n",
    "            processed_video = video_processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "            outputs = video_model(**processed_video)\n",
    "            video_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu() # Avg pool frames\n",
    "        else:\n",
    "            video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "            \n",
    "        # --- 4. Concat and Return ---\n",
    "        combined_emb = torch.cat((text_emb, image_emb, video_emb))\n",
    "        label_id = label_to_id[post_row['post_sentiment']]\n",
    "        \n",
    "        return combined_emb, label_id\n",
    "\n",
    "\n",
    "# --- 5.A: Create Embedding Dataset ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5.A: Creating Embedding Datasets ---\")\n",
    "\n",
    "def create_dataset(df, filename):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {filename}\"):\n",
    "        emb, label = get_embeddings(row)\n",
    "        embeddings.append(emb)\n",
    "        labels.append(label)\n",
    "        \n",
    "    # Stack all tensors into a single tensor\n",
    "    embeddings = torch.stack(embeddings)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # Save to disk\n",
    "    save_path = os.path.join(DATA_DIR, filename)\n",
    "    torch.save((embeddings, labels), save_path)\n",
    "    print(f\"Saved dataset to {save_path}\")\n",
    "    return embeddings, labels\n",
    "\n",
    "train_embeddings, train_labels = create_dataset(train_df, \"train_embeddings.pt\")\n",
    "val_embeddings, val_labels = create_dataset(val_df, \"val_embeddings.pt\")\n",
    "\n",
    "print(\"Embedding datasets created successfully.\")\n",
    "\n",
    "\n",
    "# --- 5.B: Train Fusion Model ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5.B: Training Fusion Model ---\")\n",
    "\n",
    "# 1. Define simple Dataset for embeddings\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "train_emb_dataset = EmbeddingDataset(train_embeddings, train_labels)\n",
    "val_emb_dataset = EmbeddingDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_emb_loader = DataLoader(train_emb_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_emb_loader = DataLoader(val_emb_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 2. Define the Fusion Model architecture\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "fusion_model = FusionModel(\n",
    "    input_dim=COMBINED_EMBED_DIM, \n",
    "    hidden_dim=512,  # A reasonable hidden layer size\n",
    "    output_dim=NUM_LABELS\n",
    ").to(device)\n",
    "\n",
    "# 3. Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 4. Training Loop\n",
    "print(\"Training fusion model...\")\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # --- Training ---\n",
    "    fusion_model.train()\n",
    "    total_train_loss = 0\n",
    "    for embs, labels in train_emb_loader:\n",
    "        embs = embs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = fusion_model(embs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_emb_loader)\n",
    "\n",
    "    # --- Validation ---\n",
    "    fusion_model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embs, labels in val_emb_loader:\n",
    "            embs = embs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = fusion_model(embs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_emb_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        print(f\"New best model! Saving to {FUSION_MODEL_SAVE_PATH}\")\n",
    "        torch.save(fusion_model.state_dict(), FUSION_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- Fusion Training Complete ---\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "print(f\"Final fusion model saved to: {FUSION_MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation (The Final Test)\n",
    "\n",
    "This is the moment of truth. We now use our **unseen** `test_set.csv`. We run each post in it through the *entire pipeline* (Specialists -> Fusion Model) and compare the final prediction to the true label. This gives us our final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Step 6: Final Evaluation on Test Set ---\n",
      "Using device: cuda\n",
      "Loaded 179 test posts.\n",
      "Using 5 labels: {'Anger': 0, 'Joy': 1, 'Neutral/Other': 2, 'Sadness': 3, 'Surprise': 4}\n",
      "Loading all models and processors for evaluation...\n",
      "Video specialist loaded.\n",
      "All models loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e684aeb5821044358f24c2784720b808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Test Set:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Performance ---G\n",
      "Overall Accuracy: 57.54%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Anger       0.61      0.63      0.62        43\n",
      "          Joy       0.59      0.69      0.63        54\n",
      "Neutral/Other       0.64      0.58      0.61        60\n",
      "      Sadness       0.25      0.20      0.22        10\n",
      "     Surprise       0.22      0.17      0.19        12\n",
      "\n",
      "     accuracy                           0.58       179\n",
      "    macro avg       0.46      0.45      0.45       179\n",
      " weighted avg       0.57      0.58      0.57       179\n",
      "\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertModel,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModel,\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEModel\n",
    ")\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting Step 6: Final Evaluation on Test Set ---\")\n",
    "\n",
    "# --- 0. Configuration & Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model paths\n",
    "TEXT_MODEL_PATH = \"./models/text_specialist\"\n",
    "IMAGE_MODEL_PATH = \"./models/image_specialist.pth\"\n",
    "VIDEO_MODEL_PATH = \"./models/video_specialist\"\n",
    "FUSION_MODEL_SAVE_PATH = \"./models/fusion_model.pth\"\n",
    "\n",
    "# Define embedding dimensions\n",
    "TEXT_EMBED_DIM = 768\n",
    "IMAGE_EMBED_DIM = 768\n",
    "VIDEO_EMBED_DIM = 768\n",
    "COMBINED_EMBED_DIM = TEXT_EMBED_DIM + IMAGE_EMBED_DIM + VIDEO_EMBED_DIM\n",
    "\n",
    "# --- 1. Re-define Model Classes (from Step 4 & 5) ---\n",
    "\n",
    "# Custom CLIP Model (from 4.B)\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        self.clip_vision_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        embedding_dim = self.clip_vision_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.clip_vision_model(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits, \"embedding\": pooled_output}\n",
    "\n",
    "# Fusion Model (from 5.B)\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "# --- 2. Re-define Helper Functions (from Step 4 & 5) ---\n",
    "\n",
    "def _sample_frames(video_path, num_frames=16):\n",
    "    frames = []\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0: raise IOError(\"Video file empty\")\n",
    "        indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(frame_rgb))\n",
    "        cap.release()\n",
    "        if not frames: raise IOError(\"Could not read frames\")\n",
    "        return frames\n",
    "    except Exception as e:\n",
    "        # print(f\"Error sampling {video_path}: {e}. Using blank frames.\")\n",
    "        return [Image.new(\"RGB\", (224, 224), (0, 0, 0))] * num_frames\n",
    "\n",
    "# Embedding extraction function (from 5.A)\n",
    "# Note: Models must be loaded and in eval() mode before calling this\n",
    "def get_embeddings(post_row, text_model, image_model, video_model, text_tokenizer, image_processor, video_processor):\n",
    "    with torch.no_grad():\n",
    "        # --- 1. Text Embedding ---\n",
    "        if text_model is not None and pd.notna(post_row['title']):\n",
    "            text_str = post_row.get('text', '')\n",
    "            text = post_row['title'] + \" [SEP] \" + (text_str if pd.notna(text_str) else '')\n",
    "            encoding = text_tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding=True).to(device)\n",
    "            outputs = text_model(**encoding)\n",
    "            text_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu()\n",
    "        else:\n",
    "            text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "\n",
    "        # --- 2. Image Embedding ---\n",
    "        if (image_model is not None and \n",
    "            post_row['post_hint'] == 'image' and \n",
    "            pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            try:\n",
    "                image = Image.open(post_row['local_media_path']).convert(\"RGB\")\n",
    "                processed_image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "                outputs = image_model(**processed_image)\n",
    "                image_emb = outputs.pooler_output.squeeze().cpu()\n",
    "            except Exception as e:\n",
    "                image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "        else:\n",
    "            image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "\n",
    "        # --- 3. Video Embedding ---\n",
    "        if (video_model is not None and \n",
    "            post_row['post_hint'] == 'hosted:video' and \n",
    "            pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            frames = _sample_frames(post_row['local_media_path'])\n",
    "            processed_video = video_processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "            outputs = video_model(**processed_video)\n",
    "            video_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu() # Avg pool frames\n",
    "        else:\n",
    "            video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "            \n",
    "        # --- 4. Concat and Return ---\n",
    "        combined_emb = torch.cat((text_emb, image_emb, video_emb))\n",
    "        label_id = label_to_id[post_row['post_sentiment']]\n",
    "        \n",
    "        return combined_emb, label_id\n",
    "\n",
    "# --- 3. Load Data and Label Maps ---\n",
    "try:\n",
    "    # Load train_df to get the definitive label map\n",
    "    train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    test_df = pd.read_csv(TEST_SET_CSV)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {TEST_SET_CSV}\")\n",
    "    raise\n",
    "\n",
    "SENTIMENT_LABELS = sorted(train_df['post_sentiment'].unique())\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Loaded {len(test_df)} test posts.\")\n",
    "print(f\"Using {NUM_LABELS} labels: {label_to_id}\")\n",
    "\n",
    "# --- 4. Load All Models and Processors ---\n",
    "print(\"Loading all models and processors for evaluation...\")\n",
    "\n",
    "# --- Text ---\n",
    "text_tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "text_specialist = DistilBertModel.from_pretrained(TEXT_MODEL_PATH).to(device)\n",
    "text_specialist.eval()\n",
    "\n",
    "# --- Image ---\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "temp_image_model = CustomCLIPModel(\"openai/clip-vit-base-patch32\", NUM_LABELS)\n",
    "temp_image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=device))\n",
    "image_specialist = temp_image_model.clip_vision_model.to(device)\n",
    "image_specialist.eval()\n",
    "\n",
    "# --- Video ---\n",
    "try:\n",
    "    video_processor = VideoMAEImageProcessor.from_pretrained(VIDEO_MODEL_PATH)\n",
    "    video_specialist = VideoMAEModel.from_pretrained(VIDEO_MODEL_PATH).to(device)\n",
    "    video_specialist.eval()\n",
    "    print(\"Video specialist loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load video model, will use zeros: {e}\")\n",
    "    video_processor = None\n",
    "    video_specialist = None\n",
    "\n",
    "# --- Fusion ---\n",
    "fusion_model = FusionModel(\n",
    "    input_dim=COMBINED_EMBED_DIM, \n",
    "    hidden_dim=512, \n",
    "    output_dim=NUM_LABELS\n",
    ").to(device)\n",
    "fusion_model.load_state_dict(torch.load(FUSION_MODEL_SAVE_PATH, map_location=device))\n",
    "fusion_model.eval()\n",
    "\n",
    "print(\"All models loaded successfully.\")\n",
    "\n",
    "# --- 5. Run Evaluation Loop ---\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating Test Set\"):\n",
    "        # Use the get_embeddings function\n",
    "        combined_emb, true_label_id = get_embeddings(\n",
    "            row, \n",
    "            text_specialist, \n",
    "            image_specialist, \n",
    "            video_specialist, \n",
    "            text_tokenizer, \n",
    "            image_processor, \n",
    "            video_processor\n",
    "        )\n",
    "        all_true_labels.append(true_label_id)\n",
    "\n",
    "        # Pass embedding through the fusion model\n",
    "        combined_emb = combined_emb.unsqueeze(0).to(device) # Add batch dim\n",
    "        logits = fusion_model(combined_emb)\n",
    "        prediction_id = torch.argmax(logits, dim=1).item()\n",
    "        all_predictions.append(prediction_id)\n",
    "\n",
    "# --- 6. Calculate and Print Metrics ---\n",
    "\n",
    "# Convert IDs back to labels for a readable report\n",
    "true_labels_names = [id_to_label[idx] for idx in all_true_labels]\n",
    "predictions_names = [id_to_label[idx] for idx in all_predictions]\n",
    "\n",
    "print(\"\\n--- Final Model Performance ---G\")\n",
    "accuracy = accuracy_score(true_labels_names, predictions_names)\n",
    "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels_names, predictions_names, labels=SENTIMENT_LABELS))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(true_labels_names, predictions_names, labels=SENTIMENT_LABELS)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=SENTIMENT_LABELS, yticklabels=SENTIMENT_LABELS, cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prediction on New, Unseen Data\n",
    "\n",
    "This is the final step: creating a single function that can take a brand new, raw Reddit post URL, run it through the entire pipeline, and return the predicted sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure all 4 models and necessary tokenizers/transforms are loaded\n",
    "# text_specialist, image_specialist, video_specialist, fusion_model\n",
    "# text_tokenizer, image_transform, video_transform\n",
    "\n",
    "def predict_sentiment_for_new_post(post_url):\n",
    "    print(f\"Analyzing new post: {post_url}\")\n",
    "    \n",
    "    # TODO: 1. Scrape the single post using PRAW\n",
    "    # try:\n",
    "    #     post = reddit.submission(url=post_url)\n",
    "    # except Exception as e:\n",
    "    #     return f\"Error: Could not fetch post. {e}\"\n",
    "\n",
    "    # TODO: 2. Download its media (use the download_media function from Step 1)\n",
    "    # local_media_path = download_media(post)\n",
    "\n",
    "    # TODO: 3. Create a dictionary or mock DataFrame row for the post\n",
    "    # post_data = {\n",
    "    #     'id': post.id, 'title': post.title, 'text': post.selftext, \n",
    "    #     'post_hint': getattr(post, 'post_hint', 'text_only'), \n",
    "    #     'local_media_path': local_media_path, \n",
    "    #     'post_sentiment': 'UNKNOWN' # Dummy value\n",
    "    # }\n",
    "\n",
    "    # TODO: 4. Get embeddings (use the get_embeddings function from Step 5.A)\n",
    "    # with torch.no_grad():\n",
    "    #     combined_emb, _ = get_embeddings(post_data, ...)\n",
    "    #     combined_emb = combined_emb.unsqueeze(0).to(DEVICE) # Add batch dim\n",
    "\n",
    "    # TODO: 5. Get final prediction from fusion model\n",
    "    #     logits = fusion_model(combined_emb)\n",
    "    #     prediction_id = torch.argmax(logits, dim=1).item()\n",
    "    #     predicted_sentiment = id_to_label[prediction_id]\n",
    "    \n",
    "    # TODO: 6. Clean up the downloaded media file\n",
    "    # if local_media_path and os.path.exists(local_media_path):\n",
    "    #     os.remove(local_media_path)\n",
    "    \n",
    "    # return predicted_sentiment\n",
    "    \n",
    "    # --- Placeholder --- \n",
    "    print(\"Placeholder: This function will scrape, process, and predict.\")\n",
    "    return \"Joy\" # Return a mock prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# (Once your models are trained and loaded)\n",
    "\n",
    "# A post that is likely 'Joy' or 'Achievement'\n",
    "test_url_1 = \"[https://www.reddit.com/r/Brawlstars/comments/1dbv10k/after_all_this_time_i_finally_got_one/](https://www.reddit.com/r/Brawlstars/comments/1dbv10k/after_all_this_time_i_finally_got_one/)\"\n",
    "\n",
    "# A post that is likely 'Anger' or 'Rant'\n",
    "test_url_2 = \"[https://www.reddit.com/r/Brawlstars/comments/1ddc6n6/im_so_sick_of_this_game_breaking_bug/](https://www.reddit.com/r/Brawlstars/comments/1ddc6n6/im_so_sick_of_this_game_breaking_bug/)\"\n",
    "\n",
    "# sentiment1 = predict_sentiment_for_new_post(test_url_1)\n",
    "# print(f\"Prediction for post 1: {sentiment1}\\n\")\n",
    "\n",
    "# sentiment2 = predict_sentiment_for_new_post(test_url_2)\n",
    "# print(f\"Prediction for post 2: {sentiment2}\\n\")\n",
    "\n",
    "print(\"Notebook execution complete. Fill in the 'TODO' sections to build the models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
