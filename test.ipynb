{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Sentiment Analysis for r/BrawlStars\n",
    "\n",
    "This notebook implements the complete, end-to-end workflow for building a multimodal (text, image, video) sentiment prediction model. We will follow the 7-step process outlined, from raw data collection to a final, usable prediction pipeline.\n",
    "\n",
    "**Workflow Overview:**\n",
    "\n",
    "1.  **Step 0: Preparation:** Install libraries, set API keys, create folders.\n",
    "2.  **Step 1: Data Collection:** Scrape Reddit data and download all media locally.\n",
    "3.  **Step 2: AI-Powered Labeling:** Use the Gemini API to create the \"golden dataset.\"\n",
    "4.  **Step 3: Data Splitting:** Create `train`, `validation`, and `test` sets.\n",
    "5.  **Step 4: Phase 1 Training:** Fine-tune specialist models (Text, Image, Video).\n",
    "6.  **Step 5: Phase 2 Training:** Train the fusion model on embeddings.\n",
    "7.  **Step 6: Evaluation:** Test the final system on unseen data.\n",
    "8.  **Step 7: Prediction:** Build the final inference function for new, raw posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Preparation\n",
    "\n",
    "First, we set up our environment. This involves installing all necessary libraries, setting up our API keys, and creating the directories where we'll store our data and media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8423d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell in your terminal within your virtual environment:\n",
    "\n",
    "# !pip install praw pandas requests google-generativeai scikit-learn transformers torch torchvision opencv-python-headless tqdm seaborn matplotlib pmaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52897ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import praw                     # For Reddit scraping\n",
    "import pandas as pd             # For data manipulation\n",
    "import requests                 # For downloading files\n",
    "import os                       # For file/directory operations\n",
    "import json                     # For handling API responses\n",
    "from tqdm.auto import tqdm      # For progress bars\n",
    "import google.generativeai as genai  # For Gemini API\n",
    "from sklearn.model_selection import train_test_split # For splitting data\n",
    "import cv2                      # For video processing (if needed)\n",
    "import time\n",
    "from pmaw import PushshiftAPI   # Import for Pushshift scraping\n",
    "import datetime as dt           # For defining date ranges\n",
    "\n",
    "# --- API Keys & Config ---\n",
    "# !! IMPORTANT: Replace with your actual API keys\n",
    "# !! Best practice: Store these in a .env file and use python-dotenv to load them\n",
    "REDDIT_CLIENT_ID = \"YOUR_CLIENT_ID_HERE\"\n",
    "REDDIT_CLIENT_SECRET = \"YOUR_CLIENT_SECRET_HERE\"\n",
    "REDDIT_USER_AGENT = \"BrawlStars Sentiment Scraper v1.0 by /u/YOUR_USERNAME\"\n",
    "GEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"\n",
    "\n",
    "# --- Project Constants ---\n",
    "SUBREDDIT_NAME = \"Brawlstars\"\n",
    "POST_LIMIT = 1200  # Target number of posts (PMAW might return more or less)\n",
    "LABEL_TARGET = 1000\n",
    "\n",
    "# --- File & Directory Setup ---\n",
    "MEDIA_DIR = \"media\"\n",
    "IMAGE_DIR = os.path.join(MEDIA_DIR, \"images\")\n",
    "VIDEO_DIR = os.path.join(MEDIA_DIR, \"videos\")\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "RAW_DATA_CSV = os.path.join(DATA_DIR, 'raw_data.csv')\n",
    "LABELED_DATA_CSV = os.path.join(DATA_DIR, 'labeled_data_1k.csv')\n",
    "TRAIN_SET_CSV = os.path.join(DATA_DIR, 'train_set.csv')\n",
    "VALIDATION_SET_CSV = os.path.join(DATA_DIR, 'validation_set.csv')\n",
    "TEST_SET_CSV = os.path.join(DATA_DIR, 'test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb32e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48751e",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection (Scraping and Downloading)\n",
    "\n",
    "Here, we connect to the Reddit API using PRAW, scrape the latest posts from r/BrawlStars, and—most importantly—download the associated image or video for each post. We save the *local path* to this media in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96670e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== CELL 1.A: Scraping with PRAW (Original) ==========\n",
    "\n",
    "# Initialize PRAW (Reddit API client)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT,\n",
    ")\n",
    "\n",
    "print(reddit.user.me()) # Should show 'None' if using read-only (script) auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cdeed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_media_praw(post):\n",
    "    \"\"\"\n",
    "    Downloads the media (image or video) for a PRAW post and returns the local file path.\n",
    "    Relies on PRAW's post object structure.\n",
    "    \"\"\"\n",
    "    post_hint = getattr(post, 'post_hint', None)\n",
    "    media_url = None\n",
    "    local_path = None\n",
    "    file_ext = \".unknown\"\n",
    "\n",
    "    try:\n",
    "        if post_hint == 'image':\n",
    "            media_url = post.url\n",
    "            file_ext = os.path.splitext(media_url)[1]\n",
    "            # Basic check for common image extensions\n",
    "            if not file_ext or file_ext.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:\n",
    "                file_ext = \".jpg\" # Default if no/unrecognized extension\n",
    "            local_path = os.path.join(IMAGE_DIR, f\"{post.id}{file_ext}\")\n",
    "\n",
    "        elif post_hint == 'hosted:video':\n",
    "            # PRAW provides a direct fallback URL\n",
    "            if post.media and 'reddit_video' in post.media and post.media['reddit_video']:\n",
    "                media_url = post.media['reddit_video']['fallback_url']\n",
    "                file_ext = \".mp4\"\n",
    "                local_path = os.path.join(VIDEO_DIR, f\"{post.id}{file_ext}\")\n",
    "            else:\n",
    "                print(f\"Warning: Post {post.id} hint is hosted:video but no media found.\")\n",
    "                return None # Skip if media info is missing\n",
    "        \n",
    "        elif post_hint == 'rich:video':\n",
    "            # Skip external videos (YouTube, etc.)\n",
    "            pass\n",
    "\n",
    "        # If we have a URL and a path, download the file\n",
    "        if media_url and local_path:\n",
    "            if os.path.exists(local_path):\n",
    "                # print(f\"Media already exists: {local_path}\")\n",
    "                return local_path # Already downloaded\n",
    "\n",
    "            # print(f\"Downloading {media_url} to {local_path}\")\n",
    "            response = requests.get(media_url, stream=True, timeout=30) # Added timeout\n",
    "            response.raise_for_status() \n",
    "            \n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            return local_path\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error downloading media for post {post.id}: {e}\")\n",
    "        # Clean up potentially incomplete file\n",
    "        if local_path and os.path.exists(local_path):\n",
    "           try: os.remove(local_path) \n",
    "           except OSError: pass \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing media for post {post.id} (URL: {getattr(post, 'url', 'N/A')} Hint: {post_hint}): {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None # No downloadable media identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Starting PRAW scrape of {POST_LIMIT} posts from r/{SUBREDDIT_NAME}...\")\n",
    "\n",
    "all_posts_data_praw = []\n",
    "subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
    "\n",
    "# Use tqdm for a progress bar\n",
    "for post in tqdm(subreddit.hot(limit=POST_LIMIT), total=POST_LIMIT, desc=\"Scraping (PRAW)\"):\n",
    "    try:\n",
    "        # 1. Download media and get local path using the PRAW-specific function\n",
    "        local_media_path = download_media_praw(post)\n",
    "        \n",
    "        # 2. Store all relevant data\n",
    "        post_data = {\n",
    "            'id': post.id,\n",
    "            'title': post.title,\n",
    "            'text': post.selftext,\n",
    "            'url': post.url, # The original URL (to image, or to post)\n",
    "            'permalink': post.permalink,\n",
    "            'score': post.score,\n",
    "            'created_utc': post.created_utc,\n",
    "            'post_hint': getattr(post, 'post_hint', 'text_only'),\n",
    "            'local_media_path': local_media_path # Our new, crucial column\n",
    "        }\n",
    "        all_posts_data_praw.append(post_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing post {post.id}: {e}\")\n",
    "\n",
    "# 3. Convert to DataFrame and save\n",
    "df_raw_praw = pd.DataFrame(all_posts_data_praw)\n",
    "df_raw_praw.to_csv(RAW_DATA_CSV, index=False) # Overwrite or use a different filename if needed\n",
    "\n",
    "print(f\"\\nSuccessfully scraped and processed {len(df_raw_praw)} posts using PRAW.\")\n",
    "print(f\"Raw data saved to: {RAW_DATA_CSV}\")\n",
    "\n",
    "# Show a summary of what we collected\n",
    "print(\"\\n--- PRAW Data Summary ---\")\n",
    "print(df_raw_praw.head())\n",
    "\n",
    "print(\"\\n--- PRAW Media Type Breakdown ---\")\n",
    "print(df_raw_praw['post_hint'].value_counts())\n",
    "\n",
    "print(\"\\n--- PRAW Downloaded Media Check ---\")\n",
    "print(f\"{df_raw_praw['local_media_path'].notna().sum()} posts have associated local media.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce64ba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'POST_LIMIT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ========== CELL 1.B: Scraping with PMAW (Alternative) ==========\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting PMAW scrape of ~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mPOST_LIMIT\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m posts from r/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUBREDDIT_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# --- PMAW Configuration ---\u001b[39;00m\n\u001b[0;32m      6\u001b[0m PMAW_POST_LIMIT \u001b[38;5;241m=\u001b[39m POST_LIMIT  \u001b[38;5;66;03m# Set how many posts PMAW should aim for\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'POST_LIMIT' is not defined"
     ]
    }
   ],
   "source": [
    "# ========== CELL 1.B: Scraping with PMAW (Alternative) ==========\n",
    "\n",
    "print(f\"Starting PMAW scrape of ~{POST_LIMIT} posts from r/{SUBREDDIT_NAME}...\")\n",
    "\n",
    "# --- PMAW Configuration ---\n",
    "PMAW_POST_LIMIT = POST_LIMIT  # Set how many posts PMAW should aim for\n",
    "# Optional: Define a date range (e.g., scrape posts from the last 30 days)\n",
    "# end_epoch = int(dt.datetime.now().timestamp())\n",
    "# start_epoch = int((dt.datetime.now() - dt.timedelta(days=30)).timestamp())\n",
    "\n",
    "# --- Initialize PMAW ---\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# --- Fields to retrieve from Pushshift ---\n",
    "# Select fields relevant to your task to minimize data transfer\n",
    "fields = [\n",
    "    'id',\n",
    "    'title',\n",
    "    'selftext', # Corresponds to 'text' in PRAW\n",
    "    'url',      # URL of the link or media\n",
    "    'permalink',\n",
    "    'score',\n",
    "    'created_utc',\n",
    "    'domain',   # Helps identify media type (e.g., i.redd.it, v.redd.it)\n",
    "    'is_video', # Boolean flag for videos\n",
    "    'media_metadata' # Sometimes contains image info\n",
    "]\n",
    "\n",
    "# --- Perform the search ---\n",
    "print(\"Querying Pushshift API...\")\n",
    "# Note: PMAW returns a generator. We need to convert it to a list.\n",
    "# This can take time depending on the limit and Pushshift's responsiveness.\n",
    "submissions_generator = api.search_submissions(\n",
    "    subreddit=SUBREDDIT_NAME,\n",
    "    limit=PMAW_POST_LIMIT,\n",
    "    # after=start_epoch, # Uncomment to use date range\n",
    "    # before=end_epoch,  # Uncomment to use date range\n",
    "    fields=fields,\n",
    "    sort='desc',      # Get newest posts first\n",
    "    sort_type='created_utc'\n",
    ")\n",
    "\n",
    "all_posts_data_pmaw = list(submissions_generator)\n",
    "\n",
    "if not all_posts_data_pmaw:\n",
    "    print(\"Error: No posts retrieved from Pushshift. Check parameters or API status.\")\n",
    "else:\n",
    "    print(f\"Retrieved {len(all_posts_data_pmaw)} posts from Pushshift.\")\n",
    "\n",
    "    # --- Convert to DataFrame ---\n",
    "    df_raw_pmaw = pd.DataFrame(all_posts_data_pmaw)\n",
    "\n",
    "    # --- Adapt PMAW data to match PRAW structure (where possible) ---\n",
    "    df_raw_pmaw = df_raw_pmaw.rename(columns={'selftext': 'text'})\n",
    "\n",
    "    # --- Infer 'post_hint' (simplified) ---\n",
    "    # This is a basic inference; PRAW's 'post_hint' is more reliable.\n",
    "    def infer_post_hint(row):\n",
    "        if row.get('is_video', False):\n",
    "            return 'hosted:video' # Assume v.redd.it videos are 'hosted'\n",
    "        elif 'i.redd.it' in row.get('domain', '') or 'i.imgur.com' in row.get('domain', ''):\n",
    "            return 'image'\n",
    "        elif not pd.isna(row.get('text', None)) and row.get('text', '').strip():\n",
    "             # If there's non-empty selftext, prioritize as text_only even if there's a link\n",
    "            return 'text_only' \n",
    "        elif 'reddit.com' in row.get('url', ''):\n",
    "            # If the URL points back to reddit and no text/image/video, likely text_only or link to another post\n",
    "            return 'text_only' \n",
    "        else:\n",
    "            return 'link' # Default for other links\n",
    "\n",
    "    if not df_raw_pmaw.empty:\n",
    "        df_raw_pmaw['post_hint'] = df_raw_pmaw.apply(infer_post_hint, axis=1)\n",
    "    else:\n",
    "        df_raw_pmaw['post_hint'] = None # Handle empty DataFrame case\n",
    "        \n",
    "    # --- Download Media (Adapted for PMAW data) ---\n",
    "    # NOTE: Downloading v.redd.it videos reliably often requires external tools\n",
    "    # like youtube-dlp as Pushshift doesn't provide the fallback URL easily.\n",
    "    # This function will primarily handle images identified by URL.\n",
    "    def download_media_pmaw(row):\n",
    "        media_url = row.get('url', None)\n",
    "        post_id = row.get('id', 'unknown')\n",
    "        hint = row.get('post_hint', 'link')\n",
    "        local_path = None\n",
    "        file_ext = \".unknown\"\n",
    "\n",
    "        try:\n",
    "            if hint == 'image' and media_url:\n",
    "                file_ext = os.path.splitext(media_url)[1]\n",
    "                if not file_ext or file_ext.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:\n",
    "                    file_ext = \".jpg\"\n",
    "                local_path = os.path.join(IMAGE_DIR, f\"{post_id}{file_ext}\")\n",
    "            \n",
    "            elif hint == 'hosted:video' and media_url:\n",
    "                # Basic placeholder - downloading v.redd.it needs more work\n",
    "                print(f\"Info: Video detected for {post_id} (URL: {media_url}). Downloading v.redd.it requires additional logic (e.g., youtube-dlp) - skipping download.\")\n",
    "                # You might store the URL here anyway if your video model can handle URLs, \n",
    "                # or implement youtube-dlp download logic separately.\n",
    "                return None # Skip download for now\n",
    "                # Example placeholder if you wanted to try direct download (often fails for v.redd.it)\n",
    "                # file_ext = \".mp4\"\n",
    "                # local_path = os.path.join(VIDEO_DIR, f\"{post_id}{file_ext}\") \n",
    "            \n",
    "            # Download if it's an image and path is set\n",
    "            if hint == 'image' and local_path:\n",
    "                if os.path.exists(local_path):\n",
    "                    return local_path\n",
    "                \n",
    "                response = requests.get(media_url, stream=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                return local_path\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Network error downloading media for post {post_id}: {e}\")\n",
    "            if local_path and os.path.exists(local_path):\n",
    "               try: os.remove(local_path) \n",
    "               except OSError: pass\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing media for post {post_id} (URL: {media_url} Hint: {hint}): {e}\")\n",
    "            return None\n",
    "            \n",
    "        return None # No downloadable media identified/handled\n",
    "\n",
    "    print(\"\\nAttempting to download media identified by PMAW (primarily images)...\")\n",
    "    tqdm.pandas(desc=\"Downloading Media (PMAW)\") \n",
    "    if not df_raw_pmaw.empty:\n",
    "       df_raw_pmaw['local_media_path'] = df_raw_pmaw.progress_apply(download_media_pmaw, axis=1)\n",
    "    else:\n",
    "       df_raw_pmaw['local_media_path'] = None\n",
    "       \n",
    "    # --- Select and order columns to match PRAW output as closely as possible ---\n",
    "    final_columns = [\n",
    "        'id', 'title', 'text', 'url', 'permalink', 'score', \n",
    "        'created_utc', 'post_hint', 'local_media_path'\n",
    "    ]\n",
    "    # Add missing columns with None/NaN\n",
    "    for col in final_columns:\n",
    "        if col not in df_raw_pmaw.columns:\n",
    "            df_raw_pmaw[col] = None \n",
    "            \n",
    "    df_raw_pmaw = df_raw_pmaw[final_columns]\n",
    "\n",
    "    # --- Save the PMAW-sourced data ---\n",
    "    # Decide whether to overwrite the PRAW data or save to a new file\n",
    "    PMAW_RAW_DATA_CSV = os.path.join(DATA_DIR, 'raw_data_pmaw.csv') \n",
    "    df_raw_pmaw.to_csv(PMAW_RAW_DATA_CSV, index=False)\n",
    "    # Or uncomment below to overwrite the main raw file:\n",
    "    # df_raw_pmaw.to_csv(RAW_DATA_CSV, index=False) \n",
    "\n",
    "    print(f\"\\nPMAW data processing complete. Saved to: {PMAW_RAW_DATA_CSV}\")\n",
    "\n",
    "    # --- Show Summary ---\n",
    "    print(\"\\n--- PMAW Data Summary ---\")\n",
    "    print(df_raw_pmaw.head())\n",
    "\n",
    "    print(\"\\n--- PMAW Inferred Media Type Breakdown ---\")\n",
    "    print(df_raw_pmaw['post_hint'].value_counts())\n",
    "\n",
    "    print(\"\\n--- PMAW Downloaded Media Check ---\")\n",
    "    print(f\"{df_raw_pmaw['local_media_path'].notna().sum()} posts have associated local media (mostly images).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023598c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b9a2d",
   "metadata": {},
   "source": [
    "## Step 2: AI-Powered Labeling (Creating the \"Golden Dataset\")\n",
    "\n",
    "Now we take our raw data and use the Gemini API to generate sentiment labels. We will define a function that takes a post's text and its *local media file*, sends them to the API, and parses the JSON response. We'll apply this to our first 1,000 posts.\n",
    "\n",
    "**Note:** This step will make many API calls and may take a long time and cost money. We'll start by processing just a few posts as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa83c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Gemini API client\n",
    "try:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    # Use the multimodal-capable model\n",
    "    gemini_model = genai.GenerativeModel('gemini-2.5-flash') # Or 'gemini-1.5-pro'\n",
    "    print(\"Gemini model configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Gemini: {e}. Please check your API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the detailed few-shot prompt for the API\n",
    "# NOTE: The JSON examples use {{ and }} to escape the braces.\n",
    "\n",
    "LABELING_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert sentiment analyst for the game Brawl Stars. Your task is to analyze a Reddit post (which may include text, an image, and/or a video) and provide a structured JSON output.\n",
    "\n",
    "Analyze the user's sentiment and categorize the post. The user's post content is provided first, followed by the media.\n",
    "\n",
    "The 5 possible 'post_sentiment' values are:\n",
    "1.  **Joy**: Happiness, excitement, pride (e.g., getting a new Brawler, winning a hard match, liking a new skin).\n",
    "2.  **Anger**: Frustration, rage, annoyance (e.g., losing to a specific Brawler, bad teammates, game bugs, matchmaking issues).\n",
    "3.  **Sadness**: Disappointment, grief (e.g., missing a shot, losing a high-stakes game, a favorite Brawler getting nerfed).\n",
    "4.  **Surprise**: Shock, disbelief (e.g., a sudden clutch play, an unexpected new feature, a rare bug).\n",
    "5.  **Neutral/Other**: Objective discussion, questions, news, or art that doesn't convey a strong emotion.\n",
    "\n",
    "The 6 possible 'post_classification' values are:\n",
    "1.  **Gameplay Clip**: A video or image showing a match, a specific play, or a replay.\n",
    "2.  **Meme/Humor**: A meme, joke, or funny edit.\n",
    "3.  **Discussion**: A text-based post asking a question or starting a conversation.\n",
    "4.  **Feedback/Rant**: A post providing feedback, suggestions, or complaining about the game.\n",
    "5.  **Art/Concept**: Fan art, skin concepts, or creative edits.\n",
    "6.  **Achievement/Loot**: A screenshot of a new Brawler unlock, a high rank, or a Starr Drop reward.\n",
    "\n",
    "--- EXAMPLES ---\n",
    "\n",
    "[EXAMPLE 1]\n",
    "Post Text: \"This is the 5th time I've lost to an Edgar in a row. FIX YOUR GAME SUPERCELL!!\"\n",
    "Post Media: \n",
    "Output:\n",
    "{{\n",
    "  \"post_classification\": \"Feedback/Rant\",\n",
    "  \"post_sentiment\": \"Anger\",\n",
    "  \"sentiment_analysis\": \"The user is clearly angry, using all-caps ('FIX YOUR GAME') and expressing frustration at repeatedly losing to a specific Brawler (Edgar). The defeat screen image confirms the loss.\"\n",
    "}}\n",
    "\n",
    "[EXAMPLE 2]\n",
    "Post Text: \"I CAN'T BELIEVE I FINALLY GOT HIM!!\"\n",
    "Post Media: \n",
    "Output:\n",
    "{{\n",
    "  \"post_classification\": \"Achievement/Loot\",\n",
    "  \"post_sentiment\": \"Joy\",\n",
    "  \"sentiment_analysis\": \"The user is excited and happy, indicated by the all-caps text and the celebratory nature of unlocking a new legendary Brawler, which is a rare event.\"\n",
    "}}\n",
    "\n",
    "[EXAMPLE 3]\n",
    "Post Text: \"Check out this insane 1v3 I pulled off with Mortis\"\n",
    "Post Media: [Video showing a fast-paced gameplay clip where the player (Mortis) defeats three opponents]\n",
    "Output:\n",
    "{{\n",
    "  \"post_classification\": \"Gameplay Clip\",\n",
    "  \"post_sentiment\": \"Joy\",\n",
    "  \"sentiment_analysis\": \"The user is proud and excited about their 'insane 1v3' play. This is a clear expression of joy and pride in their own skill. The video clip demonstrates the achievement.\"\n",
    "}}\n",
    "\n",
    "--- TASK ---\n",
    "\n",
    "Analyze the following post and provide ONLY the JSON output. Do not include '```json' or any other text outside the JSON block.\n",
    "\n",
    "[POST CONTENT]\n",
    "Title: {post_title}\n",
    "Text: {post_text}\n",
    "\n",
    "[POST MEDIA]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0246d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_label(post_row):\n",
    "    \"\"\"\n",
    "    Takes a row from the DataFrame, sends its text and local media to Gemini,\n",
    "    and returns the raw JSON string response.\n",
    "    \"\"\"\n",
    "    post_title = post_row['title']\n",
    "    post_text = post_row['text']\n",
    "    media_path = post_row['local_media_path']\n",
    "    post_hint = post_row['post_hint']\n",
    "\n",
    "    # 1. Format the text part of the prompt\n",
    "    # Handle potential NaN values in text/title before formatting\n",
    "    safe_title = str(post_title) if pd.notna(post_title) else \"\"\n",
    "    safe_text = str(post_text) if pd.notna(post_text) else \"\"\n",
    "    prompt = LABELING_PROMPT_TEMPLATE.format(post_title=safe_title, post_text=safe_text)\n",
    "\n",
    "    \n",
    "    # 2. Prepare the media part\n",
    "    media_payload = []\n",
    "    uploaded_file_resource = None # Keep track of the file to delete later\n",
    "    if pd.notna(media_path) and os.path.exists(media_path):\n",
    "        try:\n",
    "            # Use genai.upload_file for persistent storage and retrieval\n",
    "            # print(f\"Uploading {media_path}...\")\n",
    "            uploaded_file_resource = genai.upload_file(path=media_path)\n",
    "            media_payload.append(uploaded_file_resource)\n",
    "            \n",
    "            # Wait for the file to be processed, especially important for videos\n",
    "            # Add a timeout to prevent infinite loops\n",
    "            processing_timeout = 60 # seconds\n",
    "            start_time = time.time()\n",
    "            while uploaded_file_resource.state.name == \"PROCESSING\":\n",
    "                if time.time() - start_time > processing_timeout:\n",
    "                     print(f\"Warning: File processing timed out for {media_path}\")\n",
    "                     # Clean up the timed-out file\n",
    "                     if uploaded_file_resource: genai.delete_file(uploaded_file_resource.name)\n",
    "                     return {\"error\": \"File processing timed out\"}\n",
    "                time.sleep(2)\n",
    "                uploaded_file_resource = genai.get_file(uploaded_file_resource.name)\n",
    "            \n",
    "            if uploaded_file_resource.state.name == \"FAILED\":\n",
    "                print(f\"File upload failed: {media_path}\")\n",
    "                # No need to delete if it failed during upload/processing\n",
    "                return {\"error\": \"File upload failed\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading/processing file {media_path}: {e}\")\n",
    "            # Attempt cleanup if resource exists\n",
    "            if uploaded_file_resource: \n",
    "                try: genai.delete_file(uploaded_file_resource.name)\n",
    "                except Exception: pass # Ignore delete error if upload failed badly\n",
    "            return {\"error\": str(e)}\n",
    "    else:\n",
    "        media_payload.append(\"No media provided.\")\n",
    "\n",
    "    # 3. Combine prompt and media and make the API call\n",
    "    try:\n",
    "        full_prompt = [prompt] + media_payload\n",
    "        response = gemini_model.generate_content(full_prompt)\n",
    "        \n",
    "        # Clean up the successfully processed file\n",
    "        if uploaded_file_resource: \n",
    "            genai.delete_file(uploaded_file_resource.name)\n",
    "        \n",
    "        # Return the clean text response, ready for JSON parsing\n",
    "        return response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini API call for post {post_row['id']}: {e}\")\n",
    "        # Clean up if an API call error occurred after upload\n",
    "        if uploaded_file_resource: \n",
    "           try: genai.delete_file(uploaded_file_resource.name)\n",
    "           except Exception: pass\n",
    "        return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2 (Revised): AI-Powered Labeling (Interruptible & Resumable) ---\n",
    "\n",
    "# --- Choose which raw data file to use for labeling --- \n",
    "# RAW_DATA_TO_LABEL = RAW_DATA_CSV # Use PRAW data\n",
    "RAW_DATA_TO_LABEL = os.path.join(DATA_DIR, 'raw_data_pmaw.csv') # Use PMAW data\n",
    "\n",
    "# --- 1. Define How Many New Posts to Label in This Batch ---\n",
    "NEW_LABEL_TARGET = 500 \n",
    "\n",
    "# --- 2. Find Out What's Already Labeled ---\n",
    "try:\n",
    "    df_old_labeled = pd.read_csv(LABELED_DATA_CSV)\n",
    "    already_labeled_ids = set(df_old_labeled['id'])\n",
    "    print(f\"Loaded {len(df_old_labeled)} previously labeled posts from {LABELED_DATA_CSV}.\")\n",
    "except FileNotFoundError:\n",
    "    df_old_labeled = pd.DataFrame() # Create an empty one if it's the first run\n",
    "    already_labeled_ids = set()\n",
    "    print(f\"No existing file found at {LABELED_DATA_CSV}. Starting from scratch.\")\n",
    "\n",
    "# --- 3. Find Out What's Left to Label ---\n",
    "try:\n",
    "    df_all_raw = pd.read_csv(RAW_DATA_TO_LABEL)\n",
    "    print(f\"Reading raw data from: {RAW_DATA_TO_LABEL}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Raw data file not found at {RAW_DATA_TO_LABEL}. Please run Step 1 first.\")\n",
    "    raise\n",
    "    \n",
    "# Filter out posts we've already labeled\n",
    "df_to_label = df_all_raw[~df_all_raw['id'].isin(already_labeled_ids)].copy()\n",
    "\n",
    "# Limit this run to the NEW_LABEL_TARGET\n",
    "df_to_label = df_to_label.head(NEW_LABEL_TARGET)\n",
    "\n",
    "if len(df_to_label) == 0:\n",
    "    print(\"No new posts to label based on the selected raw data file.\")\n",
    "else:\n",
    "    print(f\"Found {len(df_to_label)} new posts to label. Starting labeling...\")\n",
    "    \n",
    "    # --- 4. Label New Posts (Interruptible Loop) ---\n",
    "    df_to_label['gemini_raw_json'] = None # Create the column to fill\n",
    "    \n",
    "    try:\n",
    "        # Use iterrows() for a row-by-row, interruptible loop\n",
    "        for index, row in tqdm(df_to_label.iterrows(), total=len(df_to_label), desc=\"Generating AI Labels\"):\n",
    "            json_response = get_gemini_label(row)\n",
    "            \n",
    "            # Save result immediately to the DataFrame\n",
    "            # Use .loc for safer assignment\n",
    "            df_to_label.loc[index, 'gemini_raw_json'] = json_response\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n--- INTERRUPTED BY USER ---\")\n",
    "        print(\"Labeling process stopped. Will parse and save completed posts...\")\n",
    "\n",
    "    finally:\n",
    "        # --- 5. Process and Parse Whatever Was Completed ---\n",
    "        \n",
    "        # Filter to only the rows that were actually processed in this run\n",
    "        df_newly_processed = df_to_label.dropna(subset=['gemini_raw_json']).copy()\n",
    "        \n",
    "        if len(df_newly_processed) == 0:\n",
    "            print(\"No new posts were successfully labeled in this session.\")\n",
    "        else:\n",
    "            print(f\"\\nParsing {len(df_newly_processed)} newly labeled posts...\")\n",
    "            \n",
    "            # --- JSON Parsing ---\n",
    "            parsed_labels = []\n",
    "            for index, row in df_newly_processed.iterrows():\n",
    "                raw_json = row['gemini_raw_json']\n",
    "                label_entry = {'id': row['id'], 'labeling_error': None}\n",
    "                try:\n",
    "                    # Check if it's an error dictionary from get_gemini_label\n",
    "                    if isinstance(raw_json, dict) and 'error' in raw_json:\n",
    "                        label_entry['labeling_error'] = str(raw_json['error'])\n",
    "                    elif isinstance(raw_json, str):\n",
    "                        # Clean up potential markdown blocks and extra whitespace\n",
    "                        clean_json_str = raw_json.strip().lstrip('```json').rstrip('```').strip()\n",
    "                        data = json.loads(clean_json_str)\n",
    "                        label_entry['post_classification'] = data.get('post_classification')\n",
    "                        label_entry['post_sentiment'] = data.get('post_sentiment')\n",
    "                        label_entry['sentiment_analysis'] = data.get('sentiment_analysis')\n",
    "                    else:\n",
    "                        # Handle unexpected data types\n",
    "                        label_entry['labeling_error'] = f\"Unexpected data type in gemini_raw_json: {type(raw_json)}\"\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    label_entry['labeling_error'] = f\"JSON Decode Error: {e} - Raw: {raw_json[:100]}...\" # Log snippet\n",
    "                except Exception as e:\n",
    "                    label_entry['labeling_error'] = f\"General Error: {str(e)}\"\n",
    "                \n",
    "                parsed_labels.append(label_entry)\n",
    "\n",
    "            # Convert parsed data to a DataFrame and merge with the newly processed info\n",
    "            df_labels = pd.DataFrame(parsed_labels)\n",
    "            # Merge, keeping all columns from df_newly_processed\n",
    "            df_new_labeled_final = pd.merge(df_newly_processed.drop(columns=['gemini_raw_json']), df_labels, on='id', how='left')\n",
    "\n",
    "            # Filter out posts that had errors during labeling or parsing\n",
    "            df_new_golden = df_new_labeled_final[df_new_labeled_final['labeling_error'].isna()].copy()\n",
    "            df_errors = df_new_labeled_final[df_new_labeled_final['labeling_error'].notna()]\n",
    "\n",
    "            print(f\"Successfully parsed {len(df_new_golden)} new labels.\")\n",
    "            if len(df_errors) > 0:\n",
    "                print(f\"{len(df_errors)} posts had errors during labeling/parsing and will be skipped.\")\n",
    "                # Optional: print some errors for debugging\n",
    "                # print(\"Example errors:\")\n",
    "                # print(df_errors[['id', 'labeling_error']].head())\n",
    "\n",
    "            # --- 6. Combine Old and New Datasets and Save ---\n",
    "            if len(df_new_golden) > 0:\n",
    "                 # Ensure columns match before concatenating\n",
    "                if not df_old_labeled.empty:\n",
    "                    # Align columns based on the existing labeled data file\n",
    "                    cols_existing = df_old_labeled.columns\n",
    "                    cols_new = df_new_golden.columns\n",
    "                    \n",
    "                    # Add missing columns to new data (filled with NaN)\n",
    "                    for col in cols_existing:\n",
    "                        if col not in cols_new:\n",
    "                            df_new_golden[col] = None\n",
    "                            \n",
    "                    # Add missing columns to old data (unlikely but possible)\n",
    "                    for col in cols_new:\n",
    "                         if col not in cols_existing:\n",
    "                            df_old_labeled[col] = None\n",
    "                    \n",
    "                    # Reorder new data columns to match old data\n",
    "                    df_new_golden = df_new_golden[cols_existing]\n",
    "                else:\n",
    "                     # If it's the first run, define the columns based on the new data\n",
    "                     df_old_labeled = pd.DataFrame(columns=df_new_golden.columns)\n",
    "\n",
    "                # Concatenate the old labeled data with the new golden data\n",
    "                df_combined = pd.concat([df_old_labeled, df_new_golden], ignore_index=True)\n",
    "                \n",
    "                # Save the final \"golden dataset\"\n",
    "                df_combined.to_csv(LABELED_DATA_CSV, index=False)\n",
    "                print(f\"\\nGolden dataset updated. Total labeled posts: {len(df_combined)}\")\n",
    "                print(f\"Saved to: {LABELED_DATA_CSV}\")\n",
    "\n",
    "                # Display results\n",
    "                print(\"\\n--- Updated Labeled Data Head (Last 5 rows) ---\")\n",
    "                print(df_combined[['id', 'title', 'post_sentiment', 'post_classification']].tail())\n",
    "\n",
    "                print(\"\\n--- Updated Sentiment Distribution ---\")\n",
    "                print(df_combined['post_sentiment'].value_counts())\n",
    "            else:\n",
    "                print(\"No new valid labels were generated in this session. Labeled file remains unchanged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c737ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db94b07",
   "metadata": {},
   "source": [
    "## Step 3: Data Splitting\n",
    "\n",
    "We split our `labeled_data_1k.csv` into three distinct sets: `train_set` (for training), `validation_set` (for tuning), and `test_set` (for final evaluation). We use **stratification** on the `post_sentiment` column to ensure all three sets have a similar distribution of emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384dc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the golden dataset\n",
    "try:\n",
    "    df_labeled = pd.read_csv(LABELED_DATA_CSV)\n",
    "    print(f\"Loaded {len(df_labeled)} labeled posts from {LABELED_DATA_CSV}\")\n",
    "    \n",
    "    # --- Data Cleaning before split ---\n",
    "    # Drop rows where post_sentiment might be missing (if any labeling errors occurred)\n",
    "    df_labeled.dropna(subset=['post_sentiment'], inplace=True)\n",
    "    print(f\"Using {len(df_labeled)} posts with valid sentiment labels for splitting.\")\n",
    "    \n",
    "    if len(df_labeled) < 10: # Need enough data to split reasonably\n",
    "        print(\"Warning: Very few labeled posts. Splitting might result in tiny datasets.\")\n",
    "        # Handle this case - maybe stop or adjust split sizes\n",
    "        # For now, we'll proceed but be aware of the issue.\n",
    "\n",
    "    # Define split sizes\n",
    "    TEST_SIZE = 0.10  # 10% for the final test set\n",
    "    VALIDATION_SIZE = 0.10 # 10% for the validation set (relative to original size)\n",
    "\n",
    "    # Check if there's enough data for stratification (at least 2 samples per class needed for sklearn)\n",
    "    sentiment_counts = df_labeled['post_sentiment'].value_counts()\n",
    "    if (sentiment_counts < 2).any():\n",
    "        print(\"Warning: Some sentiment classes have fewer than 2 samples.\")\n",
    "        print(\"Stratification might fail or be unreliable. Consider labeling more data for rare classes.\")\n",
    "        print(sentiment_counts)\n",
    "        # Optional: Proceed without stratification if necessary, though less ideal\n",
    "        # stratify_param = None \n",
    "        stratify_param = df_labeled['post_sentiment']\n",
    "    else:\n",
    "        stratify_param = df_labeled['post_sentiment']\n",
    "        \n",
    "    # Adjust sizes if total data is very small\n",
    "    n_total = len(df_labeled)\n",
    "    n_test = max(1, int(n_total * TEST_SIZE)) # Ensure at least 1 test sample\n",
    "    n_val = max(1, int(n_total * VALIDATION_SIZE)) # Ensure at least 1 val sample\n",
    "    \n",
    "    # Adjust relative validation size based on actual counts\n",
    "    if n_total - n_test <= 0: # Handle edge case where test size >= total size\n",
    "       print(\"Error: Not enough data to create a non-empty training/validation set after test split.\")\n",
    "       # You might want to raise an error here or adjust sizes further\n",
    "       relative_val_size = 0.5 # Default fallback\n",
    "    else:\n",
    "        relative_val_size = n_val / (n_total - n_test)\n",
    "\n",
    "    # 1. Split off the test set\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df_labeled,\n",
    "        test_size=n_test, # Use calculated count\n",
    "        random_state=42,\n",
    "        stratify=stratify_param # Use the determined stratify parameter\n",
    "    )\n",
    "\n",
    "    # 2. Split the remaining into train and validation\n",
    "    # Need to check stratification again for the smaller train_val_df\n",
    "    if not train_val_df.empty:\n",
    "        train_val_sentiment_counts = train_val_df['post_sentiment'].value_counts()\n",
    "        if (train_val_sentiment_counts < 2).any():\n",
    "             print(\"Warning: Stratifying train/validation split may be unreliable due to small class counts in the remainder.\")\n",
    "             stratify_train_val = None # Fallback to no stratification\n",
    "        else:\n",
    "             stratify_train_val = train_val_df['post_sentiment']\n",
    "             \n",
    "        train_df, val_df = train_test_split(\n",
    "            train_val_df,\n",
    "            test_size=relative_val_size, # Use calculated relative size\n",
    "            random_state=42,\n",
    "            stratify=stratify_train_val\n",
    "        )\n",
    "    else:\n",
    "        # Handle case where train_val_df is empty (shouldn't happen with max(1) adjustments)\n",
    "        train_df = pd.DataFrame(columns=df_labeled.columns)\n",
    "        val_df = pd.DataFrame(columns=df_labeled.columns)\n",
    "        print(\"Warning: train_val_df was empty after test split.\")\n",
    "\n",
    "\n",
    "    # 3. Save the splits\n",
    "    train_df.to_csv(TRAIN_SET_CSV, index=False)\n",
    "    val_df.to_csv(VALIDATION_SET_CSV, index=False)\n",
    "    test_df.to_csv(TEST_SET_CSV, index=False)\n",
    "\n",
    "    # 4. Report the results\n",
    "    print(\"\\nData splitting complete:\")\n",
    "    print(f\"  Training set:   {len(train_df)} rows -> {TRAIN_SET_CSV}\")\n",
    "    print(f\"  Validation set: {len(val_df)} rows -> {VALIDATION_SET_CSV}\")\n",
    "    print(f\"  Test set:       {len(test_df)} rows -> {TEST_SET_CSV}\")\n",
    "\n",
    "    # Check distributions (use normalize=True for percentages)\n",
    "    if not train_df.empty:\n",
    "        print(\"\\nTraining Set Sentiment Distribution:\")\n",
    "        print(train_df['post_sentiment'].value_counts(normalize=True))\n",
    "    if not val_df.empty:\n",
    "        print(\"\\nValidation Set Sentiment Distribution:\")\n",
    "        print(val_df['post_sentiment'].value_counts(normalize=True))\n",
    "    if not test_df.empty:\n",
    "        print(\"\\nTest Set Sentiment Distribution:\")\n",
    "        print(test_df['post_sentiment'].value_counts(normalize=True))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Labeled data file not found at {LABELED_DATA_CSV}\")\n",
    "    print(\"Please run Step 2 successfully before running Step 3.\")\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred during data splitting: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba03e16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c0b86",
   "metadata": {},
   "source": [
    "## Step 4: Phase 1 Training (Fine-Tuning the Specialists)\n",
    "\n",
    "This is where we build our three specialist models. Each model (Text, Image, Video) is fine-tuned *independently* to predict the **overall post sentiment**. This teaches them what features in their modality (e.g., words, pixels, motion) correspond to emotions like \"Joy\" or \"Anger\" in the context of Brawl Stars.\n",
    "\n",
    "**Note:** The code below provides the *structure* and *placeholders*. You will need to implement the detailed PyTorch/TensorFlow logic (Datasets, DataLoaders, model definitions, training loops, and evaluation loops) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Common Setup for Phase 1 ---\n",
    "import torch\n",
    "\n",
    "# Check if train_df exists from Step 3, otherwise load it\n",
    "if 'train_df' not in locals() or train_df.empty:\n",
    "    try:\n",
    "        train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "        if train_df.empty:\n",
    "             raise ValueError(\"Loaded train_set.csv is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {TRAIN_SET_CSV} not found. Please run Step 3 successfully.\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "         print(f\"Error: {e} Check {TRAIN_SET_CSV}.\")\n",
    "         raise\n",
    "\n",
    "# Make sure sentiment labels are derived correctly even if some classes were dropped during split\n",
    "if 'df_labeled' not in locals() or df_labeled.empty:\n",
    "    try:\n",
    "         df_labeled = pd.read_csv(LABELED_DATA_CSV)\n",
    "         df_labeled.dropna(subset=['post_sentiment'], inplace=True)\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Error: Cannot define labels. {LABELED_DATA_CSV} not found.\")\n",
    "         raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {LABELED_DATA_CSV}: {e}\")\n",
    "        raise\n",
    "        \n",
    "SENTIMENT_LABELS = sorted(df_labeled['post_sentiment'].unique())\n",
    "if not SENTIMENT_LABELS:\n",
    "    raise ValueError(\"No valid sentiment labels found in the labeled data.\")\n",
    "\n",
    "label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS)}\n",
    "NUM_LABELS = len(SENTIMENT_LABELS)\n",
    "\n",
    "print(f\"Found {NUM_LABELS} labels based on {LABELED_DATA_CSV}: {SENTIMENT_LABELS}\")\n",
    "print(f\"Label map: {label_to_id}\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEXT_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "IMAGE_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "VIDEO_PROCESSOR_NAME = \"MCG-NJU/videomae-base\" \n",
    "VIDEO_MODEL_NAME = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36525797",
   "metadata": {},
   "source": [
    "### 4.A: Text Model (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17907de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.A: Text Model (DistilBERT) ---\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"--- Starting Phase 1: Text Model Training ---\")\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "TEXT_MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MODEL_SAVE_PATH = \"./models/text_specialist\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5  # Start with 3-5, you can increase if needed\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 128 # Max token length for a post\n",
    "\n",
    "# Ensure model save directory exists\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Create Label Maps ---\n",
    "try:\n",
    "    # These should have been loaded or created in the common setup cell\n",
    "    if 'train_df' not in locals() or train_df.empty:\n",
    "        train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    if 'val_df' not in locals() or val_df.empty:\n",
    "        val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "        \n",
    "    # Ensure labels defined in common setup are used\n",
    "    if 'label_to_id' not in locals() or not label_to_id:\n",
    "        raise ValueError(\"Label map not defined in common setup cell.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(f\"Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    print(\"Please run Step 3: Data Splitting successfully.\")\n",
    "    raise\n",
    "except ValueError as e:\n",
    "     print(f\"Error: {e}\")\n",
    "     raise\n",
    "\n",
    "print(f\"Using {NUM_LABELS} labels for text model: {label_to_id}\")\n",
    "\n",
    "# --- 3. Define Custom PyTorch Dataset ---\n",
    "class TextSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, label_map, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        # Combine title and text for a richer input, handle NaNs\n",
    "        self.texts = (dataframe['title'].fillna('') + \" [SEP] \" + dataframe['text'].fillna('')).tolist()\n",
    "        # Map labels, handle potential missing labels gracefully (e.g., assign -1 or skip)\n",
    "        self.labels = dataframe['post_sentiment'].map(label_map).fillna(-1).tolist() # Use -1 for missing/unmappable labels\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Filter out rows with invalid labels (-1)\n",
    "        self.valid_indices = [i for i, label in enumerate(self.labels) if label != -1]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the count of valid samples only\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Map the requested index to the valid sample index\n",
    "        original_index = self.valid_indices[index]\n",
    "        \n",
    "        text = str(self.texts[original_index])\n",
    "        label = self.labels[original_index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 4. Initialize Tokenizer, Datasets, and DataLoaders ---\n",
    "print(\"Loading tokenizer and building datasets...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "train_dataset = TextSentimentDataset(train_df, tokenizer, label_to_id, MAX_LEN)\n",
    "val_dataset = TextSentimentDataset(val_df, tokenizer, label_to_id, MAX_LEN)\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    raise ValueError(\"Training dataset is empty after filtering invalid labels. Check data splitting and labeling.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Text Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Text Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# --- 5. Load Model, Optimizer, and Scheduler ---\n",
    "print(f\"Loading pre-trained model: {TEXT_MODEL_NAME}\")\n",
    "text_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    TEXT_MODEL_NAME, \n",
    "    num_labels=NUM_LABELS\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(text_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0, # Default, no warmup\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 6. The Training & Validation Loop ---\n",
    "print(\"--- Starting Training ---\")\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1} / {EPOCHS} ---\")\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    text_model.train()\n",
    "    total_train_loss = 0\n",
    "    train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch in train_progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Clear old gradients\n",
    "        text_model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # Get loss and logits\n",
    "        loss = outputs.loss\n",
    "        # Check if loss is valid (not NaN)\n",
    "        if torch.isnan(loss):\n",
    "             print(\"Warning: NaN loss detected during training. Skipping batch.\")\n",
    "             continue # Skip backprop for this batch\n",
    "             \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(text_model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    if len(train_loader) > 0:\n",
    "      avg_train_loss = total_train_loss / len(train_loader)\n",
    "      print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    else:\n",
    "      print(\"No batches processed in training.\")\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    if len(val_loader) == 0:\n",
    "        print(\"Validation dataset is empty. Skipping validation.\")\n",
    "        # Optionally save model even without validation if needed\n",
    "        # text_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        # tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "        continue # Skip to next epoch\n",
    "        \n",
    "    text_model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad(): # No need to calculate gradients\n",
    "        for batch in val_progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = text_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            if loss is not None and not torch.isnan(loss):\n",
    "                total_val_loss += loss.item()\n",
    "            else:\n",
    "                # Handle cases where loss might be None or NaN during eval\n",
    "                 print(\"Warning: Invalid loss encountered during validation.\")\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels_cpu)\n",
    "\n",
    "    # Ensure there are labels to calculate metrics on\n",
    "    if not all_labels:\n",
    "        print(\"No valid labels found in validation set for metric calculation.\")\n",
    "        continue\n",
    "        \n",
    "    avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # --- Save the Best Model ---\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        print(\"New best model! Saving...\")\n",
    "        text_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- Text Model Training Complete ---\")\n",
    "if best_val_accuracy > 0:\n",
    "   print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "   print(f\"Model and tokenizer saved to: {MODEL_SAVE_PATH}\")\n",
    "else:\n",
    "   print(\"No best model saved (validation accuracy did not improve or validation was skipped).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e61ae2",
   "metadata": {},
   "source": [
    "### 4.B: Image Model (ResNet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.B: Image Model (CLIP-Vision) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    CLIPImageProcessor, \n",
    "    CLIPVisionModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "print(\"--- Starting Phase 1: Image Model Training (CLIP) ---\")\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "IMAGE_MODEL_NAME = \"openai/clip-vit-base-patch32\" \n",
    "MODEL_SAVE_PATH = \"./models/image_specialist.pth\" # Save state dict\n",
    "BATCH_SIZE = 8 # Use a SMALLER batch size for images\n",
    "EPOCHS = 10 # Train for a few more epochs since the dataset is tiny\n",
    "LEARNING_RATE = 1e-5 # Use a smaller LR for fine-tuning vision models\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Re-create Label Maps ---\n",
    "try:\n",
    "    if 'train_df' not in locals() or train_df.empty:\n",
    "        train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    if 'val_df' not in locals() or val_df.empty:\n",
    "        val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "    if 'label_to_id' not in locals() or not label_to_id:\n",
    "         raise ValueError(\"Label map not defined in common setup cell.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    raise\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Using {NUM_LABELS} labels for image model: {label_to_id}\")\n",
    "\n",
    "# --- 3. Define Custom Image Dataset ---\n",
    "class ImageSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, label_map):\n",
    "        self.processor = processor\n",
    "        self.label_map = label_map\n",
    "        \n",
    "        # Filter for valid image paths and map labels\n",
    "        valid_data = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            path = row['local_media_path']\n",
    "            hint = row['post_hint']\n",
    "            sentiment = row['post_sentiment']\n",
    "            \n",
    "            # Check hint, path validity, and if label exists in map\n",
    "            if hint == 'image' and pd.notna(path) and os.path.exists(path) and sentiment in label_map:\n",
    "                 # Basic extension check (can be improved)\n",
    "                 if path.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):\n",
    "                     label_id = label_map[sentiment]\n",
    "                     valid_data.append({'path': path, 'label': label_id})\n",
    "        \n",
    "        self.data = pd.DataFrame(valid_data)\n",
    "        self.paths = self.data['path'].tolist()\n",
    "        self.labels = self.data['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        try:\n",
    "            # Open the image file\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, OSError, Exception) as e:\n",
    "            print(f\"Warning: Could not load/process image {img_path}. Using a blank image. Error: {e}\")\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0)) # Return blank image\n",
    "            \n",
    "        # Process the image (resize, normalize, etc.)\n",
    "        try:\n",
    "           processed_image = self.processor(\n",
    "               images=image, \n",
    "               return_tensors=\"pt\"\n",
    "           )\n",
    "           pixel_values = processed_image['pixel_values'].squeeze() \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing image {img_path} with CLIP processor. Using blank tensor. Error: {e}\")\n",
    "            # Determine expected tensor size from processor config if possible, else default\n",
    "            c, h, w = 3, 224, 224 # Default CLIP size\n",
    "            try: \n",
    "                 if hasattr(self.processor, 'size'):\n",
    "                     h = self.processor.size['height']\n",
    "                     w = self.processor.size['width']\n",
    "            except: pass\n",
    "            pixel_values = torch.zeros((c, h, w))\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 4. Define Custom Model with Classification Head ---\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        self.clip_vision_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        embedding_dim = self.clip_vision_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.clip_vision_model(\n",
    "            pixel_values=pixel_values\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "        \n",
    "\n",
    "# --- 5. Initialize Processor, Datasets, and DataLoaders ---\n",
    "print(\"Loading processor and building datasets...\")\n",
    "image_processor = CLIPImageProcessor.from_pretrained(IMAGE_MODEL_NAME)\n",
    "\n",
    "train_dataset = ImageSentimentDataset(train_df, image_processor, label_to_id)\n",
    "val_dataset = ImageSentimentDataset(val_df, image_processor, label_to_id)\n",
    "\n",
    "print(f\"\\n--- Image Dataset Sizes ---\")\n",
    "print(f\"Total valid training images found: {len(train_dataset)}\")\n",
    "print(f\"Total valid validation images found: {len(val_dataset)}\")\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"ERROR: No valid images found in training set after filtering. Cannot train image model.\")\n",
    "    print(\"Check file paths, image formats, and labels in train_set.csv.\")\n",
    "    # Optionally skip image training if this happens\n",
    "    # proceed_without_image = True \n",
    "    raise ValueError(\"No valid training data for image model.\")\n",
    "else:\n",
    "   proceed_without_image = False\n",
    "\n",
    "if not proceed_without_image:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # Only create val_loader if val_dataset is not empty\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE) if len(val_dataset) > 0 else None\n",
    "\n",
    "    # --- 6. Load Model, Optimizer, and Scheduler ---\n",
    "    print(f\"Loading pre-trained model: {IMAGE_MODEL_NAME}\")\n",
    "    image_model = CustomCLIPModel(IMAGE_MODEL_NAME, NUM_LABELS).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(image_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # --- 7. The Training & Validation Loop ---\n",
    "    print(\"--- Starting Training --- \")\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1} / {EPOCHS} ---\")\n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        image_model.train()\n",
    "        total_train_loss = 0\n",
    "        train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch in train_progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            image_model.zero_grad()\n",
    "\n",
    "            outputs = image_model(\n",
    "                pixel_values=pixel_values,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs['loss']\n",
    "            if torch.isnan(loss):\n",
    "                 print(\"Warning: NaN loss detected during image training. Skipping batch.\")\n",
    "                 continue\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(image_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        if val_loader is None or len(val_dataset) == 0:\n",
    "            print(\"Validation set is empty. Skipping validation.\")\n",
    "            # Save model from the last epoch if no validation\n",
    "            torch.save(image_model.state_dict(), MODEL_SAVE_PATH)\n",
    "            continue\n",
    "            \n",
    "        image_model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress_bar:\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = image_model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs['loss']\n",
    "                logits = outputs['logits']\n",
    "                \n",
    "                if loss is not None and not torch.isnan(loss):\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels_cpu = labels.cpu().numpy()\n",
    "                \n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels_cpu)\n",
    "                \n",
    "        if not all_labels: # Check if validation produced any results\n",
    "             print(\"No valid samples processed during validation.\")\n",
    "             continue\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Save the Best Model --- \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(\"New best model! Saving...\")\n",
    "            torch.save(image_model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "    print(\"\\n--- Image Model Training Complete ---\")\n",
    "    if best_val_accuracy > 0:\n",
    "        print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "    elif len(val_dataset) == 0:\n",
    "         print(\"Model saved from last epoch as validation was skipped.\")\n",
    "    else:\n",
    "        print(\"Validation accuracy did not improve.\")\n",
    "    print(f\"Model weights (state_dict) saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Image Model training as no valid training data was found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b1057",
   "metadata": {},
   "source": [
    "### 4.C: Video Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aaad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.C: Video Model (VideoMAE) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEForVideoClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import cv2  # OpenCV for video processing\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting Phase 1: Video Model Training (VideoMAE) ---\")\n",
    "\n",
    "# --- 1. Configuration & Setup ---\n",
    "\n",
    "VIDEO_PROCESSOR_NAME = \"MCG-NJU/videomae-base\" \n",
    "VIDEO_MODEL_NAME = \"MCG-NJU/videomae-base-finetuned-kinetics\" \n",
    "MODEL_SAVE_PATH = \"./models/video_specialist\"\n",
    "BATCH_SIZE = 2 # Keep small for video\n",
    "EPOCHS = 20 # Train longer for small dataset\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_FRAMES = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Re-create Label Maps ---\n",
    "try:\n",
    "    if 'train_df' not in locals() or train_df.empty:\n",
    "        train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    if 'val_df' not in locals() or val_df.empty:\n",
    "        val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "    if 'label_to_id' not in locals() or not label_to_id:\n",
    "         raise ValueError(\"Label map not defined in common setup cell.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    raise\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Using {NUM_LABELS} labels for video model: {label_to_id}\")\n",
    "\n",
    "\n",
    "# --- 3. Define Custom Video Dataset ---\n",
    "class VideoSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, label_map, num_frames):\n",
    "        self.processor = processor\n",
    "        self.label_map = label_map\n",
    "        self.num_frames = num_frames\n",
    "        \n",
    "        # Filter for valid video paths and map labels\n",
    "        valid_data = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            path = row['local_media_path']\n",
    "            hint = row['post_hint']\n",
    "            sentiment = row['post_sentiment']\n",
    "            \n",
    "            if hint == 'hosted:video' and pd.notna(path) and os.path.exists(path) and sentiment in label_map:\n",
    "                 # Basic extension check\n",
    "                 if path.lower().endswith('.mp4'): # Add other video formats if needed\n",
    "                     label_id = label_map[sentiment]\n",
    "                     valid_data.append({'path': path, 'label': label_id})\n",
    "        \n",
    "        self.data = pd.DataFrame(valid_data)\n",
    "        self.paths = self.data['path'].tolist()\n",
    "        self.labels = self.data['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _sample_frames(self, video_path):\n",
    "        frames = []\n",
    "        cap = None # Initialize cap outside try block for finally clause\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise IOError(f\"Cannot open video file: {video_path}\")\n",
    "                \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if total_frames <= 0:\n",
    "                raise IOError(f\"Video file seems empty or has 0 frames: {video_path}\")\n",
    "            \n",
    "            indices = np.linspace(0, total_frames - 1, num=self.num_frames, dtype=int)\n",
    "            \n",
    "            processed_frames = 0\n",
    "            for idx in indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frames.append(Image.fromarray(frame_rgb))\n",
    "                    processed_frames += 1\n",
    "                # else: # Optional: Log if a frame read fails\n",
    "                #    print(f\"Warning: Failed to read frame {idx} from {video_path}\")\n",
    "            \n",
    "            if processed_frames < self.num_frames // 2: # Check if we got a reasonable number of frames\n",
    "                print(f\"Warning: Read significantly fewer frames ({processed_frames}/{self.num_frames}) than expected from {video_path}\")\n",
    "            if not frames: \n",
    "                 raise IOError(f\"Could not read any frames from: {video_path}\")\n",
    "            # Ensure we return exactly num_frames, duplicating last frame if necessary\n",
    "            while len(frames) < self.num_frames:\n",
    "                 if frames: frames.append(frames[-1]) # Duplicate last frame\n",
    "                 else: # If absolutely no frames were read, return blanks\n",
    "                      raise IOError(\"No frames read, cannot duplicate.\")\n",
    "\n",
    "            return frames[:self.num_frames] # Ensure exactly num_frames are returned\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {e}. Returning blank frames.\")\n",
    "            # Determine expected size from processor if possible\n",
    "            h, w = 224, 224 # Default\n",
    "            try: \n",
    "                 if hasattr(self.processor, 'size'):\n",
    "                     h = self.processor.size['height']\n",
    "                     w = self.processor.size['width']\n",
    "            except: pass\n",
    "            return [Image.new(\"RGB\", (w, h), (0, 0, 0))] * self.num_frames\n",
    "        finally:\n",
    "            if cap is not None: \n",
    "                cap.release()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.paths[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        frames = self._sample_frames(video_path)\n",
    "        \n",
    "        try:\n",
    "            # VideoMAE expects a list of images\n",
    "            processed_video = self.processor(\n",
    "                images=frames, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # The processor should handle the batch dimension correctly for videos\n",
    "            pixel_values = processed_video['pixel_values'].squeeze(0) # Remove potential extra batch dim if added\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing frames from {video_path} with VideoMAE processor. Using blank tensor. Error: {e}\")\n",
    "            # Determine expected tensor size from processor config\n",
    "            t, c, h, w = self.num_frames, 3, 224, 224 # Default\n",
    "            try: \n",
    "                 if hasattr(self.processor, 'size'):\n",
    "                     h = self.processor.size['height']\n",
    "                     w = self.processor.size['width']\n",
    "            except: pass\n",
    "            pixel_values = torch.zeros((t, c, h, w)) # Shape (T, C, H, W)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "\n",
    "# --- 4. Initialize Processor, Datasets, and DataLoaders ---\n",
    "print(f\"Loading processor from: {VIDEO_PROCESSOR_NAME}\")\n",
    "video_image_processor = VideoMAEImageProcessor.from_pretrained(VIDEO_PROCESSOR_NAME) \n",
    "\n",
    "train_dataset = VideoSentimentDataset(train_df, video_image_processor, label_to_id, NUM_FRAMES)\n",
    "val_dataset = VideoSentimentDataset(val_df, video_image_processor, label_to_id, NUM_FRAMES)\n",
    "\n",
    "print(f\"\\n--- Video Dataset Sizes ---\")\n",
    "print(f\"Total valid training videos found: {len(train_dataset)}\")\n",
    "print(f\"Total valid validation videos found: {len(val_dataset)}\")\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"WARNING: No valid videos found in training set after filtering. Skipping video model training.\")\n",
    "    video_model_trained = False\n",
    "else:\n",
    "    video_model_trained = True\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # Only create val_loader if val_dataset is not empty\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE) if len(val_dataset) > 0 else None\n",
    "\n",
    "\n",
    "# --- 5. Load Model, Optimizer, and Scheduler ---\n",
    "if video_model_trained:\n",
    "    # Ensure the save directory exists before saving\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    \n",
    "    print(f\"Loading model from: {VIDEO_MODEL_NAME}\")\n",
    "    video_model = VideoMAEForVideoClassification.from_pretrained(\n",
    "        VIDEO_MODEL_NAME, \n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True # Important: Drops the old classification head\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = AdamW(video_model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping video model setup as no training data was found.\")\n",
    "\n",
    "# --- 6. The Training & Validation Loop ---\n",
    "if video_model_trained:\n",
    "    print(\"--- Starting Training --- \")\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1} / {EPOCHS} ---\")\n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        video_model.train() \n",
    "        total_train_loss = 0\n",
    "        train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch in train_progress_bar:\n",
    "            # VideoMAE expects pixel_values shape (batch_size, num_frames, num_channels, height, width)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            video_model.zero_grad()\n",
    "\n",
    "            outputs = video_model(\n",
    "                pixel_values=pixel_values,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            if torch.isnan(loss):\n",
    "                 print(\"Warning: NaN loss detected during video training. Skipping batch.\")\n",
    "                 continue\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(video_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
    "        print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        if val_loader is None:\n",
    "            print(\"Validation set is empty. Skipping validation.\")\n",
    "            # Save model from this epoch if desired when no validation is possible\n",
    "            # video_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "            # video_image_processor.save_pretrained(MODEL_SAVE_PATH)\n",
    "            continue\n",
    "            \n",
    "        video_model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress_bar:\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = video_model(\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                if loss is not None and not torch.isnan(loss):\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels_cpu = labels.cpu().numpy()\n",
    "                \n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels_cpu)\n",
    "                \n",
    "        if not all_labels: # Check if validation produced results\n",
    "            print(\"No valid samples processed during validation.\")\n",
    "            continue\n",
    "            \n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Save the Best Model ---\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            print(\"New best model! Saving...\")\n",
    "            video_model.save_pretrained(MODEL_SAVE_PATH)\n",
    "            video_image_processor.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "    print(\"\\n--- Video Model Training Complete ---\")\n",
    "    if best_val_accuracy > 0:\n",
    "        print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "    elif val_loader is None:\n",
    "        print(\"Model may have been saved from the last epoch as validation was skipped.\")\n",
    "    else:\n",
    "        print(\"Validation accuracy did not improve.\")\n",
    "    print(f\"Model and processor potentially saved to: {MODEL_SAVE_PATH}\")\n",
    "else:\n",
    "    print(\"Video Model training was skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470ecd9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0607dde",
   "metadata": {},
   "source": [
    "## Step 5: Phase 2 Training (Training the Fusion Model)\n",
    "\n",
    "Now that we have our specialists, we *discard* their temporary classification heads. We use the *output embeddings* (the feature vectors) from these models as input for our new, simple **Fusion Model**. This model's job is to learn how to combine the signals from text, image, and video to make the best final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5c3ff",
   "metadata": {},
   "source": [
    "### 5.A: Create Embedding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Phase 2 Training (The Fusion Model) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertModel,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModel,\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEModel\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting Phase 2: Fusion Model Training ---\")\n",
    "\n",
    "# --- 0. Configuration & Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model paths\n",
    "TEXT_MODEL_PATH = \"./models/text_specialist\"\n",
    "IMAGE_MODEL_PATH = \"./models/image_specialist.pth\"\n",
    "VIDEO_MODEL_PATH = \"./models/video_specialist\"\n",
    "FUSION_MODEL_SAVE_PATH = \"./models/fusion_model.pth\"\n",
    "\n",
    "# Define embedding dimensions (based on the 'base' models we used)\n",
    "TEXT_EMBED_DIM = 768  # (from DistilBERT-base)\n",
    "IMAGE_EMBED_DIM = 768  # (from CLIP-ViT-base)\n",
    "VIDEO_EMBED_DIM = 768  # (from VideoMAE-base)\n",
    "COMBINED_EMBED_DIM = TEXT_EMBED_DIM + IMAGE_EMBED_DIM + VIDEO_EMBED_DIM\n",
    "\n",
    "print(f\"Combined embedding dimension will be: {COMBINED_EMBED_DIM}\")\n",
    "\n",
    "# Fusion model training config\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50 # We can train for more epochs, it's a very small model\n",
    "LEARNING_RATE = 1e-4 # A slightly higher LR is fine for this MLP\n",
    "\n",
    "# --- 1. Re-define CustomCLIPModel (to load weights) ---\n",
    "# This MUST be the same class definition as in Step 4.B\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        self.clip_vision_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        embedding_dim = self.clip_vision_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.clip_vision_model(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n",
    "            \n",
    "        return {\"loss\": loss, \"logits\": logits, \"embedding\": pooled_output}\n",
    "\n",
    "\n",
    "# --- 2. Load Data and Label Maps ---\n",
    "try:\n",
    "    if 'train_df' not in locals() or train_df.empty:\n",
    "         train_df = pd.read_csv(TRAIN_SET_CSV)\n",
    "    if 'val_df' not in locals() or val_df.empty:\n",
    "         val_df = pd.read_csv(VALIDATION_SET_CSV)\n",
    "    if 'label_to_id' not in locals() or not label_to_id:\n",
    "        raise ValueError(\"Label map not defined.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {VALIDATION_SET_CSV}\")\n",
    "    raise\n",
    "except ValueError as e:\n",
    "     print(f\"Error: {e}\")\n",
    "     raise\n",
    "\n",
    "print(f\"Loaded {len(train_df)} train posts and {len(val_df)} validation posts.\")\n",
    "print(f\"Using {NUM_LABELS} labels: {label_to_id}\")\n",
    "\n",
    "\n",
    "# --- 3. Load All Specialist Models and Processors (Base models for embeddings) ---\n",
    "\n",
    "print(\"Loading specialist models (base versions for embeddings)...\")\n",
    "\n",
    "# --- Text Specialist ---\n",
    "text_specialist_exists = os.path.exists(TEXT_MODEL_PATH)\n",
    "if text_specialist_exists:\n",
    "    try:\n",
    "        text_tokenizer = DistilBertTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "        text_model = DistilBertModel.from_pretrained(TEXT_MODEL_PATH).to(device)\n",
    "        text_model.eval()\n",
    "        print(\"Text specialist base model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load text specialist from {TEXT_MODEL_PATH}: {e}. Text embeddings will be zeros.\")\n",
    "        text_model = None\n",
    "        text_tokenizer = None\n",
    "else:\n",
    "    print(f\"Warning: Text specialist model not found at {TEXT_MODEL_PATH}. Text embeddings will be zeros.\")\n",
    "    text_model = None\n",
    "    text_tokenizer = None\n",
    "\n",
    "# --- Image Specialist ---\n",
    "image_specialist_exists = os.path.exists(IMAGE_MODEL_PATH)\n",
    "if image_specialist_exists:\n",
    "    try:\n",
    "        image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        # Load the state dict into the custom model, then extract the base vision model\n",
    "        temp_image_model = CustomCLIPModel(\"openai/clip-vit-base-patch32\", NUM_LABELS) \n",
    "        temp_image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=device))\n",
    "        image_model = temp_image_model.clip_vision_model.to(device)\n",
    "        image_model.eval()\n",
    "        print(\"Image specialist base model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load image specialist state_dict from {IMAGE_MODEL_PATH}: {e}. Image embeddings will be zeros.\")\n",
    "        image_model = None\n",
    "        image_processor = None\n",
    "else:\n",
    "    print(f\"Warning: Image specialist model not found at {IMAGE_MODEL_PATH}. Image embeddings will be zeros.\")\n",
    "    image_model = None\n",
    "    image_processor = None\n",
    "\n",
    "# --- Video Specialist ---\n",
    "video_specialist_exists = os.path.exists(VIDEO_MODEL_PATH)\n",
    "if video_specialist_exists:\n",
    "    try:\n",
    "        video_processor = VideoMAEImageProcessor.from_pretrained(VIDEO_MODEL_PATH)\n",
    "        # Load the base model directly\n",
    "        video_model = VideoMAEModel.from_pretrained(VIDEO_MODEL_PATH).to(device)\n",
    "        video_model.eval()\n",
    "        print(\"Video specialist base model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load video specialist from {VIDEO_MODEL_PATH}: {e}. Video embeddings will be zeros.\")\n",
    "        video_model = None\n",
    "        video_processor = None\n",
    "else:\n",
    "    print(f\"Warning: Video specialist model not found at {VIDEO_MODEL_PATH}. Video embeddings will be zeros.\")\n",
    "    video_model = None\n",
    "    video_processor = None\n",
    "\n",
    "\n",
    "# --- 4. Helper Functions for Embedding Extraction ---\n",
    "\n",
    "def _sample_frames(video_path, num_frames=16):\n",
    "    frames = []\n",
    "    cap = None\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened(): raise IOError(f\"Cannot open video: {video_path}\")\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0: raise IOError(f\"Video empty: {video_path}\")\n",
    "        indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(Image.fromarray(frame_rgb))\n",
    "        if not frames: raise IOError(f\"Could not read frames: {video_path}\")\n",
    "        while len(frames) < num_frames:\n",
    "             if frames: frames.append(frames[-1])\n",
    "             else: raise IOError(\"No frames read, cannot duplicate.\")\n",
    "        return frames[:num_frames]\n",
    "    except Exception as e:\n",
    "        # print(f\"Error sampling {video_path}: {e}. Using blank frames.\") # Reduce verbosity\n",
    "        h, w = 224, 224 \n",
    "        return [Image.new(\"RGB\", (w, h), (0, 0, 0))] * num_frames\n",
    "    finally:\n",
    "        if cap is not None: cap.release()\n",
    "\n",
    "def get_embeddings(post_row):\n",
    "    with torch.no_grad():\n",
    "        # Text\n",
    "        if text_model is not None and text_tokenizer is not None and pd.notna(post_row['title']):\n",
    "            text_str = post_row.get('text', '')\n",
    "            text = str(post_row['title']) + \" [SEP] \" + (str(text_str) if pd.notna(text_str) else '')\n",
    "            try:\n",
    "                encoding = text_tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length').to(device)\n",
    "                outputs = text_model(**encoding)\n",
    "                text_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu()\n",
    "            except Exception as e:\n",
    "                 # print(f\"Error getting text embedding for {post_row['id']}: {e}\")\n",
    "                 text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "        else:\n",
    "            text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "\n",
    "        # Image\n",
    "        if (image_model is not None and image_processor is not None and\n",
    "            post_row['post_hint'] == 'image' and pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            try:\n",
    "                image = Image.open(post_row['local_media_path']).convert(\"RGB\")\n",
    "                processed_image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "                outputs = image_model(**processed_image)\n",
    "                image_emb = outputs.pooler_output.squeeze().cpu()\n",
    "            except (UnidentifiedImageError, FileNotFoundError, OSError, Exception) as e:\n",
    "                # print(f\"Error processing image {post_row['local_media_path']}: {e}\")\n",
    "                image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "        else:\n",
    "            image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "\n",
    "        # Video\n",
    "        if (video_model is not None and video_processor is not None and\n",
    "            post_row['post_hint'] == 'hosted:video' and pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            try:\n",
    "                frames = _sample_frames(post_row['local_media_path'])\n",
    "                processed_video = video_processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "                outputs = video_model(**processed_video)\n",
    "                video_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu() # Avg pool frames\n",
    "            except Exception as e:\n",
    "                 # print(f\"Error processing video {post_row['local_media_path']}: {e}\")\n",
    "                 video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "        else:\n",
    "            video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "            \n",
    "        # Concat\n",
    "        # Ensure all are tensors before concatenating\n",
    "        if not isinstance(text_emb, torch.Tensor): text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "        if not isinstance(image_emb, torch.Tensor): image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "        if not isinstance(video_emb, torch.Tensor): video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "        \n",
    "        # Detach tensors if they were accidentally left on GPU (should be CPU from above)\n",
    "        combined_emb = torch.cat((text_emb.cpu().detach(), image_emb.cpu().detach(), video_emb.cpu().detach()))\n",
    "        \n",
    "        # Get label ID, handle missing labels\n",
    "        label_id = label_to_id.get(post_row['post_sentiment'], -1) # Use -1 if sentiment not in map\n",
    "        \n",
    "        return combined_emb, label_id\n",
    "\n",
    "\n",
    "# --- 5.A: Create Embedding Dataset ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5.A: Creating Embedding Datasets ---\")\n",
    "\n",
    "def create_dataset(df, filename):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    valid_indices = [] # Keep track of rows that yield valid data\n",
    "    \n",
    "    print(f\"Processing {filename} with {len(df)} rows...\")\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating {filename}\"):\n",
    "        emb, label = get_embeddings(row)\n",
    "        # Only include if the label is valid\n",
    "        if label != -1:\n",
    "            # Basic check for embedding validity (e.g., not all zeros if expected)\n",
    "            # This check is optional and might be too strict depending on data\n",
    "            # if emb.count_nonzero() > 0:\n",
    "                embeddings.append(emb)\n",
    "                labels.append(label)\n",
    "                valid_indices.append(index)\n",
    "            # else:\n",
    "            #    print(f\"Warning: Skipping row {index} due to potentially zero embedding.\")\n",
    "        # else:\n",
    "            # print(f\"Warning: Skipping row {index} due to invalid label '{row['post_sentiment']}'.\")\n",
    "            \n",
    "    if not embeddings: # Check if any valid data was processed\n",
    "        print(f\"Error: No valid embeddings or labels generated for {filename}. Check source data and specialist models.\")\n",
    "        # Return empty tensors or raise error\n",
    "        return torch.empty((0, COMBINED_EMBED_DIM)), torch.empty((0), dtype=torch.long)\n",
    "        \n",
    "    # Stack valid tensors\n",
    "    embeddings = torch.stack(embeddings)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    # Save to disk\n",
    "    save_path = os.path.join(DATA_DIR, filename)\n",
    "    torch.save((embeddings, labels), save_path)\n",
    "    print(f\"Saved dataset with {len(labels)} valid entries to {save_path}\")\n",
    "    return embeddings, labels\n",
    "\n",
    "train_embeddings, train_labels = create_dataset(train_df, \"train_embeddings.pt\")\n",
    "val_embeddings, val_labels = create_dataset(val_df, \"val_embeddings.pt\")\n",
    "\n",
    "if train_embeddings.nelement() == 0: # Check if tensor is empty\n",
    "    raise ValueError(\"Training embedding dataset is empty. Cannot proceed.\")\n",
    "\n",
    "print(\"Embedding datasets creation process finished.\")\n",
    "\n",
    "\n",
    "# --- 5.B: Train Fusion Model ---\n",
    "\n",
    "print(\"\\n--- Starting Step 5.B: Training Fusion Model ---\")\n",
    "\n",
    "# 1. Define simple Dataset for embeddings\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "train_emb_dataset = EmbeddingDataset(train_embeddings, train_labels)\n",
    "train_emb_loader = DataLoader(train_emb_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Only create validation loader if embeddings exist\n",
    "if val_embeddings.nelement() > 0:\n",
    "    val_emb_dataset = EmbeddingDataset(val_embeddings, val_labels)\n",
    "    val_emb_loader = DataLoader(val_emb_dataset, batch_size=BATCH_SIZE)\n",
    "    print(f\"Fusion Validation dataset size: {len(val_emb_dataset)}\")\n",
    "else:\n",
    "    val_emb_loader = None\n",
    "    print(\"Warning: Validation embedding dataset is empty. Skipping validation during fusion training.\")\n",
    "\n",
    "print(f\"Fusion Training dataset size: {len(train_emb_dataset)}\")\n",
    "\n",
    "# 2. Define the Fusion Model architecture\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "fusion_model = FusionModel(\n",
    "    input_dim=COMBINED_EMBED_DIM, \n",
    "    hidden_dim=512,  # A reasonable hidden layer size\n",
    "    output_dim=NUM_LABELS\n",
    ").to(device)\n",
    "\n",
    "# 3. Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 4. Training Loop\n",
    "print(\"Training fusion model...\")\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # --- Training ---\n",
    "    fusion_model.train()\n",
    "    total_train_loss = 0\n",
    "    for embs, labels in train_emb_loader:\n",
    "        embs = embs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = fusion_model(embs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Warning: NaN loss detected during fusion training epoch {epoch+1}. Skipping batch.\")\n",
    "            continue\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_emb_loader) if len(train_emb_loader) > 0 else 0\n",
    "\n",
    "    # --- Validation ---\n",
    "    if val_emb_loader is None:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Validation Skipped\")\n",
    "        # Save the model from the last epoch if validation is skipped\n",
    "        torch.save(fusion_model.state_dict(), FUSION_MODEL_SAVE_PATH)\n",
    "        continue # Skip validation logic\n",
    "\n",
    "    fusion_model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embs, labels in val_emb_loader:\n",
    "            embs = embs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = fusion_model(embs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if loss is not None and not torch.isnan(loss):\n",
    "                 total_val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    if not all_labels: # Check if validation processed anything\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | No valid validation samples.\")\n",
    "        continue\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_emb_loader) if len(val_emb_loader) > 0 else 0\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        print(f\"New best model! Saving to {FUSION_MODEL_SAVE_PATH}\")\n",
    "        torch.save(fusion_model.state_dict(), FUSION_MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- Fusion Training Complete ---\")\n",
    "if val_emb_loader is not None and best_val_accuracy > 0:\n",
    "    print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "elif val_emb_loader is None:\n",
    "    print(\"Model saved from last epoch as validation was skipped.\")\n",
    "else:\n",
    "    print(\"Validation accuracy did not improve.\")\n",
    "print(f\"Final fusion model potentially saved to: {FUSION_MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb138f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8431c",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation (The Final Test)\n",
    "\n",
    "This is the moment of truth. We now use our **unseen** `test_set.csv`. We run each post in it through the *entire pipeline* (Specialists -> Fusion Model) and compare the final prediction to the true label. This gives us our final performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertModel,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionModel,\n",
    "    VideoMAEImageProcessor,\n",
    "    VideoMAEModel\n",
    ")\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Starting Step 6: Final Evaluation on Test Set ---\")\n",
    "\n",
    "# --- 0. Configuration & Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model paths\n",
    "TEXT_MODEL_PATH = \"./models/text_specialist\"\n",
    "IMAGE_MODEL_PATH = \"./models/image_specialist.pth\"\n",
    "VIDEO_MODEL_PATH = \"./models/video_specialist\"\n",
    "FUSION_MODEL_SAVE_PATH = \"./models/fusion_model.pth\"\n",
    "\n",
    "# Define embedding dimensions\n",
    "TEXT_EMBED_DIM = 768\n",
    "IMAGE_EMBED_DIM = 768\n",
    "VIDEO_EMBED_DIM = 768\n",
    "COMBINED_EMBED_DIM = TEXT_EMBED_DIM + IMAGE_EMBED_DIM + VIDEO_EMBED_DIM\n",
    "\n",
    "# --- 1. Re-define Model Classes (from Step 4 & 5) ---\n",
    "\n",
    "# Custom CLIP Model (from 4.B)\n",
    "class CustomCLIPModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(CustomCLIPModel, self).__init__()\n",
    "        self.clip_vision_model = CLIPVisionModel.from_pretrained(model_name)\n",
    "        embedding_dim = self.clip_vision_model.config.hidden_size\n",
    "        # Classifier not strictly needed for eval, but defining for consistency\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels) \n",
    "        \n",
    "    # We only need the base model's forward pass for embeddings\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.clip_vision_model(pixel_values=pixel_values)\n",
    "        return outputs # Return the full output sequence\n",
    "\n",
    "# Fusion Model (from 5.B)\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "# --- 2. Re-define Helper Functions (from Step 4 & 5) ---\n",
    "\n",
    "def _sample_frames(video_path, num_frames=16):\n",
    "    frames = []\n",
    "    cap = None\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened(): raise IOError(f\"Cannot open video: {video_path}\")\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames <= 0: raise IOError(f\"Video empty: {video_path}\")\n",
    "        indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(Image.fromarray(frame_rgb))\n",
    "        if not frames: raise IOError(f\"Could not read frames: {video_path}\")\n",
    "        while len(frames) < num_frames:\n",
    "             if frames: frames.append(frames[-1])\n",
    "             else: raise IOError(\"No frames read, cannot duplicate.\")\n",
    "        return frames[:num_frames]\n",
    "    except Exception as e:\n",
    "        h, w = 224, 224 # Default size\n",
    "        return [Image.new(\"RGB\", (w, h), (0, 0, 0))] * num_frames\n",
    "    finally:\n",
    "        if cap is not None: cap.release()\n",
    "\n",
    "# Updated embedding extraction for evaluation (takes loaded models as args)\n",
    "def get_embeddings_for_eval(post_row, text_model, image_model, video_model, text_tokenizer, image_processor, video_processor, label_map):\n",
    "    with torch.no_grad():\n",
    "        # Text\n",
    "        if text_model is not None and text_tokenizer is not None and pd.notna(post_row['title']):\n",
    "            text_str = post_row.get('text', '')\n",
    "            text = str(post_row['title']) + \" [SEP] \" + (str(text_str) if pd.notna(text_str) else '')\n",
    "            try:\n",
    "                encoding = text_tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length').to(device)\n",
    "                outputs = text_model(**encoding)\n",
    "                text_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu()\n",
    "            except Exception: text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "        else: text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "\n",
    "        # Image\n",
    "        if (image_model is not None and image_processor is not None and\n",
    "            post_row['post_hint'] == 'image' and pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            try:\n",
    "                image = Image.open(post_row['local_media_path']).convert(\"RGB\")\n",
    "                processed_image = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "                # Pass through the base vision model only\n",
    "                outputs = image_model(processed_image['pixel_values'])\n",
    "                image_emb = outputs.pooler_output.squeeze().cpu()\n",
    "            except Exception: image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "        else: image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "\n",
    "        # Video\n",
    "        if (video_model is not None and video_processor is not None and\n",
    "            post_row['post_hint'] == 'hosted:video' and pd.notna(post_row['local_media_path']) and \n",
    "            os.path.exists(post_row['local_media_path'])):\n",
    "            try:\n",
    "                frames = _sample_frames(post_row['local_media_path'])\n",
    "                processed_video = video_processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "                outputs = video_model(**processed_video)\n",
    "                video_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu()\n",
    "            except Exception: video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "        else: video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "            \n",
    "        # Concat\n",
    "        if not isinstance(text_emb, torch.Tensor): text_emb = torch.zeros(TEXT_EMBED_DIM)\n",
    "        if not isinstance(image_emb, torch.Tensor): image_emb = torch.zeros(IMAGE_EMBED_DIM)\n",
    "        if not isinstance(video_emb, torch.Tensor): video_emb = torch.zeros(VIDEO_EMBED_DIM)\n",
    "        combined_emb = torch.cat((text_emb.cpu().detach(), image_emb.cpu().detach(), video_emb.cpu().detach()))\n",
    "        \n",
    "        # Get true label ID\n",
    "        true_label_id = label_map.get(post_row['post_sentiment'], -1)\n",
    "        \n",
    "        return combined_emb, true_label_id\n",
    "\n",
    "# --- 3. Load Data and Label Maps ---\n",
    "try:\n",
    "    # Load train_df only to get the label map consistently\n",
    "    if 'label_to_id' not in locals() or not label_to_id: \n",
    "         train_df_labels = pd.read_csv(TRAIN_SET_CSV)\n",
    "         SENTIMENT_LABELS_EVAL = sorted(train_df_labels['post_sentiment'].dropna().unique())\n",
    "         label_to_id = {label: i for i, label in enumerate(SENTIMENT_LABELS_EVAL)}\n",
    "         id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS_EVAL)}\n",
    "         NUM_LABELS = len(SENTIMENT_LABELS_EVAL)\n",
    "         print(\"(Re)loaded label map for evaluation.\")\n",
    "    else:\n",
    "         SENTIMENT_LABELS_EVAL = list(label_to_id.keys())\n",
    "         print(\"Using existing label map for evaluation.\")\n",
    "\n",
    "    test_df = pd.read_csv(TEST_SET_CSV)\n",
    "    # Drop rows in test set with missing sentiment labels\n",
    "    test_df.dropna(subset=['post_sentiment'], inplace=True)\n",
    "    # Filter test set for labels that are actually in our map\n",
    "    test_df = test_df[test_df['post_sentiment'].isin(label_to_id.keys())]\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find {TRAIN_SET_CSV} or {TEST_SET_CSV}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "     print(f\"Error loading data: {e}\")\n",
    "     raise\n",
    "\n",
    "if test_df.empty:\n",
    "     raise ValueError(\"Test dataset is empty after filtering for valid labels. Cannot evaluate.\")\n",
    "\n",
    "print(f\"Loaded {len(test_df)} valid test posts.\")\n",
    "print(f\"Using {NUM_LABELS} labels for evaluation: {label_to_id}\")\n",
    "\n",
    "# --- 4. Load All Trained Models and Processors ---\n",
    "print(\"Loading all models and processors...\")\n",
    "models_loaded = {'text': False, 'image': False, 'video': False, 'fusion': False}\n",
    "\n",
    "# Text\n",
    "try:\n",
    "    text_tokenizer_eval = DistilBertTokenizer.from_pretrained(TEXT_MODEL_PATH)\n",
    "    text_specialist_eval = DistilBertModel.from_pretrained(TEXT_MODEL_PATH).to(device)\n",
    "    text_specialist_eval.eval()\n",
    "    models_loaded['text'] = True\n",
    "    print(\"Text model loaded.\")\n",
    "except Exception as e: \n",
    "    print(f\"Warning: Could not load text model: {e}. Text embeddings will be zeros.\")\n",
    "    text_tokenizer_eval = None\n",
    "    text_specialist_eval = None\n",
    "\n",
    "# Image\n",
    "try:\n",
    "    image_processor_eval = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    # Need to instantiate the custom class first\n",
    "    temp_image_model_eval = CustomCLIPModel(\"openai/clip-vit-base-patch32\", NUM_LABELS) \n",
    "    # Load the saved state dict\n",
    "    temp_image_model_eval.load_state_dict(torch.load(IMAGE_MODEL_PATH, map_location=device))\n",
    "    # Extract the base vision model\n",
    "    image_specialist_eval = temp_image_model_eval.clip_vision_model.to(device)\n",
    "    image_specialist_eval.eval()\n",
    "    models_loaded['image'] = True\n",
    "    print(\"Image model loaded.\")\n",
    "except Exception as e: \n",
    "    print(f\"Warning: Could not load image model: {e}. Image embeddings will be zeros.\")\n",
    "    image_processor_eval = None\n",
    "    image_specialist_eval = None\n",
    "\n",
    "# Video\n",
    "try:\n",
    "    video_processor_eval = VideoMAEImageProcessor.from_pretrained(VIDEO_MODEL_PATH)\n",
    "    video_specialist_eval = VideoMAEModel.from_pretrained(VIDEO_MODEL_PATH).to(device)\n",
    "    video_specialist_eval.eval()\n",
    "    models_loaded['video'] = True\n",
    "    print(\"Video model loaded.\")\n",
    "except Exception as e: \n",
    "    print(f\"Warning: Could not load video model: {e}. Video embeddings will be zeros.\")\n",
    "    video_processor_eval = None\n",
    "    video_specialist_eval = None\n",
    "\n",
    "# Fusion\n",
    "try:\n",
    "    fusion_model_eval = FusionModel(\n",
    "        input_dim=COMBINED_EMBED_DIM, \n",
    "        hidden_dim=512, \n",
    "        output_dim=NUM_LABELS\n",
    "    ).to(device)\n",
    "    fusion_model_eval.load_state_dict(torch.load(FUSION_MODEL_SAVE_PATH, map_location=device))\n",
    "    fusion_model_eval.eval()\n",
    "    models_loaded['fusion'] = True\n",
    "    print(\"Fusion model loaded.\")\n",
    "except Exception as e:\n",
    "     print(f\"CRITICAL ERROR: Could not load fusion model: {e}. CANNOT EVALUATE.\")\n",
    "     # Stop execution if fusion model fails\n",
    "     raise SystemExit(\"Fusion model loading failed.\") \n",
    "\n",
    "if not any(models_loaded.values()):\n",
    "     raise SystemExit(\"CRITICAL ERROR: No models loaded successfully. Cannot evaluate.\")\n",
    "elif not models_loaded['fusion']:\n",
    "     raise SystemExit(\"CRITICAL ERROR: Fusion model did not load. Cannot evaluate.\")\n",
    "     \n",
    "print(\"Model loading process finished.\")\n",
    "\n",
    "# --- 5. Run Evaluation Loop ---\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "print(\"\\nStarting evaluation on test set...\")\n",
    "with torch.no_grad():\n",
    "    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating Test Set\"):\n",
    "        # Get embeddings using the loaded models for evaluation\n",
    "        combined_emb, true_label_id = get_embeddings_for_eval(\n",
    "            row, \n",
    "            text_specialist_eval, \n",
    "            image_specialist_eval, \n",
    "            video_specialist_eval, \n",
    "            text_tokenizer_eval, \n",
    "            image_processor_eval, \n",
    "            video_processor_eval,\n",
    "            label_to_id # Pass the correct label map\n",
    "        )\n",
    "        \n",
    "        # Ensure the true label is valid before adding\n",
    "        if true_label_id != -1:\n",
    "            all_true_labels.append(true_label_id)\n",
    "            \n",
    "            # Pass embedding through the fusion model\n",
    "            combined_emb = combined_emb.unsqueeze(0).to(device) # Add batch dim and move to device\n",
    "            logits = fusion_model_eval(combined_emb)\n",
    "            prediction_id = torch.argmax(logits, dim=1).item()\n",
    "            all_predictions.append(prediction_id)\n",
    "        # else: # Optional: Log skipped rows due to label issues\n",
    "            # print(f\"Skipping row {index} due to invalid true label during evaluation.\")\n",
    "\n",
    "# --- 6. Calculate and Print Metrics ---\n",
    "\n",
    "if not all_true_labels or not all_predictions:\n",
    "    print(\"\\nError: No valid predictions or labels were generated. Cannot calculate metrics.\")\n",
    "    print(f\"Length of true labels: {len(all_true_labels)}\")\n",
    "    print(f\"Length of predictions: {len(all_predictions)}\")\n",
    "else:\n",
    "    # Convert IDs back to labels for a readable report\n",
    "    # Use the evaluation label set derived from the training data\n",
    "    true_labels_names = [id_to_label[idx] for idx in all_true_labels]\n",
    "    predictions_names = [id_to_label[idx] for idx in all_predictions]\n",
    "    \n",
    "    print(\"\\n--- Final Model Performance on Test Set ---\")\n",
    "    accuracy = accuracy_score(true_labels_names, predictions_names)\n",
    "    print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Ensure labels parameter uses the correct set of labels present in the data\n",
    "    report = classification_report(true_labels_names, predictions_names, labels=SENTIMENT_LABELS_EVAL, zero_division=0)\n",
    "    print(report)\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    try:\n",
    "        cm = confusion_matrix(true_labels_names, predictions_names, labels=SENTIMENT_LABELS_EVAL)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=SENTIMENT_LABELS_EVAL, yticklabels=SENTIMENT_LABELS_EVAL, cmap='Blues')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix on Test Set')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating confusion matrix plot: {e}\")\n",
    "        # Print the matrix numerically as a fallback\n",
    "        print(pd.DataFrame(cm, index=SENTIMENT_LABELS_EVAL, columns=SENTIMENT_LABELS_EVAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31869cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fc243",
   "metadata": {},
   "source": [
    "## Step 7: Prediction on New, Unseen Data\n",
    "\n",
    "This is the final step: creating a single function that can take a brand new, raw Reddit post URL, run it through the entire pipeline, and return the predicted sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure PRAW is initialized (from Step 1 or re-init here)\n",
    "if 'reddit' not in locals():\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=REDDIT_CLIENT_ID,\n",
    "            client_secret=REDDIT_CLIENT_SECRET,\n",
    "            user_agent=REDDIT_USER_AGENT,\n",
    "        )\n",
    "        print(\"PRAW re-initialized for prediction.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing PRAW for prediction: {e}\")\n",
    "        reddit = None # Set to None if init fails\n",
    "\n",
    "# Ensure models are loaded and in eval mode (should be from Step 6)\n",
    "# Add checks here to ensure all necessary models (_eval versions) are loaded\n",
    "required_models = {\n",
    "    'text': 'text_specialist_eval' in locals() and text_specialist_eval is not None,\n",
    "    'image': 'image_specialist_eval' in locals() and image_specialist_eval is not None,\n",
    "    'video': 'video_specialist_eval' in locals() and video_specialist_eval is not None,\n",
    "    'fusion': 'fusion_model_eval' in locals() and fusion_model_eval is not None\n",
    "}\n",
    "\n",
    "required_processors = {\n",
    "    'text': 'text_tokenizer_eval' in locals() and text_tokenizer_eval is not None,\n",
    "    'image': 'image_processor_eval' in locals() and image_processor_eval is not None,\n",
    "    'video': 'video_processor_eval' in locals() and video_processor_eval is not None,\n",
    "}\n",
    "\n",
    "models_ready = all(required_models.values()) and all(required_processors.values())\n",
    "\n",
    "if not models_ready:\n",
    "    print(\"\\n--- WARNING: Not all models/processors loaded successfully for prediction. --- \")\n",
    "    print(\"Missing Text Components:\", not (required_models['text'] and required_processors['text']))\n",
    "    print(\"Missing Image Components:\", not (required_models['image'] and required_processors['image']))\n",
    "    print(\"Missing Video Components:\", not (required_models['video'] and required_processors['video']))\n",
    "    print(\"Missing Fusion Model:\", not required_models['fusion'])\n",
    "    print(\"Prediction function might fail or produce inaccurate results.\")\n",
    "    # Depending on requirements, you might want to raise an error here\n",
    "    # raise SystemExit(\"Cannot proceed with prediction - models not ready.\")\n",
    "\n",
    "# Ensure label map exists\n",
    "if 'id_to_label' not in locals() or not id_to_label:\n",
    "     print(\"Error: id_to_label map not found. Cannot decode predictions.\")\n",
    "     # Attempt to reload from Step 6 variables if they exist\n",
    "     if 'SENTIMENT_LABELS_EVAL' in locals():\n",
    "          id_to_label = {i: label for i, label in enumerate(SENTIMENT_LABELS_EVAL)}\n",
    "          print(\"Reloaded id_to_label map.\")\n",
    "     else:\n",
    "          raise ValueError(\"Label map missing and cannot be reloaded.\")\n",
    "\n",
    "# Use the PRAW download function defined earlier\n",
    "if 'download_media_praw' not in locals():\n",
    "     raise NameError(\"Function 'download_media_praw' not defined. Run the Step 1 PRAW cells.\")\n",
    "\n",
    "# Use the evaluation embedding function defined in Step 6\n",
    "if 'get_embeddings_for_eval' not in locals():\n",
    "     raise NameError(\"Function 'get_embeddings_for_eval' not defined. Run the Step 6 cell.\")\n",
    "\n",
    "def predict_sentiment_for_new_post(post_url):\n",
    "    if not models_ready or reddit is None:\n",
    "        return \"Error: Prediction environment not fully initialized (models/PRAW missing).\"\n",
    "        \n",
    "    print(f\"\\nAnalyzing new post: {post_url}\")\n",
    "    local_media_path = None # Initialize path\n",
    "    \n",
    "    try:\n",
    "        # 1. Scrape the single post using PRAW\n",
    "        print(\"Fetching post data via PRAW...\")\n",
    "        try:\n",
    "            # Extract submission ID from URL for robust fetching\n",
    "            submission_id = praw.models.Submission.id_from_url(post_url)\n",
    "            post = reddit.submission(id=submission_id)\n",
    "            post.load() # Eagerly load attributes\n",
    "        except Exception as e:\n",
    "            return f\"Error: Could not fetch post data from PRAW. {e}\"\n",
    "\n",
    "        # 2. Download its media (use the PRAW download function)\n",
    "        print(\"Attempting to download media...\")\n",
    "        local_media_path = download_media_praw(post)\n",
    "        if local_media_path:\n",
    "             print(f\"Media downloaded to: {local_media_path}\")\n",
    "        else:\n",
    "             print(\"No downloadable media found or download failed.\")\n",
    "\n",
    "        # 3. Create a dictionary (like a DataFrame row) for the post\n",
    "        post_data = {\n",
    "            'id': post.id, \n",
    "            'title': post.title, \n",
    "            'text': post.selftext, \n",
    "            'post_hint': getattr(post, 'post_hint', 'text_only'), \n",
    "            'local_media_path': local_media_path, \n",
    "            'post_sentiment': 'UNKNOWN' # Dummy value, not used by get_embeddings_for_eval\n",
    "        }\n",
    "\n",
    "        # 4. Get embeddings using the evaluation function and loaded models\n",
    "        print(\"Generating embeddings...\")\n",
    "        with torch.no_grad():\n",
    "            # Pass the loaded models explicitly to the function\n",
    "            combined_emb, _ = get_embeddings_for_eval(\n",
    "                post_data, \n",
    "                text_specialist_eval, \n",
    "                image_specialist_eval, \n",
    "                video_specialist_eval, \n",
    "                text_tokenizer_eval, \n",
    "                image_processor_eval, \n",
    "                video_processor_eval,\n",
    "                label_to_id # Pass label_to_id (needed internally by func)\n",
    "            )\n",
    "            combined_emb = combined_emb.unsqueeze(0).to(device) # Add batch dim & move\n",
    "\n",
    "        # 5. Get final prediction from fusion model\n",
    "        print(\"Making final prediction...\")\n",
    "        with torch.no_grad():\n",
    "            logits = fusion_model_eval(combined_emb)\n",
    "            prediction_id = torch.argmax(logits, dim=1).item()\n",
    "            predicted_sentiment = id_to_label.get(prediction_id, \"Unknown Label ID\")\n",
    "        \n",
    "        print(f\"Prediction complete.\")\n",
    "        return predicted_sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        # General error catching during the prediction process\n",
    "        print(f\"An unexpected error occurred during prediction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"Error: Prediction failed. {e}\"\n",
    "\n",
    "    finally:\n",
    "        # 6. Clean up the downloaded media file\n",
    "        if local_media_path and os.path.exists(local_media_path):\n",
    "            try:\n",
    "                print(f\"Cleaning up downloaded media: {local_media_path}\")\n",
    "                os.remove(local_media_path)\n",
    "            except OSError as e:\n",
    "                 print(f\"Error removing temporary file {local_media_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109014af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "# Ensure the models_ready check passed before calling\n",
    "if models_ready and reddit is not None:\n",
    "    # A post that is likely 'Joy' or 'Achievement'\n",
    "    # Make sure these are valid, accessible Reddit post URLs\n",
    "    test_url_1 = \"[https://www.reddit.com/r/Brawlstars/comments/1dbv10k/after_all_this_time_i_finally_got_one/](https://www.reddit.com/r/Brawlstars/comments/1dbv10k/after_all_this_time_i_finally_got_one/)\"\n",
    "\n",
    "    # A post that is likely 'Anger' or 'Rant'\n",
    "    test_url_2 = \"[https://www.reddit.com/r/Brawlstars/comments/1ddc6n6/im_so_sick_of_this_game_breaking_bug/](https://www.reddit.com/r/Brawlstars/comments/1ddc6n6/im_so_sick_of_this_game_breaking_bug/)\"\n",
    "    \n",
    "    # A text-only discussion post\n",
    "    test_url_3 = \"[https://www.reddit.com/r/Brawlstars/comments/1e5w8us/is_it_just_me_or_is_ranked_really_easy_rn/](https://www.reddit.com/r/Brawlstars/comments/1e5w8us/is_it_just_me_or_is_ranked_really_easy_rn/)\"\n",
    "\n",
    "    sentiment1 = predict_sentiment_for_new_post(test_url_1)\n",
    "    print(f\"\\n>>> Prediction for post 1 ({test_url_1}): {sentiment1}\\n\")\n",
    "\n",
    "    sentiment2 = predict_sentiment_for_new_post(test_url_2)\n",
    "    print(f\"\\n>>> Prediction for post 2 ({test_url_2}): {sentiment2}\\n\")\n",
    "\n",
    "    sentiment3 = predict_sentiment_for_new_post(test_url_3)\n",
    "    print(f\"\\n>>> Prediction for post 3 ({test_url_3}): {sentiment3}\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nSkipping example usage because prediction environment is not ready.\")\n",
    "\n",
    "print(\"\\nNotebook execution complete (up to Step 7 example). Review results and potential warnings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
